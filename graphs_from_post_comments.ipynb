{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import ast\n",
    "import time\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "# import torch_geometric as tg\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import re\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', 10)  # Limit number of rows displayed\n",
    "pd.set_option('display.width', 1000)  # Set max width for table\n",
    "pd.set_option('display.colheader_justify', 'center')  # Center-align column headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_chars(value):\n",
    "    if isinstance(value, str):  \n",
    "        return value.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').replace('  ', ' ').strip()\n",
    "    return value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading gab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id                                                text                        hate_speech_idx                      response                     \n",
      "0                                    1. 39869714\\r\\n  1. i joined gab to remind myself how retarded ...         [1]      [\"Using words that insult one group while defe...\n",
      "1  1. 39845588\\r\\n2. \\t39848775\\r\\n3. \\t\\t3991101...  1. This is what the left is really scared of. ...         [3]      ['You can disagree with someones opinion witho...\n",
      "2                   1. 37485560\\r\\n2. \\t37528625\\r\\n  1. It makes you an asshole.\\r\\n2. \\tGive it to...         [2]      ['Your argument is more rational if you leave ...\n",
      "3                   1. 39787626\\r\\n2. \\t39794481\\r\\n  1. So they manage to provide a whole lot of da...         [2]      [\"You shouldn't generalize a specific group or...\n",
      "4  1. 37957930\\r\\n2. \\t39953348\\r\\n3. \\t\\t3996521...  1. Hi there, i,m Keith, i hope you are doing w...         [3]      ['If someone is rude it is better to ignore th...\n",
      "5                                    1. 38462712\\r\\n                    1. you sound like a faggot \\r\\n         [1]      [\"Please be careful with the words you choose ...\n",
      "6  1. 38052531\\r\\n2. \\t38103723\\r\\n3. \\t\\t3851658...  1. Hi developers, give us a follow for updates...         [3]      [\"The words you've chosen are hateful and dero...\n",
      "7                   1. 38352488\\r\\n2. \\t38373190\\r\\n  1. Well, you are the fuckers that lit the matc...         [2]      ['Please refrain from using such horrible bigo...\n",
      "8  1. 37238116\\r\\n2. \\t38348543\\r\\n3. \\t\\t3837623...  1. SELF-HATING WHITE CUCKS ON PARADE\\r\\n2. \\tD...      [1, 3]      ['Your words are derogatory and offensive, and...\n",
      "9  1. 37358018\\r\\n2. \\t37359176\\r\\n3. \\t\\t3738104...  1. So after 6 years and nearly 11K followers, ...         [3]      [\"Woah! Please don't use such strong and offen...\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "Index(['id', 'text', 'hate_speech_idx', 'response'], dtype='object')\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "1. 39845588\n",
      "2. \t39848775\n",
      "3. \t\t39911017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_gab = pd.read_csv('gab_reddit_benchmark/gab.csv')\n",
    "\n",
    "content_gab[\"text\"] = content_gab[\"text\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "content_gab[\"response\"] = content_gab[\"response\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\")\n",
    "content_gab[\"hate_speech_idx\"] = content_gab[\"hate_speech_idx\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "\n",
    "# content_gab[\"text\"] = content_gab[\"text\"].apply(clean_special_chars)\n",
    "# content_gab[\"response\"] = content_gab[\"response\"].apply(clean_special_chars)\n",
    "\n",
    "for index, row in content_gab.iterrows():\n",
    "    row['text'] = row['text'].replace(\"'\", '\"')\n",
    "    row['response'] = row['response'].replace(\"'\", '\"')\n",
    "\n",
    "# content_gab = content_gab.applymap(clean_special_chars)\n",
    "print(content_gab.head(n=10))\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab.columns)\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab.iloc[1]['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_first_number(input_string):\n",
    "    match = re.search(r'\\d{2,}', input_string)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None\n",
    "\n",
    "content_gab['extracted_id'] = content_gab['id'].apply(get_first_number)\n",
    "\n",
    "# Find duplicate rows based on 'extracted_id'\n",
    "duplicates = content_gab[content_gab.duplicated(subset=['extracted_id'], keep=False)]\n",
    "filtered_groups = []\n",
    "grouped = content_gab.groupby('extracted_id')\n",
    "for key, group in grouped:\n",
    "    if len(group) > 1:\n",
    "        filtered_groups.append(group)\n",
    "\n",
    "merged_df = pd.concat(filtered_groups, ignore_index=True)\n",
    "\n",
    "merged_df.to_csv('gab_reddit_benchmark/gab_groups.csv', index=False)\n",
    "\n",
    "merged_df = grouped.agg({\n",
    "    'id': ' '.join,\n",
    "    'text': ' '.join,\n",
    "    'hate_speech_idx': ' '.join,\n",
    "    'response': ' '.join\n",
    "}).reset_index()\n",
    "merged_df.to_csv('gab_reddit_benchmark/gab_merged.csv')\n",
    "\n",
    "df = pd.read_csv('gab_reddit_benchmark/gab_merged.csv')\n",
    "df = df.applymap(lambda x: x.replace('] [', ', ') if isinstance(x, str) else x)\n",
    "df = df.applymap(lambda x: x.replace(']  [', ', ') if isinstance(x, str) else x)\n",
    "df = df.applymap(lambda x: 'n/a' if isinstance(x, str) and x.strip() == '' else x)\n",
    "df.to_csv('gab_reddit_benchmark/gab_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id                                                text                        hate_speech_idx                      response                     \n",
      "0  1. 37011807\\r\\n2. \\t37012801\\r\\n3. \\t\\t3701333...  1. 64% of Hispanics have IQ scores too low to ...     [2, 2, 3]    ['Racist and hate speech will not be tolerated...\n",
      "1                    1. 37012081\\r\\n 1. 37012081\\r\\n  1. LYING CUNT BITCH !!\\r\\n 1. LYING CUNT BITCH...        [1, 1]    ['Please refrain from using derogatory terms f...\n",
      "2                                    1. 37012105\\r\\n  1. Nigger Kills 2 cops https://www.google.com/...           [1]    ['The N word is highly offensive.   All ethnic...\n",
      "3                   1. 37012146\\r\\n2. \\t37013020\\r\\n  1. STABBED IN NECK   arrested but no descr   #...           [2]    ['Use of the n-word is unacceptable in our dis...\n",
      "4  1. 37012285\\r\\n2. \\t37013809\\r\\n3. \\t37013868\\r\\n  1. Retard 1st ClassÂ  w/ Cluster & Ribbon\\r\\n2....           [1]    ['We should try and not use words making fun o...\n",
      "5                                    1. 37012571\\r\\n  1. We should be happy they do fuck their cousi...           [1]    ['I cannot listen to your argument if you keep...\n",
      "6  1. 37012882\\r\\n2. \\t37013415\\r\\n3. \\t\\t3701686...  1. Stolen Valor! Prosecute the scum.\\r\\n2. \\t\\...        [3, 4]    ['Stick to your own beliefs without dismissing...\n",
      "7                                    1. 37013030\\r\\n  1. Yea, here's the scoop.  Fake News! \"We told...                                                                   \n",
      "8                                    1. 37013358\\r\\n           1. Throat-punch that faggot DeNigger\\r\\n           [1]    ['I hope that you can see that you are advocat...\n",
      "9                                    1. 37013922\\r\\n  1. https://www.youtube.com/watch?v=DmNRkp_fuoo...           [1]    ['Gender is a category that shouldnât be the b...\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "Index(['id', 'text', 'hate_speech_idx', 'response'], dtype='object')\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "1. 37011807\n",
      "2. \t37012801\n",
      "3. \t\t37013338\n",
      "4. \t\t37013511\n",
      "5. \t\t37333801\n",
      " 1. 37011807\n",
      "2. \t37012913\n",
      "3. \t\t37013738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_gab_m = pd.read_csv('gab_reddit_benchmark/gab_merged.csv')\n",
    "content_gab_m = content_gab_m.drop('Unnamed: 0', axis=1)\n",
    "content_gab_m = content_gab_m.drop('extracted_id', axis=1)\n",
    "\n",
    "\n",
    "content_gab_m[\"text\"] = content_gab_m[\"text\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "content_gab_m[\"response\"] = content_gab_m[\"response\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\")\n",
    "content_gab_m[\"hate_speech_idx\"] = content_gab_m[\"hate_speech_idx\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "\n",
    "for index, row in content_gab_m.iterrows():\n",
    "    row['text'] = row['text'].replace(\"'\", '\"')\n",
    "    row['response'] = row['response'].replace(\"'\", '\"')\n",
    "\n",
    "print(content_gab_m.head(n=10))\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab_m.columns)\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab_m.iloc[0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_text_labels(text_utterances_length, labels):\n",
    "    if not labels:\n",
    "        # return ['other'] * text_utterances_length\n",
    "        return [0] * text_utterances_length\n",
    "    new_labels = []\n",
    "    int_list = ast.literal_eval(labels)\n",
    "    for i in range(text_utterances_length):\n",
    "        if i+1 in int_list:\n",
    "            # new_labels.append('hate_speech')\n",
    "            new_labels.append(1)\n",
    "        else:\n",
    "            # new_labels.append('other')\n",
    "            new_labels.append(0)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting 'text' and 'response' into individual rows, so that I can construct a graph from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id                                                text                        text_labels                      response                       extracted_id response_labels\n",
      "0                                    1. 39869714\\r\\n  [1. i joined gab to remind myself how retarded...         [1]  [Using words that insult one group while defen...    39869714       [0, 0, 0]  \n",
      "1  1. 39845588\\r\\n2. \\t39848775\\r\\n3. \\t\\t3991101...  [1. This is what the left is really scared of....   [0, 0, 1]  [You can disagree with someones opinion withou...    39845588       [0, 0, 0]  \n",
      "2                   1. 37485560\\r\\n2. \\t37528625\\r\\n  [1. It makes you an asshole., 2. Give it to a ...      [0, 1]  [Your argument is more rational if you leave y...    37485560       [0, 0, 0]  \n",
      "3                   1. 39787626\\r\\n2. \\t39794481\\r\\n  [1. So they manage to provide a whole lot of d...      [0, 1]  [You shouldn't generalize a specific group or ...    39787626       [0, 0, 0]  \n",
      "4  1. 37957930\\r\\n2. \\t39953348\\r\\n3. \\t\\t3996521...  [1. Hi there, i,m Keith, i hope you are doing ...   [0, 0, 1]  [If someone is rude it is better to ignore the...    37957930       [0, 0, 0]  \n",
      "- - - - \n",
      "Index(['id', 'text', 'text_labels', 'response', 'extracted_id', 'response_labels'], dtype='object')\n",
      "1. 39869714\n",
      "\n",
      "[\"1. i joined gab to remind myself how retarded jew haters are. You wouldn't be typing on your abacus without them you retard.\"]\n",
      "[1]\n",
      "[\"Using words that insult one group while defending another group doesn't come across as helpful.\", 'You can make the same point more effectively without the use of hateful terminology.', 'Use of the r-word is unacceptable in our discourse as it demeans and insults people with mental disabilities.']\n",
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "text_column = []\n",
    "text_labels_column = []\n",
    "response_column = []\n",
    "response_labels_column = []\n",
    "\n",
    "for index, row in content_gab.iterrows():\n",
    "    text_utterances = row['text'].split('\\n')\n",
    "    text_utterances = list(filter(None, text_utterances))\n",
    "\n",
    "    for i, t in enumerate(text_utterances):\n",
    "        text_utterances[i] = clean_special_chars(t)\n",
    "    text_labels = mark_text_labels(len(text_utterances), row['hate_speech_idx'])\n",
    "\n",
    "    response_utterances = ast.literal_eval(row['response']) if row['response'] else []\n",
    "    for i, r in enumerate(response_utterances):\n",
    "        response_utterances[i] = clean_special_chars(r)\n",
    "    # response_labels = ['other'] * len(response_utterances)  \n",
    "    response_labels = [0] * len(response_utterances)  \n",
    "\n",
    "    \n",
    "    text_column.append(text_utterances)\n",
    "    text_labels_column.append(text_labels)\n",
    "    response_column.append(response_utterances)\n",
    "    response_labels_column.append(response_labels)\n",
    "\n",
    "content_gab['text'] = text_column\n",
    "content_gab['hate_speech_idx'] = text_labels_column\n",
    "content_gab['response'] = response_column\n",
    "content_gab['response_labels'] = response_labels_column\n",
    "\n",
    "content_gab = content_gab.rename(columns={'hate_speech_idx': 'text_labels'})\n",
    "print(content_gab.head())\n",
    "print('- - - - ')\n",
    "print(content_gab.columns)\n",
    "\n",
    "for index, row in content_gab.iterrows():\n",
    "    if index == 1:\n",
    "        continue\n",
    "    print(row['id'])\n",
    "    print(row['text'])\n",
    "    print(row['text_labels'])\n",
    "    print(row['response'])\n",
    "    print(row['response_labels'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_encoder = LabelEncoder()\n",
    "# content_gab['all_labels'] = content_gab['text_labels'] + content_gab['response_labels']\n",
    "# content_gab['all_labels_encoded'] = content_gab['all_labels'].apply(label_encoder.fit_transform)\n",
    "# print(content_gab.iloc[0])\n",
    "# content_gab['text_labels_encoded'] = content_gab['text_labels'].apply(label_encoder.fit_transform)\n",
    "# content_gab['response_labels_encoded'] = content_gab['response_labels'].apply(label_encoder.fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating BERT encoding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def generate_embeddings(sentences):\n",
    "    if isinstance(sentences, list):\n",
    "        return bert.encode(sentences, show_progress_bar=True).tolist()\n",
    "    elif isinstance(sentences, str):\n",
    "        return bert.encode([sentences], show_progress_bar=True).tolist()\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 13.10it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 29.53it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 22.52it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 20.04it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 26.88it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 78.18it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.98it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 27.84it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 25.98it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 15.90it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 45.33it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 50.79it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 22.56it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 55.91it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 46.25it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.44it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 56.19it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.81it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 21.20it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 62.22it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.93it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 60.61it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 54.23it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 52.65it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 76.89it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 29.92it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 59.54it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 74.28it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 23.17it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 16.13it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 59.02it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 57.64it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 40.66it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.14it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.78it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 26.86it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.67it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 15.70it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 15.86it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 22.80it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 17.75it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 53.71it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 24.58it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 30.68it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 18.58it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.80it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 71.04it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 68.73it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  6.16it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.37it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 46.67it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.91it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 18.83it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  8.78it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 46.61it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 25.94it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.53it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 21.92it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.37it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 57.79it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 29.44it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  1.21it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 62.26it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  6.45it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 52.61it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 16.08it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 42.27it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 29.47it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 52.62it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 31.83it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 20.13it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 56.55it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 24.63it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 14.90it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 19.29it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.44it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  4.52it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 50.37it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 52.81it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  8.38it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.70it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 15.91it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  4.12it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 12.77it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 27.75it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 12.63it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 17.03it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 16.44it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 58.76it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 77.36it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 43.04it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 27.92it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 17.40it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 19.63it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 25.95it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 40.93it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 45.22it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 10.23it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  5.67it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 39.67it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 50.84it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 15.04it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 52.91it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 44.98it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 27.93it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 49.59it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 44.16it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 18.95it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 14.94it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.72it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 28.28it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 28.39it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  6.98it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 28.03it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 17.20it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 11.17it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 25.57it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 56.39it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 25.34it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 59.58it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 28.45it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 15.76it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 43.59it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 60.96it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.82it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.99it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 40.25it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.20it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 48.20it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  8.41it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  8.67it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 41.01it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 65.54it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 19.10it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 30.85it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 26.23it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 31.70it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 19.27it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 32.73it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 68.23it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 26.50it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 26.59it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 24.95it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 43.13it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 59.03it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  4.05it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 16.93it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 47.25it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 13.91it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 40.28it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 31.89it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 25.18it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 28.76it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  2.95it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 25.85it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.65it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 49.63it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 21.03it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 25.49it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 48.11it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 41.63it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 51.89it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 40.98it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  8.73it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 31.12it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 27.54it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 90.30it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  9.24it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 50.65it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 40.59it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 11.98it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 46.20it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 61.93it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 31.44it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 17.33it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 17.10it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  4.32it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 27.03it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 20.56it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 20.39it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 62.50it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 12.55it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 57.03it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 17.46it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 19.11it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 25.47it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 31.84it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 46.25it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 52.07it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 72.67it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00,  4.95it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 13.70it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 30.14it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 40.60it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 134.89it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.13it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 23.81it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 31.33it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 17.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.023572057485580444, 0.01794440858066082, 0.0405656173825264, 0.06729649752378464, 0.09804009646177292, 0.035114046186208725, 0.06723955273628235, -0.07981331646442413, 0.012592255137860775, -0.06395190209150314, 0.014613417908549309, -0.028686098754405975, 0.06557455658912659, -0.05138685926795006, -0.1029239371418953, 0.015551798976957798, -0.06676264107227325, -0.0029045091941952705, -0.027871334925293922, 0.0603627972304821, -0.027235597372055054, 0.02632732316851616, 0.03128805756568909, 0.017424048855900764, 0.013893608935177326, -0.06276204437017441, -0.013789285905659199, -0.01572624407708645, -0.03531145676970482, -0.05476396158337593, 0.013463081791996956, -0.028276406228542328, -0.03120226040482521, -0.054336175322532654, -0.011610085144639015, -0.04129831865429878, 0.10698013752698898, -0.050249602645635605, -0.02981388196349144, 0.06209121271967888, -0.017547253519296646, -0.01501332875341177, 0.08790478855371475, 0.07822359353303909, -0.09159855544567108, 0.04815983772277832, -0.06916674971580505, 0.05322776734828949, -0.024256447330117226, -0.09537238627672195, -0.03579239547252655, -0.04258187115192413, -0.08481573313474655, 0.031696710735559464, 0.03722597286105156, -0.0579882487654686, -0.059830546379089355, 0.014472158625721931, 0.03947818651795387, 0.014918514527380466, 0.0374993272125721, 0.023774776607751846, 0.03108145482838154, 0.009576061740517616, -0.02806719020009041, -0.0549272745847702, 0.039897371083498, -0.08561886101961136, -0.05795484781265259, 0.08154644817113876, -0.0017612621886655688, 0.01556019764393568, -0.057281170040369034, -0.09787726402282715, 0.013083030469715595, -0.018553076311945915, 0.007453093770891428, -0.019630098715424538, -0.054940689355134964, -0.0953664556145668, 0.026371384039521217, -0.006277969107031822, 0.10078250616788864, -0.012945770286023617, 0.014498650096356869, 0.017524247989058495, -0.04861271753907204, -0.014787748456001282, 0.01737138070166111, 0.05227299779653549, 0.02295733243227005, 0.001686697592958808, 0.07582608610391617, 0.017705759033560753, -0.0015716354828327894, -0.002998440060764551, -0.0462193563580513, 0.024339446797966957, -0.04856687784194946, 0.047270383685827255, -0.016229182481765747, -0.0022451779805123806, 0.0593828521668911, 0.04684929922223091, -0.02564956247806549, 0.01817881129682064, -0.04995886981487274, -0.01995387114584446, -0.06652817130088806, 0.000959291122853756, -0.05363905429840088, -0.018837444484233856, -0.04111718386411667, -0.0021198345348238945, -0.040226440876722336, -0.033901579678058624, 0.017404159530997276, -0.007449411787092686, 0.013669039122760296, 0.03172919526696205, 0.013172303326427937, -0.006265211850404739, 0.017268143594264984, -0.033285729587078094, 0.08197329193353653, -0.06818144023418427, -0.12145192176103592, -5.181774891702452e-33, 0.07206414639949799, -0.10233228653669357, -0.03951829671859741, 0.0013451091945171356, 0.07297976315021515, 0.04217559099197388, -0.08489369601011276, 0.03341776132583618, -0.03820201754570007, 0.033236436545848846, -0.04697813838720322, -0.0037153016310185194, -0.04916464164853096, -0.028877070173621178, 0.0029163137078285217, -0.0770014077425003, 0.03615826740860939, 0.021831408143043518, -0.0713278129696846, -0.056093573570251465, -0.00012905545008834451, 0.10050742328166962, -0.011927759274840355, 0.03246558830142021, 0.0372706800699234, 0.0172326248139143, -0.005269011482596397, 0.024150235578417778, 0.046569254249334335, 0.02438509836792946, -0.12742935121059418, 0.013551454059779644, -0.04947509244084358, -0.029007917270064354, 0.009385428391397, 0.06105904281139374, 0.005882322322577238, -0.054570652544498444, -0.05731366574764252, -0.09142463654279709, -0.029561210423707962, 0.03298034146428108, -0.09254152327775955, 0.014611337333917618, 0.06021145358681679, 0.03362573683261871, -0.017116008326411247, -0.015008974820375443, -0.004058348946273327, 0.021523673087358475, -0.018500056117773056, 0.07692427933216095, 0.07741913199424744, 0.05040502920746803, 0.05784923583269119, -0.07141830027103424, 0.0095024099573493, 0.04143378138542175, -0.04927181825041771, 0.00903945043683052, -0.0006089740199968219, 0.03323495015501976, 0.060909807682037354, -0.055581800639629364, -0.013429153710603714, 0.04045286774635315, -0.018049901351332664, -0.07930324226617813, 0.02816927805542946, 0.04188494384288788, -0.02306269481778145, 0.0070374091155827045, 0.048659682273864746, 0.06050277128815651, -0.04568563401699066, -0.005206800531595945, -0.006128524895757437, 0.024518268182873726, 0.006941821426153183, -0.04208432883024216, 0.0421958863735199, -0.03775396570563316, 0.010521989315748215, 0.0410795696079731, -0.012251405045390129, -0.040389999747276306, 0.012214913964271545, -0.1506490558385849, 0.010156752541661263, 0.03339754790067673, 0.0025313724763691425, -0.022614017128944397, 0.053126584738492966, -0.04465081915259361, -0.05430527403950691, 1.773386777164997e-33, -0.11583428829908371, 0.010014028288424015, -0.04472268000245094, 0.043904632329940796, -0.06872110068798065, 0.02242632769048214, 0.1263907253742218, 0.05892787128686905, 0.061748016625642776, -0.009931759908795357, 0.020472878590226173, 0.02589953877031803, -0.05042511224746704, 0.013905087485909462, -0.010195986367762089, -0.02641150914132595, 0.12776246666908264, 0.006280369125306606, -0.011610600166022778, 0.021490734070539474, -0.052388232201337814, 0.07279004156589508, -0.07466274499893188, 0.14213718473911285, -0.040154829621315, 0.05863190069794655, 0.03182356432080269, -0.020553387701511383, 0.0016153521137312055, -0.05780056118965149, -0.03013593703508377, -0.020728135481476784, 0.04495265707373619, 0.06770161539316177, -0.038545697927474976, -0.03968843072652817, -0.016269374638795853, 0.013275776989758015, 0.04998328909277916, -0.05824997276067734, 0.10717650502920151, -0.01973535120487213, 0.004112995695322752, -0.029574410989880562, -0.05759349465370178, 0.06029912829399109, 0.04811272770166397, 0.08564700186252594, -0.07532630860805511, -0.0437580831348896, -0.014880255796015263, 0.05099138990044594, -0.03473413735628128, -0.019710764288902283, -0.004233859479427338, -0.0840367004275322, -0.049637891352176666, -0.008992845192551613, 0.047729380428791046, 0.1066170185804367, -0.02718396484851837, 0.09788494557142258, -0.06811606884002686, 0.020569635555148125, 0.03360128402709961, -0.05402734875679016, -0.0499679334461689, 0.07933534681797028, 0.04568912461400032, -0.031186381354928017, 0.06635911762714386, 0.0014810968423262239, -0.06889209151268005, -0.042748868465423584, 0.0004744485195260495, -0.029906777665019035, 0.027813278138637543, -0.00603790208697319, 0.021638398990035057, -0.003204121720045805, 0.04991651326417923, -0.0588565468788147, -0.02456951141357422, 0.03823620826005936, 0.07614948600530624, 0.06207279488444328, 0.1086224764585495, 0.003921786788851023, 0.047083284705877304, -0.0036237433087080717, 0.029689164832234383, 0.08975242078304291, -0.016617093235254288, 0.09280365705490112, 0.07856971770524979, -3.2214195755386754e-08, 0.07493723928928375, -0.04131816327571869, -0.00918601080775261, -0.0634353905916214, 0.046850062906742096, 0.060324110090732574, -0.01446310430765152, 0.04866188392043114, -0.11761873960494995, 0.06345327943563461, 0.04961284250020981, 0.038172051310539246, -0.0397590734064579, -0.04137649014592171, -0.06160252168774605, 0.07888319343328476, -0.11216012388467789, -0.01504907850176096, 0.003678123466670513, -0.04987558349967003, 0.013319593854248524, -0.007126920856535435, -0.05422540381550789, -0.030421504750847816, -0.04588665813207626, 0.07505374401807785, -0.1035812571644783, -0.004367319401353598, -0.009592040441930294, 0.06975304335355759, 0.010770153254270554, -0.07311354577541351, -0.03480406105518341, 0.02835475467145443, -0.013685842975974083, -0.026222359389066696, 0.01925831474363804, 0.02957685850560665, 0.042983733117580414, -0.019282333552837372, 0.03362332656979561, -0.02270379476249218, 0.04089226946234703, 0.014580373652279377, -0.10697577893733978, 0.010506035760045052, -0.039530012756586075, -0.10012613236904144, 0.03301755338907242, -0.04779036343097687, -0.029263583943247795, 0.000636313867289573, 0.00781335961073637, 0.08117810636758804, 0.04891378805041313, -0.027057385072112083, 0.012799398973584175, 0.07448851317167282, -0.008968248032033443, 0.07786200940608978, 0.07612894475460052, 0.06673554331064224, -0.03327760845422745, 0.11221782118082047], [-0.09077426791191101, -0.004321129526942968, 0.048581358045339584, 0.01600240357220173, 0.063059963285923, 0.007225271314382553, 0.1289822906255722, 0.03402089700102806, 0.07319037616252899, 0.06843877583742142, 0.004195555113255978, -0.08952050656080246, -0.027654731646180153, -0.054983608424663544, 0.007451079785823822, -0.005147560965269804, -0.07560396194458008, -0.0010118413483723998, 0.01611790619790554, 0.015321220271289349, 0.06880339980125427, 0.027464482933282852, 0.058212801814079285, -0.013553108088672161, -0.045118093490600586, -0.006279462948441505, -0.0068413130939006805, 0.0029208168853074312, -0.03365723788738251, -0.011803730390965939, 0.04119858890771866, -0.04805603623390198, -0.004039316438138485, -0.002998518757522106, -0.08565224707126617, -0.009894272312521935, 0.11572100222110748, -0.027901334688067436, 0.0383949801325798, 0.022395724430680275, -0.018354257568717003, 0.02843811921775341, 0.009978462010622025, 0.00012173593131592497, 0.061500441282987595, 0.062968909740448, -0.02182989940047264, 0.1005239263176918, 0.00426061125472188, -0.09350602328777313, -0.02023398131132126, 0.014279811643064022, -0.0464613102376461, 0.009929378516972065, -0.07782071828842163, 0.06705363094806671, -0.03904207423329353, -0.009611629880964756, 0.04601975530385971, 0.07742089778184891, 0.011429923586547375, -0.0013719657436013222, 0.014855190180242062, 0.04563784971833229, 0.012635408900678158, -0.025114409625530243, -0.05547075346112251, -0.13479098677635193, 0.019971735775470734, 0.04321139305830002, 0.03492508828639984, -0.052899450063705444, 0.010165165178477764, -0.018294615671038628, -0.057120975106954575, -0.14432503283023834, 0.07252795249223709, -0.020182989537715912, -0.006986842956393957, -0.0025313899386674166, -0.05025387555360794, -0.03976421430706978, 0.10722938925027847, 0.01447085291147232, -0.013130540028214455, 0.0941372737288475, -0.0973615050315857, -0.03282913193106651, -0.1397237926721573, 0.02238614670932293, -0.02385745197534561, 0.00989785511046648, 0.09143147617578506, 0.0345420315861702, 0.04380427673459053, 0.0009382412536069751, -0.06505825370550156, 0.07799241691827774, 0.012868504039943218, 0.10730026662349701, -0.019915755838155746, -0.021757127717137337, 0.041546404361724854, -0.008801428601145744, 0.040621932595968246, -0.009801342152059078, -0.04056771844625473, -0.06910770386457443, 0.032480984926223755, -0.03244447335600853, 0.02478601224720478, -0.01928533986210823, -0.04623979330062866, 0.028794879093766212, 0.004943212494254112, 0.0061772963963449, 0.02729879505932331, -0.05179404467344284, -0.05644158273935318, 0.005550819914788008, 0.017456460744142532, 0.04696749150753021, -0.02875385619699955, -0.08048640936613083, 0.03437250107526779, -0.1070290356874466, -0.05876627191901207, -5.352898951191345e-33, -0.006287301890552044, -0.013408520258963108, 0.08930912613868713, -0.03146149218082428, 0.07935462146997452, 0.10619652271270752, -0.05945824459195137, 0.008286995813250542, 0.08319014310836792, 0.06524445116519928, -0.04254871979355812, -0.014957445673644543, -0.09232839941978455, 0.07507062703371048, -0.04178762435913086, 0.06916014850139618, 0.008016485720872879, 0.043761998414993286, -0.05831659585237503, -0.0102305943146348, 0.04148801043629646, 0.044439125806093216, -0.003480916377156973, -0.01428033784031868, -0.04202083498239517, 0.04659470170736313, 0.02901596948504448, -0.04187263175845146, 0.07494302093982697, 0.04662925377488136, -0.06684386730194092, -0.015876440331339836, -0.025631053373217583, -0.014308995567262173, -0.058740127831697464, -0.11437097936868668, 0.08765881508588791, -0.08930796384811401, -0.02531752549111843, 0.010946551337838173, -0.004341466818004847, 0.027367355301976204, -0.052350692451000214, 0.08464670926332474, -0.025068633258342743, 0.04946338012814522, 0.062043800950050354, 0.017245309427380562, 0.0036487241741269827, 0.0818067193031311, 0.044673021882772446, 0.04281897470355034, 0.11324352025985718, 0.032398711889982224, 0.03701508417725563, -0.01307677011936903, -0.0472821407020092, 0.05446520075201988, -0.008659145794808865, 0.011441816575825214, 0.03761905059218407, -0.0072248224169015884, 0.04716646298766136, 0.030068086460232735, 0.052578188478946686, -0.04520205035805702, -0.012098746374249458, -0.016033926978707314, 0.03551924228668213, -0.002098334953188896, 0.0681239441037178, 0.06064704805612564, 0.003167615504935384, -0.06187468394637108, 0.006869085598737001, -0.04608903452754021, 0.009950915351510048, 0.04881925508379936, 0.038603801280260086, 0.01621711440384388, -0.00634901225566864, 0.06591642647981644, 0.014696729369461536, -0.002329253824427724, -0.02188892476260662, -0.01795201748609543, 0.026980668306350708, -0.015907801687717438, -0.025598352774977684, -0.032206375151872635, 0.08449702709913254, -0.022893881425261497, -0.04050298035144806, -0.0730055496096611, -0.030852507799863815, 1.807377114673985e-33, -0.05230112746357918, 0.01419652346521616, -0.044968269765377045, 0.017020784318447113, 0.0036471830680966377, -0.05043288320302963, 0.06829755008220673, 0.025683019310235977, -0.04125538840889931, 0.0013852815609425306, 0.006468631327152252, -0.07029663026332855, -0.009580693207681179, -0.013528102077543736, 0.023790497332811356, -0.036890722811222076, -0.005132612772285938, 0.027188455685973167, -0.02819216437637806, -0.009005936793982983, 0.020995551720261574, -0.0073378970846533775, -0.036337707191705704, 0.016526078805327415, -0.11412344872951508, 0.026558099314570427, 0.02083509974181652, -0.04809976741671562, -0.007241069804877043, 0.0026464120019227266, -0.044935520738363266, 0.0644175186753273, -0.022319423034787178, -0.07922090590000153, 0.06910736858844757, -0.00498548848554492, -0.08609599620103836, -0.003845705185085535, 0.024758044630289078, -0.019033806398510933, 0.04496239125728607, -0.015579553321003914, -0.04503335431218147, -0.043414950370788574, 0.04773915931582451, -0.0810801312327385, -0.024714183062314987, -0.056396860629320145, -0.07916368544101715, 0.01937537081539631, -0.07004602998495102, -0.06934064626693726, -0.04161323979496956, -0.09426996111869812, -0.07260120660066605, -0.03644664213061333, -0.14986597001552582, -0.009570317342877388, 0.04930362477898598, 0.11612330377101898, 0.022939175367355347, -0.04145437479019165, -0.08526183664798737, 0.07278040796518326, -0.0362955704331398, -0.02532808855175972, 0.006364536006003618, 0.08927850425243378, 0.026802847161889076, -0.05549145117402077, 0.0717470645904541, -0.02297559566795826, -0.01257498562335968, -0.015541987493634224, -0.03273480758070946, 0.016138987615704536, 0.04726500064134598, -0.06111883372068405, 0.056073036044836044, 0.01830550841987133, -0.08866332471370697, -0.08189428597688675, 0.03104999288916588, 0.08009018003940582, -0.0035104325506836176, -0.021703621372580528, 0.012455567717552185, 0.03309236839413643, -0.03735201433300972, 0.07492285221815109, 0.06298674643039703, 0.03162549436092377, -0.08552482724189758, 0.054921168833971024, -0.01661379262804985, -2.6253188067926203e-08, -0.005043746903538704, 0.0973062813282013, 0.05342255160212517, -0.018845725804567337, 0.0655365139245987, 0.09893176704645157, -0.04412873089313507, -0.05685996636748314, -0.04830349236726761, -0.032440170645713806, -0.013385062105953693, -0.006123469211161137, -0.025111297145485878, 0.029516924172639847, -0.00611607450991869, 0.08838699758052826, -0.12799589335918427, 0.014528878964483738, -0.01966588944196701, 0.0338340625166893, 0.01370571181178093, 0.03742438554763794, -0.013599760830402374, -0.015512016601860523, -0.03373889997601509, 0.027643835172057152, -0.06709185242652893, 0.11620388180017471, -0.028457509353756905, 0.09794431924819946, 0.025078492239117622, 0.05181735381484032, -0.08401434123516083, -0.0037119262851774693, 0.03724723681807518, -0.09510159492492676, -0.012896780855953693, -0.008086774498224258, 0.009548542089760303, -0.0601385235786438, 0.008680700324475765, -0.002830189187079668, 0.019955022260546684, 0.05506434664130211, 0.053104497492313385, 0.02636396326124668, -0.021042732521891594, -0.07735630124807358, -0.04009256884455681, -0.027792172506451607, -0.027644142508506775, -0.08522052317857742, -0.02516891621053219, 0.1101594865322113, -0.04527197405695915, -0.047056082636117935, -0.01671939343214035, 0.022366860881447792, -0.03911849856376648, -0.003360369708389044, -0.005907749757170677, 0.011773264035582542, -0.04848779737949371, 0.0395224504172802], [0.008080711588263512, -0.09539888799190521, -0.030773358419537544, -0.022631937637925148, -0.07728615403175354, 0.032472867518663406, 0.08644342422485352, -0.01748623326420784, 0.023008596152067184, 0.026931732892990112, -0.012913193553686142, -0.09954977035522461, -0.025455322116613388, -0.06620162725448608, -0.0704817920923233, 0.04873642325401306, -0.07056586444377899, -0.10960452258586884, -0.00981439184397459, -0.0803423523902893, 0.029831750318408012, 0.021925145760178566, 0.09854550659656525, 0.0009720309171825647, 0.0050886026583611965, -0.035403184592723846, 0.012405410408973694, -0.022730467841029167, -0.027505310252308846, -0.002322605811059475, -0.011340518482029438, 0.05622837319970131, 0.02918357588350773, -0.014123586937785149, -0.011095233261585236, -0.038050513714551926, -0.04091372340917587, 0.032156046479940414, -0.005078315734863281, 0.05239526182413101, 0.028330672532320023, -0.06980035454034805, 0.036486219614744186, 0.02715999074280262, 0.04318193718791008, -0.038291025906801224, 0.03963205963373184, -0.027299772948026657, 0.01866263523697853, -0.051943860948085785, 0.016716858372092247, 0.07359474152326584, -0.032110586762428284, 0.0520259328186512, -0.003587504383176565, -0.07705038040876389, -0.049507979303598404, 0.03334396705031395, -0.05955389142036438, 0.06343350559473038, 0.07417023181915283, -0.04209043085575104, 0.0030464634764939547, -0.011782951653003693, 0.051098957657814026, -0.0001350410602753982, -0.08637798577547073, -0.12473417073488235, -0.04188784211874008, 0.07785394787788391, 0.03680673986673355, 0.04839127138257027, 0.00613388093188405, -0.024020614102482796, -0.02700227126479149, -0.05617436021566391, 0.022975469008088112, -0.0688067376613617, 0.03747405484318733, 0.025554560124874115, -0.056267786771059036, 0.07052543759346008, 0.018991312012076378, 0.04184054955840111, -0.0028655303176492453, -0.07721622288227081, 0.025495855137705803, 0.009666228666901588, -0.0731314867734909, 0.08246062695980072, 0.06467888504266739, 0.010289357975125313, 0.14305506646633148, 0.048000648617744446, -0.03249433636665344, 0.023189904168248177, 0.015835512429475784, -0.03490106388926506, -0.04355061054229736, 0.07404258847236633, 0.03346407413482666, 0.10190625488758087, 0.0360240675508976, 0.03428279608488083, 0.09003105759620667, 0.07280085235834122, 0.02414071187376976, 0.020896954461932182, -0.09953875094652176, 0.009472835808992386, -0.05451289936900139, -0.07355302572250366, 0.06916225701570511, -0.0020785978995263577, -0.03501376882195473, 0.06568204611539841, 0.03277268260717392, 0.04597487300634384, 0.08742585778236389, -0.07478149980306625, 0.10328702628612518, -0.031894050538539886, 0.012014864943921566, 0.007906675338745117, -0.032022442668676376, -0.05379030108451843, -0.0037271471228450537, 1.8279231028874276e-34, 0.0050093187019228935, 0.03918766975402832, -0.02847474254667759, 0.04069356992840767, 0.04589221253991127, 0.04610351473093033, -0.026543304324150085, -0.048183612525463104, 0.02088586799800396, -0.0020288110245019197, 0.0816764235496521, -0.01139878947287798, 0.00022294383961707354, -0.001890053739771247, 0.02073173224925995, 0.027862954884767532, 0.017902810126543045, 0.08875707536935806, 0.014715389348566532, -0.07468044757843018, -0.048698876053094864, 0.07993558794260025, -0.06348960846662521, 0.0655917152762413, -0.04011177271604538, -0.005668213590979576, 0.007279981393367052, -0.05056377872824669, -0.012413766235113144, -0.002010062336921692, 0.008123446255922318, 0.046994682401418686, 0.05535564199090004, 0.02034623920917511, -0.0012217710027471185, -0.10184323787689209, 0.10169916599988937, -0.034093618392944336, -0.0309029258787632, 0.008021282963454723, -0.01502931397408247, 0.0008118023397400975, 0.046915724873542786, 0.012176107615232468, 0.054647140204906464, 0.04477979615330696, -0.009253396652638912, -0.008242934010922909, 0.009977003559470177, 0.017530161887407303, 0.010699549689888954, 0.03754172846674919, 0.05122923478484154, 0.06261452287435532, 0.025533275678753853, -0.0037065905053168535, 0.06957687437534332, -0.010279888287186623, 0.02615094929933548, 0.0020822437945753336, 0.0038684711325913668, -0.020775988698005676, 0.012700295075774193, -0.04121045023202896, -0.08708054572343826, -0.05266943201422691, -0.04988298565149307, 0.04015504568815231, 2.3344787223322783e-06, 0.0674474909901619, -0.02008383721113205, 0.0499715581536293, -0.0674726590514183, -0.08824130147695541, -0.035174135118722916, -0.06264480948448181, 0.04031972587108612, -0.02248426340520382, -0.03776630014181137, -0.12003500014543533, -0.016910450533032417, 0.09409449994564056, -0.020762139931321144, 0.05564181134104729, -0.06826725602149963, 0.03621707111597061, -0.03261701762676239, -0.06822937726974487, 0.09408803284168243, -0.0367269329726696, -0.021736176684498787, 0.009966426528990269, 0.023078233003616333, -0.008611335419118404, -0.0619976669549942, 8.941951547571563e-34, 0.00036352992174215615, 0.012690436094999313, 0.026589035987854004, 0.06478539109230042, 0.02548210322856903, 0.02860984206199646, 0.0012570017715916038, 0.05658910050988197, 0.0781116709113121, 0.020748967304825783, 0.016074134036898613, -0.05106382817029953, 0.03079255484044552, -0.056950632482767105, 0.12897665798664093, 0.008377077989280224, 0.05113306641578674, -0.031216779723763466, -0.045274753123521805, -0.004246346186846495, 0.04971003904938698, -0.03990734741091728, -0.02811526693403721, 0.04774903878569603, -0.049286723136901855, 0.07545077055692673, -0.03053143434226513, -0.07274259626865387, 0.015440759249031544, -0.0017576440004631877, 0.03929729759693146, -0.045735422521829605, 0.03634228557348251, 0.03500661998987198, 0.0062796282581985, -0.017700696364045143, 0.0714467316865921, 0.040730610489845276, -0.0394870862364769, 0.07122262567281723, 0.11299610882997513, -0.013451826758682728, 0.017438828945159912, 0.030202679336071014, -0.005799288395792246, -0.00021199458569753915, 0.04479778930544853, 0.02060711570084095, -0.05177623778581619, 0.039304252713918686, -0.06698282063007355, -0.02119085192680359, -0.08656778186559677, -0.029952237382531166, 0.023406168445944786, -0.010621941648423672, -0.00694050220772624, -0.02704007364809513, 0.0721914991736412, -0.023389747366309166, -0.0023735947906970978, -0.01837790198624134, -0.001989508979022503, 0.03238983824849129, 0.021187754347920418, -0.03936571255326271, -0.08798239380121231, 0.136001855134964, -0.06605536490678787, -0.06085571274161339, -0.04243520647287369, 0.09640292078256607, 0.04086538404226303, -0.09723318368196487, 0.006801843177527189, -0.008721238933503628, -0.09353083372116089, 0.0005397840868681669, 0.007901034317910671, 0.05311150848865509, -0.10052897781133652, -0.03189704194664955, 0.03413189575076103, 0.0037660994566977024, -0.04544129595160484, -0.03456147015094757, 0.18371503055095673, -0.005453404039144516, 0.039594415575265884, -0.025877341628074646, 0.04954717680811882, 0.025953145697712898, -0.02908100187778473, 0.03526001051068306, -0.005633258726447821, -1.9027325492970704e-08, 0.01245121005922556, 0.05363508313894272, -0.03357085958123207, -0.07271043956279755, -0.008326188661158085, 0.05630642548203468, -0.0463874451816082, 0.08589570224285126, 0.04216998443007469, 0.03112049587070942, 0.057772446423769, -0.03909040614962578, 0.014506219886243343, 0.022036977112293243, -0.04963081702589989, 0.06102835014462471, 0.017512403428554535, 0.011143183335661888, -0.014998376369476318, 0.020809488371014595, -0.012584054842591286, 0.020533466711640358, -0.016007278114557266, 0.04199577495455742, -0.05905057489871979, 0.08003123849630356, -0.054228730499744415, 0.09618335217237473, -0.044407665729522705, 0.07981469482183456, -0.004549198783934116, 0.02889895811676979, -0.05525488033890724, -0.049912940710783005, 0.019717350602149963, 0.002761115552857518, -0.011931728571653366, -0.052367448806762695, 0.018666556105017662, 0.08221228420734406, 0.015605738386511803, -0.02665865607559681, -0.027583174407482147, -0.0503116250038147, -0.028661265969276428, -0.010513977147638798, -0.07250862568616867, -0.0686410516500473, -0.08933313935995102, -0.130082368850708, -0.07196871191263199, -0.009833923541009426, 0.02290228195488453, 0.07230091094970703, 0.14229951798915863, -0.0970807895064354, -0.03360007703304291, -0.006086405366659164, -0.10948299616575241, -0.017041537910699844, 0.028167875483632088, 0.03441556170582771, 0.011432717554271221, 0.0065767802298069]]\n",
      "\n",
      "TIME FOR TEXT EMBEDDINGS:  12.733107805252075\n",
      "\n",
      "- - - - - -\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.94it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 39.47it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.57it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.04it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 46.29it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 29.97it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 39.25it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 50.37it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.73it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.13it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 46.73it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.27it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 43.07it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 45.24it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.69it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 51.63it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.20it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.27it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 46.88it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 42.13it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 46.02it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.30it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 42.51it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.29it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.70it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 39.65it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.03it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.42it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 31.66it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 32.64it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 39.60it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.88it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.03it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.67it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.84it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 44.44it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.20it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.27it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 18.34it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 46.08it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.01it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.05it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.92it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 30.37it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 41.76it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 44.21it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 39.65it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 42.43it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 44.09it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 45.99it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 31.90it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 48.75it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 47.36it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 39.18it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 53.48it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.89it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 39.68it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 54.13it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.45it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 41.37it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 42.31it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.30it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 43.73it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.64it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 42.60it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 49.69it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.19it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.08it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 44.38it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 14.06it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 71.91it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.69it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 42.56it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 43.69it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 53.48it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 88.22it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 45.88it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 45.77it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 50.36it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 63.58it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.82it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 46.04it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.51it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.24it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 47.80it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.77it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 43.20it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.69it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 42.08it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 43.57it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 50.41it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 62.43it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.99it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 39.93it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 39.15it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 43.36it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 28.67it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.96it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 39.42it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 41.20it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 54.22it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 45.99it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 45.78it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 41.79it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.97it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 52.15it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.41it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.05it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.98it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 31.89it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.45it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.62it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 41.06it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.05it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 44.99it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 40.35it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 43.80it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 40.67it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 53.29it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.71it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 44.58it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 53.02it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 56.87it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 49.77it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 40.36it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 46.18it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 44.12it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 52.69it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 41.82it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 48.15it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.84it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 32.35it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 41.86it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.41it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.42it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 29.70it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 31.67it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.46it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 47.78it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.15it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.75it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 40.83it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 57.27it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 45.60it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 44.26it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 12.03it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.79it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.75it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.30it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 28.52it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.99it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.12it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.25it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 44.48it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 39.26it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.07it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.26it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 28.85it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.02it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.87it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 39.88it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 40.57it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.18it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.64it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 40.50it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.72it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 28.31it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 30.16it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.16it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.51it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 33.39it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 53.62it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.27it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.85it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 36.58it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.16it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.02it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.22it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 38.25it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 44.11it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 42.28it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 37.96it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 44.44it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 30.60it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 34.61it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 45.23it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 16.23it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 35.86it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 73.60it/s]\n",
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 59.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09269341826438904, 0.0053440057672560215, -0.021850084885954857, -0.0007701154099777341, 0.04873264953494072, 0.028574705123901367, 0.06419847160577774, -0.009560780599713326, 0.053361181169748306, -0.0879056453704834, -0.014494847506284714, -0.0013367488281801343, 0.0982813909649849, -0.013169684447348118, 0.03653031960129738, 0.08743903785943985, 0.05540039762854576, 0.001535855932161212, -0.03637176379561424, 0.00894644483923912, -0.08317050337791443, 0.09316875785589218, -0.046798184514045715, -0.020233850926160812, -0.0499594546854496, -0.0417790450155735, -0.015589868649840355, 0.033974144607782364, -0.014390970580279827, 0.1086299866437912, 0.020122447982430458, -0.013854089193046093, 0.04655636101961136, 0.014802754856646061, 0.0009073888068087399, -0.0037359357811510563, 0.00721629848703742, 0.047422803938388824, 0.015439050272107124, 0.013636752963066101, 0.012506725266575813, 0.008980403654277325, -0.002120441058650613, -0.07505074888467789, -0.0013401504838839173, -0.0007081580115482211, -0.03652456775307655, 0.029818832874298096, 0.026721274480223656, -0.08640063554048538, -0.08417931944131851, 0.006127768661826849, -0.059236589819192886, 0.05942682549357414, 0.005135268904268742, 0.0057946257293224335, -0.007825972512364388, 0.11261799186468124, -0.020295370370149612, 0.028986863791942596, 0.05184929072856903, -0.02701093629002571, -0.002103114500641823, -0.04580675810575485, 0.012710793875157833, 0.09872577339410782, 0.03393743559718132, 0.06304049491882324, -0.07262536138296127, 0.07092614471912384, -0.046225689351558685, 0.10023762285709381, 0.11196377128362656, 0.01841040328145027, -0.0037346261087805033, -0.014729264192283154, 0.02416234090924263, -0.04097258299589157, 0.04131739214062691, -0.022084109485149384, -0.016477545723319054, -0.012203256599605083, -0.04959903284907341, -0.0759759172797203, -0.017102191224694252, -0.0021037363912910223, 0.04519075155258179, -0.018981702625751495, 0.013932647183537483, 0.02007799968123436, 0.006097547244280577, 0.03774693235754967, 0.019818266853690147, -0.05266401544213295, 0.06843788176774979, -0.045102011412382126, -0.12242233008146286, 0.04205157607793808, -0.023320388048887253, 0.04568549618124962, 0.017341947183012962, -0.014790158718824387, -0.027331937104463577, -0.017455192282795906, 0.07769674807786942, -0.020747004076838493, -0.08738737553358078, -0.03205586224794388, -0.09111244231462479, 0.03686106950044632, -0.03311072662472725, -0.040134649723768234, -0.016983695328235626, -0.004547488410025835, 0.03450160473585129, 0.07777281105518341, 0.08191961795091629, 0.0609041228890419, -0.02624419517815113, 0.008872183971107006, 0.03701314330101013, 0.04779651015996933, -0.026972465217113495, 0.15925352275371552, -0.027176465839147568, -0.07614260911941528, -0.0623168908059597, -2.571606032419879e-33, -0.04442668333649635, 0.007310524582862854, -0.03771062195301056, -0.032474834471940994, 0.019967909902334213, -0.030969103798270226, -0.0846376121044159, -0.059568099677562714, 0.09636640548706055, -0.011295563541352749, -0.03427683934569359, -0.03152262791991234, 0.048911161720752716, 0.00795748457312584, 0.059126704931259155, 0.020901089534163475, -0.03965403884649277, 0.062448401004076004, -0.01772821694612503, -0.06113921478390694, 0.01283897366374731, -0.021642884239554405, -0.02302018366754055, 0.005752349738031626, -0.061735428869724274, -0.14628490805625916, 0.051892977207899094, -0.0004944919492118061, -0.02375756949186325, 0.011434944346547127, -0.09186822175979614, -0.07906457036733627, -0.007517629768699408, -0.007253993768244982, 0.046002645045518875, -0.014169986359775066, 0.0404745489358902, 0.048691265285015106, -0.009246019646525383, -0.07348710298538208, 0.010274077765643597, 0.08677572011947632, -0.026179958134889603, -0.02140418253839016, 0.11437877267599106, 0.060243017971515656, 0.010301604866981506, -0.009255919605493546, -0.06202416494488716, -0.04021847993135452, 0.00999785028398037, -0.009421605616807938, 0.024818090721964836, 0.05408206209540367, -0.04078041762113571, -0.022144027054309845, -0.05682731792330742, 0.04081587865948677, 0.04387408494949341, -0.005246262066066265, -0.10526284575462341, 0.11851855367422104, -0.024268455803394318, 0.039799537509679794, 0.013352434150874615, -0.007793794851750135, -0.059014320373535156, -0.014424859546124935, -0.04775916039943695, -0.05760326236486435, 0.11392092704772949, -0.016051694750785828, 0.02120901085436344, 0.047993697226047516, 0.00950564257800579, -0.04933144897222519, -0.023286424577236176, 0.0020123100839555264, 0.11531011760234833, -0.01627109758555889, 0.13887867331504822, 0.03264710679650307, -0.05117448791861534, -0.03816383332014084, 0.003204388776794076, -0.01596912555396557, 0.056390903890132904, -0.0390876829624176, 0.08742181211709976, -0.015590048395097256, -0.030634690076112747, 0.036289479583501816, -0.024919982999563217, -0.0565137080848217, -0.11102382838726044, 6.07695911475323e-34, -0.06599244475364685, -0.008227706886827946, 0.04425226151943207, 0.08388872444629669, -0.11768152564764023, 0.08831866830587387, -0.018532615154981613, 0.00573795847594738, 0.06345853209495544, 0.031013382598757744, 0.026527516543865204, -0.01932409219443798, 0.006759768817573786, 0.009838076308369637, 0.02678844891488552, -0.10548337548971176, -0.02655034139752388, -0.033559657633304596, -0.07796969264745712, -0.020896222442388535, -0.03958204761147499, 0.07856205105781555, -0.020350858569145203, 0.02547948993742466, -0.03126655891537666, -0.07198557257652283, 0.11346634477376938, -0.017934385687112808, 0.10661368072032928, -0.07247737050056458, 0.004973030183464289, 0.028926946222782135, -0.0814918801188469, -0.06507786363363266, -0.006710114423185587, -0.03185907378792763, -0.0954732596874237, 0.0763862133026123, -0.017990341410040855, -0.027803512290120125, -0.03189041465520859, 0.000696527655236423, -0.032402828335762024, -0.13260947167873383, -0.05504116415977478, -0.047219108790159225, 0.07467764616012573, -0.02727321721613407, 0.02574571967124939, -0.008881310932338238, -0.050859928131103516, -0.02258465811610222, 0.10509802401065826, 0.025873644277453423, -0.029957370832562447, -0.052439745515584946, 0.07640969753265381, -0.016877632588148117, -0.08452364057302475, -0.07604217529296875, 0.003783182241022587, 0.010043874382972717, -0.02630266174674034, -0.05206344276666641, 0.08440504223108292, -0.0640454888343811, -0.09405899792909622, 0.005777672864496708, 0.07394685596227646, -0.03362569585442543, -0.03526245057582855, 0.01594538800418377, -0.04790881648659706, 0.025151239708065987, -0.039317019283771515, 0.01596958376467228, -0.027216045185923576, 0.005253609735518694, 0.025195622816681862, 0.0009437703993171453, 0.05801168829202652, 0.01321752741932869, 0.037789832800626755, 0.0008823853568173945, -0.03796041011810303, -0.06345684826374054, -0.0011166606564074755, -0.04156646877527237, 0.023284509778022766, 0.011803616769611835, 0.06208194047212601, -0.023282505571842194, 0.024088546633720398, -0.017585212364792824, 0.01568574272096157, -2.9682279745202322e-08, 0.010659562423825264, -0.05633000284433365, 0.07712181657552719, 0.05101722851395607, -0.011079982854425907, -0.0005807302077300847, 0.002367499051615596, -0.018592074513435364, 0.03200189024209976, 0.04977916181087494, -0.052581291645765305, -0.04296502098441124, -0.03333304449915886, 0.020073171705007553, -0.08089343458414078, 0.07841379195451736, 0.08487144112586975, -0.13421697914600372, 0.03482995182275772, -0.04534032195806503, 0.008037547580897808, 0.05420053005218506, -0.030730189755558968, 0.024961791932582855, 0.011558441445231438, 0.025057030841708183, 0.04879523813724518, -0.029437728226184845, -0.023896215483546257, 0.02489546872675419, 0.04927957430481911, 0.011983109638094902, -0.1202002763748169, 0.005318854469805956, 0.016125252470374107, 0.00445230957120657, -0.08056430518627167, 0.0176426712423563, 0.03630785644054413, 0.08196452260017395, -0.025738423690199852, 0.04738207161426544, 0.03808915242552757, -0.00514637678861618, -0.0029029378201812506, -0.04597635567188263, 0.008389142341911793, -0.03481914475560188, -0.040013428777456284, 0.013514876365661621, 0.07065839320421219, -0.004429492633789778, 0.04025430604815483, -0.037400372326374054, 0.08259782940149307, 0.029114387929439545, 0.003579111769795418, -0.00880461186170578, -0.05012710019946098, 0.04007803276181221, 0.03561469912528992, 0.07238143682479858, 0.019006839022040367, 0.004914341028779745], [0.042391158640384674, 0.06892797350883484, -0.012829643674194813, 0.022921059280633926, -0.031988516449928284, -0.039736200124025345, -0.013293083757162094, -0.009726449847221375, 0.10041040182113647, -0.04075906053185463, 0.05534392222762108, -0.009186570532619953, 0.10898082703351974, 0.014859150163829327, 0.02183174341917038, 0.07840778678655624, 0.0595310777425766, 0.09031140804290771, -0.0017572760116308928, 0.0333414152264595, 0.03440377861261368, -0.010233302600681782, -0.00550819281488657, 0.06482840329408646, -0.04752611741423607, -0.06029044836759567, -0.035957422107458115, 0.0989808440208435, 0.01041746512055397, 0.09730382263660431, 0.010743072256445885, 0.024951502680778503, -0.052936047315597534, 0.04062076285481453, -0.053609639406204224, -0.04439689591526985, 0.0011169719509780407, 0.08049368113279343, -0.01614558696746826, -0.09232457727193832, 0.005971087608486414, -0.03063451498746872, -0.0668906643986702, -0.015797074884176254, -0.08515521138906479, -0.01634185016155243, 0.055997271090745926, 0.00024009484332054853, -0.0015571204712614417, -0.061006058007478714, -0.07788405567407608, -0.00728041073307395, -0.09796082228422165, -0.0033349134027957916, 0.014183804392814636, -0.007687231060117483, -0.051015689969062805, 0.09605483710765839, -0.004167805891484022, 0.03168301656842232, -0.013458545319736004, 0.04018283635377884, 0.00495762936770916, 0.01872016303241253, 0.039810195565223694, -0.019506094977259636, 0.014330861158668995, 0.019947361201047897, -0.08653587102890015, 0.10226568579673767, -0.020608076825737953, 0.07482314109802246, 0.1392129510641098, 0.04058516025543213, -0.03484807908535004, -0.008807645179331303, -0.015144060365855694, 0.03913532570004463, -0.022510582581162453, -0.004130078013986349, -0.068105548620224, 0.060463182628154755, 0.023172276094555855, 0.009624551981687546, 0.006095971912145615, 0.035916704684495926, 0.02094644494354725, -0.04225621744990349, 0.031852006912231445, -0.030812256038188934, -0.0917266383767128, -0.009825976565480232, 0.11844398826360703, -0.07484187930822372, 0.07271211594343185, -0.09946554154157639, -0.01505451649427414, 0.036050599068403244, -0.05158247426152229, 0.049462903290987015, 0.005246560089290142, 0.0020478572696447372, -0.07226932793855667, -0.030025212094187737, 0.0012676932383328676, -0.05315446853637695, -0.08411944657564163, -0.027568217366933823, -0.05262221023440361, -0.0979183167219162, -0.09486529231071472, -0.022862296551465988, 0.019658539444208145, -0.06677854061126709, 0.04014856368303299, -0.026300683617591858, 0.03908589482307434, 0.04480909928679466, 0.08304345607757568, -0.015867717564105988, 0.025671357288956642, -0.03430228680372238, -0.008837081491947174, 0.10396873950958252, 0.06083481386303902, -0.03947160765528679, -0.053554873913526535, -1.674769780628715e-33, -0.019451893866062164, 0.08432461321353912, 0.02281729318201542, 0.00654288986697793, -0.06658325344324112, -0.004636063240468502, -0.0479559563100338, 0.06363356113433838, 0.0978030189871788, -0.011961806565523148, 0.05763581022620201, -0.010652433149516582, 0.08791428059339523, 0.03254753723740578, 0.08301316946744919, 0.02048836089670658, -0.03641008585691452, 0.06959374248981476, 0.020071782171726227, -0.004830540623515844, 0.05332473665475845, -0.010593758895993233, -0.030535414814949036, -0.02252158708870411, -0.03259887173771858, -0.08948793262243271, 0.014341404661536217, 0.03988242149353027, 0.016171716153621674, 0.000991524662822485, -0.047749124467372894, -0.05678432062268257, 0.022112753242254257, -0.01710517518222332, 0.09063870459794998, -0.05110383406281471, 0.043428849428892136, 0.04757951945066452, -0.016882842406630516, -0.05453913286328316, 0.01638788729906082, 0.052501481026411057, -0.05529896169900894, -0.003732281271368265, 0.05874672532081604, 0.0991571918129921, 0.08661498129367828, -0.07828595489263535, 0.021427471190690994, -0.004860807675868273, 0.08748242259025574, 0.0006012170924805105, 0.08024862408638, 0.0529700443148613, -0.026026947423815727, -0.03469552844762802, -0.055124443024396896, 0.028124617412686348, -0.002097957767546177, 0.00897535402327776, -0.09244364500045776, 0.06195404380559921, -0.024523138999938965, 0.04758518561720848, 0.015433593653142452, -0.03904943913221359, -0.02027197740972042, 0.07893203943967819, -0.013554233126342297, -0.05646128952503204, 0.08298981934785843, -0.005623190198093653, -0.03806411847472191, 0.0022033601999282837, -0.033741097897291183, -0.0532667376101017, 0.00014694640412926674, 0.08186452835798264, 0.11419802159070969, -0.03600406274199486, 0.03566611930727959, -0.009456820785999298, -0.00453045591711998, -0.06371422857046127, -0.06926040351390839, 0.0032274755649268627, 0.07997225970029831, -0.03826358541846275, 0.061580002307891846, -0.05032937601208687, -0.048905014991760254, 0.0004105539701413363, -0.05572087690234184, 0.038861848413944244, -0.12815150618553162, 7.785826221243957e-34, -0.0620470866560936, -0.09857077151536942, -0.003074552398175001, 0.10212291777133942, -0.04457705095410347, 0.025363527238368988, -0.02196989394724369, -0.041382450610399246, 0.05155045911669731, 0.07402733713388443, 0.03752955421805382, -0.03085128404200077, -0.0724661648273468, -0.008286280557513237, 0.03220491483807564, -0.07615527510643005, -0.02050420641899109, -0.019862612709403038, -0.1005212813615799, 0.05088134855031967, -0.02343102917075157, 0.0962766483426094, -0.09565430134534836, 0.016985710710287094, 0.009336676448583603, -0.03730770945549011, 0.0517050102353096, 0.005356883630156517, -0.013238655403256416, -0.06268025934696198, 0.0012461227597668767, 0.028150999918580055, -0.02499799244105816, -0.05768612399697304, -0.05247628688812256, -0.04090118408203125, -0.015466129407286644, 0.049107570201158524, 0.0032370316330343485, -0.02469918131828308, 0.03777908906340599, 0.016371101140975952, -0.08349846303462982, -0.03191877901554108, 0.003874043934047222, -0.038908470422029495, -0.008297929540276527, -0.05644872784614563, 0.06771368533372879, 0.004304153844714165, -0.08550671488046646, -0.07297290861606598, -0.052355945110321045, -0.020406218245625496, -0.030583763495087624, -0.05975541099905968, 0.06802108883857727, -0.04854077845811844, -0.04969286918640137, -0.04142278432846069, 0.021468371152877808, -0.0619548074901104, -0.03828996419906616, 0.05977611988782883, 0.05406440794467926, -0.025965021923184395, -0.05721611529588699, -0.026482341811060905, 0.08805742859840393, -0.016507096588611603, -0.06097770109772682, -0.01021626777946949, -0.03592361882328987, -0.017445409670472145, -0.054157838225364685, -0.043243657797575, 0.0461965911090374, -0.03868399187922478, -0.033561620861291885, 0.03765648230910301, 0.005395061802119017, -0.065399669110775, -0.014689349569380283, -0.006559269968420267, -0.0444263331592083, -0.006270509213209152, -0.03859391435980797, 0.04986265301704407, 0.023595457896590233, -0.008817165158689022, -0.013707509264349937, -0.03578471764922142, 0.031375739723443985, 0.03396362066268921, 0.024212786927819252, -2.8516787153876066e-08, -0.0488642118871212, -0.030507277697324753, 0.10325787961483002, 0.1082780510187149, -0.022523600608110428, 0.014312464743852615, -0.030158108100295067, 0.010144272819161415, 0.013477249071002007, 0.014159051701426506, 0.005809408612549305, -0.07157707214355469, -0.0633188784122467, 0.0021550734527409077, -0.052119359374046326, 0.06510463356971741, 0.0931762233376503, -0.006258736830204725, 0.029944948852062225, 0.07327685505151749, -0.05019056051969528, 0.07659517228603363, -0.031982775777578354, 0.024883108213543892, 0.04175812005996704, -0.04858547821640968, 0.03648795187473297, -0.029131198301911354, 0.026969360187649727, -0.03355111554265022, -0.009886183775961399, 0.0021693864837288857, -0.057865578681230545, 0.061013348400592804, 0.029211759567260742, 0.05330391228199005, -0.08926509320735931, 0.07885832339525223, 0.0333186574280262, 0.025669503957033157, -0.02933702990412712, 0.06813660264015198, 0.07020706683397293, -0.016331816092133522, 0.0749734491109848, 0.03991148993372917, -0.014953013509511948, 0.027499660849571228, -0.0209356639534235, -0.05608295649290085, 0.07501430064439774, 0.052288297563791275, -0.024663159623742104, 0.011798178777098656, 0.09885257482528687, -0.028668981045484543, 0.013835479505360126, 0.025909533724188805, -0.037930928170681, 0.10386262089014053, 0.09220705181360245, -0.02454002946615219, 0.04758811742067337, 0.009064510464668274], [0.03910248726606369, 0.007743379566818476, -0.022013172507286072, -0.015991592779755592, -0.021642761304974556, -0.04806598275899887, -0.0029944228008389473, -0.04200822487473488, -0.008370015770196915, -0.041180990636348724, 0.01890300028026104, 0.06546343863010406, 0.032653097063302994, -0.049915723502635956, 0.03711460530757904, 0.00011842587264254689, -0.010946976952254772, 0.0014798205811530352, 0.009033866226673126, 0.05914038047194481, 0.13843844830989838, 0.06440231949090958, 0.020374886691570282, -0.03613514453172684, -0.045861225575208664, -0.020132947713136673, 0.04883328452706337, -0.004101731348782778, 0.014790377579629421, 0.07851654291152954, -0.025900373235344887, -0.03564813360571861, -0.03266479820013046, 0.006301163230091333, -0.08119019865989685, -0.017555993050336838, 0.011737787164747715, 0.0014237764989957213, 0.10744474083185196, 0.027540113776922226, -0.02289855107665062, -0.07348616421222687, -0.06519411504268646, -0.07355977594852448, 0.010595712810754776, 0.0005711850826628506, -0.048158060759305954, 0.004148161504417658, -0.06206366419792175, -0.05449174717068672, 0.01497486513108015, -0.02552901953458786, 0.055208638310432434, 0.012727928347885609, 0.017470352351665497, 0.019451798871159554, -0.017539897933602333, -0.05671161413192749, 0.011829097755253315, -0.037559255957603455, 0.0006529812817461789, -0.014763140119612217, 0.009313817135989666, -0.012957392260432243, -0.0008687779773026705, -0.06957236677408218, 0.018814049661159515, -0.002321289386600256, -0.03826155513525009, 0.11319079250097275, -0.10528168082237244, 0.04112430289387703, 0.04267843812704086, 0.09569410979747772, -0.0909866914153099, -0.011651422828435898, 0.0032898844219744205, 0.05039919912815094, 0.05704126134514809, -0.030547751113772392, 0.09166644513607025, -0.012251104228198528, 0.0959925577044487, -0.00030556702404282987, 0.009918403811752796, -0.029934249818325043, -0.010460259392857552, -0.006370853632688522, 0.019265862181782722, 0.058236461132764816, -0.08773978054523468, 0.031304746866226196, 0.08434274047613144, -0.027542345225811005, 0.02996773086488247, -0.13131295144557953, -0.06263111531734467, 0.06378242373466492, -0.11692158132791519, 0.05032459273934364, -0.004831964615732431, 0.06255693733692169, -0.007171470671892166, -0.003189775627106428, 0.03480357676744461, 0.005655177868902683, -0.06753607839345932, -0.06222197040915489, -0.07642144709825516, 0.016786159947514534, -0.10606078058481216, -0.020534951239824295, -0.016881238669157028, 0.0013107472332194448, 0.08634880185127258, -0.015982741490006447, 0.08382797241210938, -0.0030687747057527304, -0.11268371343612671, 0.005623610224574804, -0.04542319104075432, 0.007046777755022049, -0.03411845490336418, 0.0833304226398468, 0.05051172897219658, -0.0621233768761158, -0.056229542940855026, -1.970242591664382e-33, 0.009972771629691124, 0.03417760133743286, -0.026245778426527977, 0.0022494306322187185, -0.04703250154852867, 0.0363503135740757, -0.008383491076529026, 0.015500372275710106, 0.027656231075525284, -0.014749142341315746, 0.08827196806669235, 0.00786826852709055, -0.012634256854653358, -0.006944708991795778, -0.04178592562675476, 0.07454120367765427, 0.038818489760160446, 0.07198683172464371, -0.004308544099330902, -0.025936802849173546, 0.0439235083758831, -0.005510827526450157, 0.024246301501989365, -0.00801146775484085, -0.13651539385318756, -0.08252845704555511, 0.029965396970510483, -0.015615923330187798, 0.051095739006996155, 0.02376299537718296, -0.020013177767395973, -0.033410053700208664, 0.0455818809568882, -0.009435866959393024, 0.05869221314787865, -0.0562688484787941, 0.08645408600568771, -0.007330899592489004, -0.0897291898727417, 0.04107815772294998, 0.014463940635323524, 0.020716028288006783, 0.036140359938144684, -0.03991485759615898, -0.044708915054798126, 0.16271542012691498, 0.07835033535957336, -0.04905468225479126, 0.01732145994901657, 0.04628936946392059, 0.0341867133975029, 0.08108880370855331, 0.020181043073534966, 0.0944700762629509, 0.019679447636008263, -0.02827855572104454, -0.037433017045259476, 0.01393609493970871, 0.0025936735328286886, -0.06163507327437401, -0.04462888091802597, 0.006370189134031534, 0.0515829436480999, -0.08064811676740646, -0.005140361376106739, 0.03468073904514313, -0.08304306864738464, 0.06124046817421913, -0.05047176405787468, 0.018608974292874336, -0.006359109189361334, -0.04372202232480049, 0.008152446709573269, 0.09087321162223816, -0.017212623730301857, -0.007383438292890787, 0.019339963793754578, 0.002408725442364812, 0.1494971513748169, 0.016482938081026077, 0.02501864731311798, -0.03227046877145767, -0.056810520589351654, -0.10758624225854874, -0.06724727153778076, -0.05820336192846298, 0.004688503220677376, -0.05842401087284088, 0.05491294339299202, 0.004966605454683304, -0.10002332925796509, 0.04028064385056496, -0.06528265029191971, -0.07129672169685364, -0.08578094840049744, -7.346439289876048e-34, 0.01502203568816185, -0.04380141198635101, -0.03651130571961403, 0.12815696001052856, -0.0051934788934886456, 0.040540557354688644, 0.09960878640413284, -0.029112648218870163, 0.0453350804746151, 0.057524193078279495, 0.0025075033772736788, -0.07275515794754028, 0.015495717525482178, -0.040265779942274094, 0.17449899017810822, 0.02553965151309967, 0.0019904598593711853, -0.05837472528219223, 0.019099626690149307, -0.004833586979657412, 0.05545734241604805, 0.07378887385129929, -0.03095174953341484, 0.006717441137880087, -0.0006808983744122088, -0.01739221252501011, 0.025430358946323395, -0.030652500689029694, 0.00512947840616107, -0.05232049524784088, 0.002514809835702181, 0.0405881404876709, -0.001207858556881547, -0.049782320857048035, -0.021315908059477806, 0.003512077033519745, 0.05702337250113487, 0.11029922217130661, -0.02548973634839058, -0.12624575197696686, -0.010183993726968765, -0.05059884116053581, -0.028312433511018753, 0.10773568600416183, 0.13125748932361603, -0.10685171186923981, -0.055436428636312485, -0.019610833376646042, -0.002240461530163884, 0.037632860243320465, -0.03856448829174042, -0.08129427582025528, -0.013313704170286655, -0.016353726387023926, -0.021150866523385048, -0.07075010985136032, -0.018759507685899734, -0.01628648117184639, -0.13563303649425507, 0.03596757352352142, 0.03495759144425392, 0.059007223695516586, -0.11144809424877167, -0.01674184575676918, 0.05668897554278374, -0.020417965948581696, 0.013170107267796993, -0.022222355008125305, 0.029814347624778748, -0.03007189929485321, -0.04390144720673561, -0.026948947459459305, -0.02124723605811596, -0.049606066197156906, 0.012206926010549068, -0.0030108909122645855, 0.0344608798623085, 0.01815163902938366, -0.018926549702882767, 0.007716967724263668, 0.03479895740747452, -0.028601422905921936, 0.03242858871817589, 0.0003420039138291031, -0.008439300581812859, -0.04256350174546242, 0.03470894321799278, 0.009699603542685509, 0.01828162372112274, -0.02796347811818123, -0.06140923500061035, -0.06877724081277847, -0.04450356960296631, 0.0219615139067173, 0.06042841449379921, -2.4255568220610257e-08, -0.06133619695901871, -0.03689977899193764, 0.02163892425596714, 0.04065711423754692, 0.006360065191984177, 0.06229574978351593, -0.030729278922080994, -0.06051265075802803, 0.035331349819898605, -0.01492348313331604, -0.019891507923603058, -0.006494286470115185, -0.015088164247572422, 0.0015393258072435856, -0.025155888870358467, 0.04537792503833771, 0.008658129721879959, 0.008639431558549404, -0.014319656416773796, 0.020847009494900703, -0.01840939186513424, 0.011788738891482353, -0.02062615379691124, 0.1101401075720787, 0.008153284899890423, -0.003604518249630928, 0.022485675290226936, -0.0727836862206459, -0.0018367567099630833, 0.009594489820301533, 0.07062878459692001, 0.02936154045164585, 0.0016945037059485912, 0.034725453704595566, 0.012408293783664703, -0.049244046211242676, -0.056939829140901566, 0.03716426342725754, 0.14618203043937683, 0.03244384378194809, -0.026533838361501694, 0.012756813317537308, 0.034315936267375946, -0.0023267657961696386, 0.03337793052196503, 0.05610352009534836, -0.07454035431146622, 0.03579789027571678, -0.03544608876109123, 0.05269216001033783, 0.013791176490485668, 0.004140035714954138, -0.050181541591882706, 0.08103525638580322, 0.07455189526081085, -0.023226916790008545, -0.038920335471630096, 0.005684755276888609, -0.019587663933634758, 0.016143113374710083, 0.07365424931049347, -0.047901902347803116, 0.03867434337735176, -0.05133134126663208]]\n",
      "\n",
      "TIME FOR RESPONSE EMBEDDINGS:  6.561243772506714\n",
      "\n",
      "- - - - - -\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "content_gab = content_gab[:200]\n",
    "before = time.time()\n",
    "content_gab['text_embeddings'] = content_gab['text'].apply(generate_embeddings)\n",
    "after_text = time.time()\n",
    "print(content_gab.iloc[1]['text_embeddings'])\n",
    "print('\\nTIME FOR TEXT EMBEDDINGS: ', after_text - before)\n",
    "print('\\n- - - - - -\\n')\n",
    "content_gab['response_embeddings'] = content_gab['response'].apply(generate_embeddings)\n",
    "after_response = time.time()\n",
    "print(content_gab.iloc[2]['response_embeddings'])\n",
    "print('\\nTIME FOR RESPONSE EMBEDDINGS: ', after_response - after_text)\n",
    "print('\\n- - - - - -\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for constructing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(row):\n",
    "    text_utterances = row['text_embeddings']\n",
    "    response_utterances = row['response_embeddings']\n",
    "    # text_utterances = row['text']\n",
    "    # response_utterances = row['response']\n",
    "\n",
    "    root = text_utterances[0]\n",
    "    children = text_utterances[1:] + response_utterances\n",
    "    num_nodes = len(children) +1\n",
    "\n",
    "    #for t in text_utterances:\n",
    "     #    print(t)\n",
    "    #print()\n",
    "    #for r in response_utterances:\n",
    "    #     print(r)\n",
    "    # print()\n",
    "    # ids = [[0, i] for i in range(1, num_nodes)]\n",
    "    # print(ids)\n",
    "    # edge_index = torch.tensor(\n",
    "    #     [[0]*num_nodes, list(range(1, num_nodes)\n",
    "    # )], dtype=torch.long)\n",
    "    edge_index = torch.tensor(\n",
    "        [[0, i] for i in range(1, num_nodes)], dtype=torch.long\n",
    "    ).t().contiguous()\n",
    "    # edge_index = torch.tensor(\n",
    "    #     [[0] * len(children), list(range(1, num_nodes))], dtype=torch.long\n",
    "    # )\n",
    "    \n",
    "\n",
    "    # print(row['text_labels_encoded'])\n",
    "    # print()\n",
    "    # print(row['response_labels_encoded'])\n",
    "    # print(type(row['text_labels_encoded']), row['text_labels_encoded'].shape, row['text_labels_encoded'])\n",
    "    # print(type(row['response_labels_encoded']), row['response_labels_encoded'].shape, row['response_labels_encoded'])\n",
    "\n",
    "    # ls = np.concatenate((row['text_labels_encoded'], row['response_labels_encoded']))\n",
    "    ls = np.concatenate((row['text_labels'], row['response_labels'])).astype(int)\n",
    "    \n",
    "    print(ls)\n",
    "\n",
    "    print(ls.shape)\n",
    "    print(type(ls))\n",
    "    print(type(ls[0]))\n",
    "    \n",
    "    labels = torch.tensor(ls, dtype=torch.int32)\n",
    "\n",
    "    print(labels)\n",
    "\n",
    "    node_features = torch.tensor([root] + children, dtype=torch.float)\n",
    "    \n",
    "    # print(node_features.shape)\n",
    "    # print(edge_index.shape)\n",
    "    # print(labels.shape)\n",
    "    # print('sss')\n",
    "    data = Data(x=node_features, edge_index=edge_index, y=labels)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing graphs for all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0]\n",
      "(2,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "(13,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0 0 0]\n",
      "(8,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 0 0 1 0 0 0]\n",
      "(11,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "(18,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       dtype=torch.int32)\n",
      "[1 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(20,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "(18,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 1 0 0 0]\n",
      "(7,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "(12,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0]\n",
      "(16,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0 0 0 0 0 0]\n",
      "(11,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0]\n",
      "(2,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0 1 0 0 0]\n",
      "(10,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "(12,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0 0]\n",
      "(7,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 1 1 0 0 0]\n",
      "(9,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 0]\n",
      "(2,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0 0 0 0]\n",
      "(9,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0 0 0 0 0 0]\n",
      "(11,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(15,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 1 0 0 0 0 0 1 0 0 0 0]\n",
      "(13,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 0 0 1 1 0 0 0]\n",
      "(12,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 1 1 1 0 0 0]\n",
      "(8,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 1, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0]\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0], dtype=torch.int32)\n",
      "[1 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(14,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
      "(14,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "Number of nodes: 4\n",
      "Number of edges: 3\n",
      "\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "graphs = []\n",
    "for index, row in content_gab.iterrows():\n",
    "    graphs.append(construct_graph(row))\n",
    "\n",
    "print(graphs[0])\n",
    "print('\\n- - - - - -\\n')\n",
    "print(f\"Number of nodes: {graphs[0].num_nodes}\")\n",
    "print(f\"Number of edges: {graphs[0].num_edges}\")\n",
    "\n",
    "print()\n",
    "print(len(graphs))\n",
    "#print('Graphs: ')\n",
    "#for i in range(0, 100):\n",
    "#    print(graphs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge to one graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[1103, 384], edge_index=[2, 903], y=[1103])\n",
      "Merged Node Features:\n",
      "tensor([[-0.0227,  0.0438, -0.0208,  ..., -0.0035, -0.1306, -0.0359],\n",
      "        [ 0.0437,  0.0129, -0.0366,  ...,  0.0327,  0.0519, -0.0313],\n",
      "        [ 0.0424,  0.0689, -0.0128,  ..., -0.0245,  0.0476,  0.0091],\n",
      "        ...,\n",
      "        [ 0.0546,  0.0675,  0.0257,  ..., -0.0255, -0.0265,  0.0186],\n",
      "        [-0.0521, -0.0178,  0.0458,  ..., -0.1562,  0.0278, -0.0080],\n",
      "        [ 0.0443,  0.0244, -0.0057,  ...,  0.0071,  0.0421, -0.0769]])\n",
      "Merged Edge Index:\n",
      "tensor([[   0,    0,    0,  ..., 1097, 1097, 1097],\n",
      "        [   1,    2,    3,  ..., 1100, 1101, 1102]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize empty lists to store the merged node features, edge indices, and labels (y)\n",
    "merged_x = []\n",
    "merged_edge_index = []\n",
    "merged_y = []\n",
    "\n",
    "# Keep track of the offset for node indices in subsequent graphs\n",
    "node_offset = 0\n",
    "\n",
    "# Iterate over each graph in the list\n",
    "for graph in graphs:\n",
    "    # Concatenate node features\n",
    "    merged_x.append(graph.x)\n",
    "    \n",
    "    # Adjust edge indices: add the current node_offset to the second row of edge_index\n",
    "    merged_edge_index.append(graph.edge_index + node_offset)\n",
    "    \n",
    "    # Concatenate labels (y), the target labels from each graph\n",
    "    merged_y.append(graph.y)\n",
    "    \n",
    "    # Update node_offset for the next graph\n",
    "    node_offset += graph.x.size(0)\n",
    "\n",
    "# Concatenate all node features, edge indices, and labels\n",
    "merged_x = torch.cat(merged_x, dim=0)\n",
    "merged_edge_index = torch.cat(merged_edge_index, dim=1)\n",
    "merged_y = torch.cat(merged_y, dim=0)\n",
    "\n",
    "# Create a new graph with merged node features, edge indices, and labels (y)\n",
    "merged_graph = Data(x=merged_x, edge_index=merged_edge_index, y=merged_y)\n",
    "\n",
    "print(merged_graph)\n",
    "# Print the merged graph details\n",
    "print(\"Merged Node Features:\")\n",
    "print(merged_graph.x)\n",
    "print(\"Merged Edge Index:\")\n",
    "print(merged_graph.edge_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = []\n",
    "#for index, row in content_gab.iterrows():\n",
    "#    y.append(np.concatenate((row['text_labels'], row['response_labels'])).astype(int))\n",
    "\n",
    "#for i, q in enumerate(y):\n",
    "#    print(q)\n",
    "#    if i >= 5:\n",
    "#       print('\\n- - - -')\n",
    "#       break\n",
    "#for i, q in enumerate(graphs):\n",
    "#    print(q)\n",
    "#    if i >= 5:\n",
    "#       break\n",
    "\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=36851234)\n",
    "#folds = list(rskf.split(graphs, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(384, 16)\n",
      "  (conv2): GCNConv(16, 2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "input_dim = merged_graph.x.shape[1]    # embedding dimensionality\n",
    "data = merged_graph\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(input_dim, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "model.eval()\n",
    "\n",
    "out = model(data.x, data.edge_index)\n",
    "#visualize(out, color=data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6868, Test Accuracy: 0.7960\n",
      "Epoch: 002, Loss: 0.6286, Test Accuracy: 0.7960\n",
      "Epoch: 003, Loss: 0.5808, Test Accuracy: 0.7960\n",
      "Epoch: 004, Loss: 0.5488, Test Accuracy: 0.7960\n",
      "Epoch: 005, Loss: 0.5251, Test Accuracy: 0.7960\n",
      "Epoch: 006, Loss: 0.5191, Test Accuracy: 0.7960\n",
      "Epoch: 007, Loss: 0.5219, Test Accuracy: 0.7960\n",
      "Epoch: 008, Loss: 0.5250, Test Accuracy: 0.7960\n",
      "Epoch: 009, Loss: 0.5323, Test Accuracy: 0.7960\n",
      "Epoch: 010, Loss: 0.5278, Test Accuracy: 0.7960\n",
      "Epoch: 011, Loss: 0.5255, Test Accuracy: 0.7960\n",
      "Epoch: 012, Loss: 0.5123, Test Accuracy: 0.7960\n",
      "Epoch: 013, Loss: 0.4966, Test Accuracy: 0.7960\n",
      "Epoch: 014, Loss: 0.4971, Test Accuracy: 0.7960\n",
      "Epoch: 015, Loss: 0.4867, Test Accuracy: 0.7960\n",
      "Epoch: 016, Loss: 0.4888, Test Accuracy: 0.7960\n",
      "Epoch: 017, Loss: 0.4942, Test Accuracy: 0.7960\n",
      "Epoch: 018, Loss: 0.4880, Test Accuracy: 0.7960\n",
      "Epoch: 019, Loss: 0.4883, Test Accuracy: 0.7960\n",
      "Epoch: 020, Loss: 0.4840, Test Accuracy: 0.7960\n",
      "Epoch: 021, Loss: 0.4877, Test Accuracy: 0.7960\n",
      "Epoch: 022, Loss: 0.4785, Test Accuracy: 0.7960\n",
      "Epoch: 023, Loss: 0.4764, Test Accuracy: 0.7960\n",
      "Epoch: 024, Loss: 0.4745, Test Accuracy: 0.7960\n",
      "Epoch: 025, Loss: 0.4700, Test Accuracy: 0.7960\n",
      "Epoch: 026, Loss: 0.4683, Test Accuracy: 0.7960\n",
      "Epoch: 027, Loss: 0.4682, Test Accuracy: 0.7960\n",
      "Epoch: 028, Loss: 0.4602, Test Accuracy: 0.7960\n",
      "Epoch: 029, Loss: 0.4635, Test Accuracy: 0.7960\n",
      "Epoch: 030, Loss: 0.4601, Test Accuracy: 0.7960\n",
      "Epoch: 031, Loss: 0.4601, Test Accuracy: 0.7960\n",
      "Epoch: 032, Loss: 0.4489, Test Accuracy: 0.7960\n",
      "Epoch: 033, Loss: 0.4514, Test Accuracy: 0.7960\n",
      "Epoch: 034, Loss: 0.4444, Test Accuracy: 0.7960\n",
      "Epoch: 035, Loss: 0.4444, Test Accuracy: 0.7960\n",
      "Epoch: 036, Loss: 0.4410, Test Accuracy: 0.7960\n",
      "Epoch: 037, Loss: 0.4377, Test Accuracy: 0.7960\n",
      "Epoch: 038, Loss: 0.4379, Test Accuracy: 0.7960\n",
      "Epoch: 039, Loss: 0.4323, Test Accuracy: 0.7960\n",
      "Epoch: 040, Loss: 0.4310, Test Accuracy: 0.7960\n",
      "Epoch: 041, Loss: 0.4342, Test Accuracy: 0.7960\n",
      "Epoch: 042, Loss: 0.4309, Test Accuracy: 0.7960\n",
      "Epoch: 043, Loss: 0.4261, Test Accuracy: 0.7960\n",
      "Epoch: 044, Loss: 0.4259, Test Accuracy: 0.7960\n",
      "Epoch: 045, Loss: 0.4242, Test Accuracy: 0.7960\n",
      "Epoch: 046, Loss: 0.4183, Test Accuracy: 0.7960\n",
      "Epoch: 047, Loss: 0.4179, Test Accuracy: 0.7960\n",
      "Epoch: 048, Loss: 0.4190, Test Accuracy: 0.7960\n",
      "Epoch: 049, Loss: 0.4146, Test Accuracy: 0.7960\n",
      "Epoch: 050, Loss: 0.4147, Test Accuracy: 0.7960\n",
      "Epoch: 051, Loss: 0.4087, Test Accuracy: 0.7960\n",
      "Epoch: 052, Loss: 0.4096, Test Accuracy: 0.7960\n",
      "Epoch: 053, Loss: 0.4129, Test Accuracy: 0.7960\n",
      "Epoch: 054, Loss: 0.4060, Test Accuracy: 0.7960\n",
      "Epoch: 055, Loss: 0.4017, Test Accuracy: 0.7960\n",
      "Epoch: 056, Loss: 0.4057, Test Accuracy: 0.7960\n",
      "Epoch: 057, Loss: 0.3990, Test Accuracy: 0.7960\n",
      "Epoch: 058, Loss: 0.3984, Test Accuracy: 0.7960\n",
      "Epoch: 059, Loss: 0.3969, Test Accuracy: 0.7960\n",
      "Epoch: 060, Loss: 0.3951, Test Accuracy: 0.7960\n",
      "Epoch: 061, Loss: 0.3936, Test Accuracy: 0.7960\n",
      "Epoch: 062, Loss: 0.3889, Test Accuracy: 0.7960\n",
      "Epoch: 063, Loss: 0.3910, Test Accuracy: 0.7960\n",
      "Epoch: 064, Loss: 0.3844, Test Accuracy: 0.7960\n",
      "Epoch: 065, Loss: 0.3852, Test Accuracy: 0.7960\n",
      "Epoch: 066, Loss: 0.3814, Test Accuracy: 0.7960\n",
      "Epoch: 067, Loss: 0.3898, Test Accuracy: 0.7960\n",
      "Epoch: 068, Loss: 0.3835, Test Accuracy: 0.7960\n",
      "Epoch: 069, Loss: 0.3766, Test Accuracy: 0.7960\n",
      "Epoch: 070, Loss: 0.3763, Test Accuracy: 0.7960\n",
      "Epoch: 071, Loss: 0.3767, Test Accuracy: 0.7960\n",
      "Epoch: 072, Loss: 0.3749, Test Accuracy: 0.7960\n",
      "Epoch: 073, Loss: 0.3787, Test Accuracy: 0.7960\n",
      "Epoch: 074, Loss: 0.3774, Test Accuracy: 0.7960\n",
      "Epoch: 075, Loss: 0.3703, Test Accuracy: 0.7960\n",
      "Epoch: 076, Loss: 0.3723, Test Accuracy: 0.7960\n",
      "Epoch: 077, Loss: 0.3723, Test Accuracy: 0.7960\n",
      "Epoch: 078, Loss: 0.3680, Test Accuracy: 0.7960\n",
      "Epoch: 079, Loss: 0.3712, Test Accuracy: 0.7960\n",
      "Epoch: 080, Loss: 0.3691, Test Accuracy: 0.7960\n",
      "Epoch: 081, Loss: 0.3711, Test Accuracy: 0.7960\n",
      "Epoch: 082, Loss: 0.3644, Test Accuracy: 0.7960\n",
      "Epoch: 083, Loss: 0.3640, Test Accuracy: 0.7960\n",
      "Epoch: 084, Loss: 0.3605, Test Accuracy: 0.7960\n",
      "Epoch: 085, Loss: 0.3586, Test Accuracy: 0.7960\n",
      "Epoch: 086, Loss: 0.3619, Test Accuracy: 0.7960\n",
      "Epoch: 087, Loss: 0.3625, Test Accuracy: 0.7960\n",
      "Epoch: 088, Loss: 0.3584, Test Accuracy: 0.7960\n",
      "Epoch: 089, Loss: 0.3576, Test Accuracy: 0.7960\n",
      "Epoch: 090, Loss: 0.3560, Test Accuracy: 0.7960\n",
      "Epoch: 091, Loss: 0.3570, Test Accuracy: 0.7960\n",
      "Epoch: 092, Loss: 0.3542, Test Accuracy: 0.7960\n",
      "Epoch: 093, Loss: 0.3588, Test Accuracy: 0.7960\n",
      "Epoch: 094, Loss: 0.3522, Test Accuracy: 0.7960\n",
      "Epoch: 095, Loss: 0.3560, Test Accuracy: 0.7960\n",
      "Epoch: 096, Loss: 0.3508, Test Accuracy: 0.7960\n",
      "Epoch: 097, Loss: 0.3462, Test Accuracy: 0.7960\n",
      "Epoch: 098, Loss: 0.3460, Test Accuracy: 0.7960\n",
      "Epoch: 099, Loss: 0.3456, Test Accuracy: 0.7960\n",
      "Epoch: 100, Loss: 0.3466, Test Accuracy: 0.7960\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "data = merged_graph\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "    loss = criterion(out, data.y.long())  # Compute the loss for the training dataset.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    test_correct = pred == data.y.long()  # Check against ground-truth labels.\n",
    "    test_acc = int(test_correct.sum()) / len(data.y)  # Derive ratio of correct predictions.\n",
    "    return test_acc\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()  # Pass the training dataset to train function.\n",
    "    test_acc = test()  # Pass the test dataset to test function.\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Test Accuracy: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7960\n"
     ]
    }
   ],
   "source": [
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
