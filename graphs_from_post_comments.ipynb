{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import ast\n",
    "import time\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "# import torch_geometric as tg\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import re\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', 10)  # Limit number of rows displayed\n",
    "pd.set_option('display.width', 1000)  # Set max width for table\n",
    "pd.set_option('display.colheader_justify', 'center')  # Center-align column headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_chars(value):\n",
    "    if isinstance(value, str):  \n",
    "        return value.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').replace('  ', ' ').strip()\n",
    "    return value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading gab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id                                                text                        hate_speech_idx                      response                     \n",
      "0                                    1. 39869714\\r\\n  1. i joined gab to remind myself how retarded ...         [1]      [\"Using words that insult one group while defe...\n",
      "1  1. 39845588\\r\\n2. \\t39848775\\r\\n3. \\t\\t3991101...  1. This is what the left is really scared of. ...         [3]      ['You can disagree with someones opinion witho...\n",
      "2                   1. 37485560\\r\\n2. \\t37528625\\r\\n  1. It makes you an asshole.\\r\\n2. \\tGive it to...         [2]      ['Your argument is more rational if you leave ...\n",
      "3                   1. 39787626\\r\\n2. \\t39794481\\r\\n  1. So they manage to provide a whole lot of da...         [2]      [\"You shouldn't generalize a specific group or...\n",
      "4  1. 37957930\\r\\n2. \\t39953348\\r\\n3. \\t\\t3996521...  1. Hi there, i,m Keith, i hope you are doing w...         [3]      ['If someone is rude it is better to ignore th...\n",
      "5                                    1. 38462712\\r\\n                    1. you sound like a faggot \\r\\n         [1]      [\"Please be careful with the words you choose ...\n",
      "6  1. 38052531\\r\\n2. \\t38103723\\r\\n3. \\t\\t3851658...  1. Hi developers, give us a follow for updates...         [3]      [\"The words you've chosen are hateful and dero...\n",
      "7                   1. 38352488\\r\\n2. \\t38373190\\r\\n  1. Well, you are the fuckers that lit the matc...         [2]      ['Please refrain from using such horrible bigo...\n",
      "8  1. 37238116\\r\\n2. \\t38348543\\r\\n3. \\t\\t3837623...  1. SELF-HATING WHITE CUCKS ON PARADE\\r\\n2. \\tD...      [1, 3]      ['Your words are derogatory and offensive, and...\n",
      "9  1. 37358018\\r\\n2. \\t37359176\\r\\n3. \\t\\t3738104...  1. So after 6 years and nearly 11K followers, ...         [3]      [\"Woah! Please don't use such strong and offen...\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "Index(['id', 'text', 'hate_speech_idx', 'response'], dtype='object')\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "1. 39845588\n",
      "2. \t39848775\n",
      "3. \t\t39911017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_gab = pd.read_csv('gab_reddit_benchmark/gab.csv')\n",
    "\n",
    "content_gab[\"text\"] = content_gab[\"text\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "content_gab[\"response\"] = content_gab[\"response\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\")\n",
    "content_gab[\"hate_speech_idx\"] = content_gab[\"hate_speech_idx\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "\n",
    "# content_gab[\"text\"] = content_gab[\"text\"].apply(clean_special_chars)\n",
    "# content_gab[\"response\"] = content_gab[\"response\"].apply(clean_special_chars)\n",
    "\n",
    "for index, row in content_gab.iterrows():\n",
    "    row['text'] = row['text'].replace(\"'\", '\"')\n",
    "    row['response'] = row['response'].replace(\"'\", '\"')\n",
    "\n",
    "# content_gab = content_gab.applymap(clean_special_chars)\n",
    "print(content_gab.head(n=10))\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab.columns)\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab.iloc[1]['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wrobl\\AppData\\Local\\Temp\\ipykernel_12916\\2213304540.py:30: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace('] [', ', ') if isinstance(x, str) else x)\n",
      "C:\\Users\\Wrobl\\AppData\\Local\\Temp\\ipykernel_12916\\2213304540.py:31: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace(']  [', ', ') if isinstance(x, str) else x)\n",
      "C:\\Users\\Wrobl\\AppData\\Local\\Temp\\ipykernel_12916\\2213304540.py:32: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: 'n/a' if isinstance(x, str) and x.strip() == '' else x)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_first_number(input_string):\n",
    "    match = re.search(r'\\d{2,}', input_string)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None\n",
    "\n",
    "content_gab['extracted_id'] = content_gab['id'].apply(get_first_number)\n",
    "\n",
    "# Find duplicate rows based on 'extracted_id'\n",
    "duplicates = content_gab[content_gab.duplicated(subset=['extracted_id'], keep=False)]\n",
    "filtered_groups = []\n",
    "grouped = content_gab.groupby('extracted_id')\n",
    "for key, group in grouped:\n",
    "    if len(group) > 1:\n",
    "        filtered_groups.append(group)\n",
    "\n",
    "merged_df = pd.concat(filtered_groups, ignore_index=True)\n",
    "\n",
    "merged_df.to_csv('gab_reddit_benchmark/gab_groups.csv', index=False)\n",
    "\n",
    "merged_df = grouped.agg({\n",
    "    'id': ' '.join,\n",
    "    'text': ' '.join,\n",
    "    'hate_speech_idx': ' '.join,\n",
    "    'response': ' '.join\n",
    "}).reset_index()\n",
    "merged_df.to_csv('gab_reddit_benchmark/gab_merged.csv')\n",
    "\n",
    "df = pd.read_csv('gab_reddit_benchmark/gab_merged.csv')\n",
    "df = df.applymap(lambda x: x.replace('] [', ', ') if isinstance(x, str) else x)\n",
    "df = df.applymap(lambda x: x.replace(']  [', ', ') if isinstance(x, str) else x)\n",
    "df = df.applymap(lambda x: 'n/a' if isinstance(x, str) and x.strip() == '' else x)\n",
    "df.to_csv('gab_reddit_benchmark/gab_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id                                                text                        hate_speech_idx                      response                     \n",
      "0  1. 37011807\\r\\n2. \\t37012801\\r\\n3. \\t\\t3701333...  1. 64% of Hispanics have IQ scores too low to ...     [2, 2, 3]    ['Racist and hate speech will not be tolerated...\n",
      "1                    1. 37012081\\r\\n 1. 37012081\\r\\n  1. LYING CUNT BITCH !!\\r\\n 1. LYING CUNT BITCH...        [1, 1]    ['Please refrain from using derogatory terms f...\n",
      "2                                    1. 37012105\\r\\n  1. Nigger Kills 2 cops https://www.google.com/...           [1]    ['The N word is highly offensive.   All ethnic...\n",
      "3                   1. 37012146\\r\\n2. \\t37013020\\r\\n  1. STABBED IN NECK   arrested but no descr   #...           [2]    ['Use of the n-word is unacceptable in our dis...\n",
      "4  1. 37012285\\r\\n2. \\t37013809\\r\\n3. \\t37013868\\r\\n  1. Retard 1st Class  w/ Cluster & Ribbon\\r\\n2....           [1]    ['We should try and not use words making fun o...\n",
      "5                                    1. 37012571\\r\\n  1. We should be happy they do fuck their cousi...           [1]    ['I cannot listen to your argument if you keep...\n",
      "6  1. 37012882\\r\\n2. \\t37013415\\r\\n3. \\t\\t3701686...  1. Stolen Valor! Prosecute the scum.\\r\\n2. \\t\\...        [3, 4]    ['Stick to your own beliefs without dismissing...\n",
      "7                                    1. 37013030\\r\\n  1. Yea, here's the scoop.  Fake News! \"We told...                                                                   \n",
      "8                                    1. 37013358\\r\\n           1. Throat-punch that faggot DeNigger\\r\\n           [1]    ['I hope that you can see that you are advocat...\n",
      "9                                    1. 37013922\\r\\n  1. https://www.youtube.com/watch?v=DmNRkp_fuoo...           [1]    ['Gender is a category that shouldn’t be the b...\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "Index(['id', 'text', 'hate_speech_idx', 'response'], dtype='object')\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "1. 37011807\n",
      "2. \t37012801\n",
      "3. \t\t37013338\n",
      "4. \t\t37013511\n",
      "5. \t\t37333801\n",
      " 1. 37011807\n",
      "2. \t37012913\n",
      "3. \t\t37013738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_gab_m = pd.read_csv('gab_reddit_benchmark/gab_merged.csv')\n",
    "content_gab_m = content_gab_m.drop('Unnamed: 0', axis=1)\n",
    "content_gab_m = content_gab_m.drop('extracted_id', axis=1)\n",
    "\n",
    "\n",
    "content_gab_m[\"text\"] = content_gab_m[\"text\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "content_gab_m[\"response\"] = content_gab_m[\"response\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\")\n",
    "content_gab_m[\"hate_speech_idx\"] = content_gab_m[\"hate_speech_idx\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "\n",
    "for index, row in content_gab_m.iterrows():\n",
    "    row['text'] = row['text'].replace(\"'\", '\"')\n",
    "    row['response'] = row['response'].replace(\"'\", '\"')\n",
    "\n",
    "print(content_gab_m.head(n=10))\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab_m.columns)\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab_m.iloc[0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_text_labels(text_utterances_length, labels):\n",
    "    if not labels:\n",
    "        # return ['other'] * text_utterances_length\n",
    "        return [0] * text_utterances_length\n",
    "    new_labels = []\n",
    "    int_list = ast.literal_eval(labels)\n",
    "    for i in range(text_utterances_length):\n",
    "        if i+1 in int_list:\n",
    "            # new_labels.append('hate_speech')\n",
    "            new_labels.append(1)\n",
    "        else:\n",
    "            # new_labels.append('other')\n",
    "            new_labels.append(0)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting 'text' and 'response' into individual rows, so that I can construct a graph from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id                                                text                        text_labels                      response                       extracted_id response_labels\n",
      "0                                    1. 39869714\\r\\n  [1. i joined gab to remind myself how retarded...         [1]  [Using words that insult one group while defen...    39869714       [0, 0, 0]  \n",
      "1  1. 39845588\\r\\n2. \\t39848775\\r\\n3. \\t\\t3991101...  [1. This is what the left is really scared of....   [0, 0, 1]  [You can disagree with someones opinion withou...    39845588       [0, 0, 0]  \n",
      "2                   1. 37485560\\r\\n2. \\t37528625\\r\\n  [1. It makes you an asshole., 2. Give it to a ...      [0, 1]  [Your argument is more rational if you leave y...    37485560       [0, 0, 0]  \n",
      "3                   1. 39787626\\r\\n2. \\t39794481\\r\\n  [1. So they manage to provide a whole lot of d...      [0, 1]  [You shouldn't generalize a specific group or ...    39787626       [0, 0, 0]  \n",
      "4  1. 37957930\\r\\n2. \\t39953348\\r\\n3. \\t\\t3996521...  [1. Hi there, i,m Keith, i hope you are doing ...   [0, 0, 1]  [If someone is rude it is better to ignore the...    37957930       [0, 0, 0]  \n",
      "- - - - \n",
      "Index(['id', 'text', 'text_labels', 'response', 'extracted_id', 'response_labels'], dtype='object')\n",
      "1. 39869714\n",
      "\n",
      "[\"1. i joined gab to remind myself how retarded jew haters are. You wouldn't be typing on your abacus without them you retard.\"]\n",
      "[1]\n",
      "[\"Using words that insult one group while defending another group doesn't come across as helpful.\", 'You can make the same point more effectively without the use of hateful terminology.', 'Use of the r-word is unacceptable in our discourse as it demeans and insults people with mental disabilities.']\n",
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "text_column = []\n",
    "text_labels_column = []\n",
    "response_column = []\n",
    "response_labels_column = []\n",
    "\n",
    "for index, row in content_gab.iterrows():\n",
    "    text_utterances = row['text'].split('\\n')\n",
    "    text_utterances = list(filter(None, text_utterances))\n",
    "\n",
    "    for i, t in enumerate(text_utterances):\n",
    "        text_utterances[i] = clean_special_chars(t)\n",
    "    text_labels = mark_text_labels(len(text_utterances), row['hate_speech_idx'])\n",
    "\n",
    "    response_utterances = ast.literal_eval(row['response']) if row['response'] else []\n",
    "    for i, r in enumerate(response_utterances):\n",
    "        response_utterances[i] = clean_special_chars(r)\n",
    "    # response_labels = ['other'] * len(response_utterances)  \n",
    "    response_labels = [0] * len(response_utterances)  \n",
    "\n",
    "    \n",
    "    text_column.append(text_utterances)\n",
    "    text_labels_column.append(text_labels)\n",
    "    response_column.append(response_utterances)\n",
    "    response_labels_column.append(response_labels)\n",
    "\n",
    "content_gab['text'] = text_column\n",
    "content_gab['hate_speech_idx'] = text_labels_column\n",
    "content_gab['response'] = response_column\n",
    "content_gab['response_labels'] = response_labels_column\n",
    "\n",
    "content_gab = content_gab.rename(columns={'hate_speech_idx': 'text_labels'})\n",
    "print(content_gab.head())\n",
    "print('- - - - ')\n",
    "print(content_gab.columns)\n",
    "\n",
    "for index, row in content_gab.iterrows():\n",
    "    if index == 1:\n",
    "        continue\n",
    "    print(row['id'])\n",
    "    print(row['text'])\n",
    "    print(row['text_labels'])\n",
    "    print(row['response'])\n",
    "    print(row['response_labels'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_encoder = LabelEncoder()\n",
    "# content_gab['all_labels'] = content_gab['text_labels'] + content_gab['response_labels']\n",
    "# content_gab['all_labels_encoded'] = content_gab['all_labels'].apply(label_encoder.fit_transform)\n",
    "# print(content_gab.iloc[0])\n",
    "# content_gab['text_labels_encoded'] = content_gab['text_labels'].apply(label_encoder.fit_transform)\n",
    "# content_gab['response_labels_encoded'] = content_gab['response_labels'].apply(label_encoder.fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating BERT encoding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def generate_embeddings(sentences):\n",
    "    if isinstance(sentences, list):\n",
    "        return bert.encode(sentences, show_progress_bar=True).tolist()\n",
    "    elif isinstance(sentences, str):\n",
    "        return bert.encode([sentences], show_progress_bar=True).tolist()\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 199.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 199.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 199.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 199.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 199.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 199.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 100.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 199.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 199.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.023572085425257683, 0.017944442108273506, 0.040565647184848785, 0.06729648262262344, 0.09804016351699829, 0.03511403501033783, 0.06723953783512115, -0.07981330156326294, 0.012592197395861149, -0.06395190954208374, 0.014613415114581585, -0.02868604101240635, 0.06557449698448181, -0.05138687416911125, -0.1029239222407341, 0.015551761724054813, -0.06676268577575684, -0.0029045867267996073, -0.027871351689100266, 0.06036287546157837, -0.027235571295022964, 0.02632731758058071, 0.03128807619214058, 0.017424041405320168, 0.013893619179725647, -0.06276202201843262, -0.013789272867143154, -0.015726275742053986, -0.03531144559383392, -0.054763998836278915, 0.013463125564157963, -0.02827640064060688, -0.03120230697095394, -0.05433619022369385, -0.011610044166445732, -0.04129823297262192, 0.10698012262582779, -0.05024966597557068, -0.029813911765813828, 0.062091145664453506, -0.017547206953167915, -0.015013372525572777, 0.08790481090545654, 0.07822362333536148, -0.0915985107421875, 0.048159778118133545, -0.06916669756174088, 0.05322777479887009, -0.02425643615424633, -0.09537237882614136, -0.03579240292310715, -0.042581893503665924, -0.08481574058532715, 0.03169671818614006, 0.037226006388664246, -0.05798828601837158, -0.05983056500554085, 0.014472068287432194, 0.039478182792663574, 0.014918514527380466, 0.03749939426779747, 0.02377479523420334, 0.031081492081284523, 0.009576133452355862, -0.028067227452993393, -0.054927289485931396, 0.039897385984659195, -0.08561882376670837, -0.0579548180103302, 0.08154647052288055, -0.0017612491501495242, 0.015560157597064972, -0.05728122591972351, -0.09787733852863312, 0.013083047233521938, -0.0185531135648489, 0.007453033234924078, -0.019630108028650284, -0.05494064465165138, -0.09536644071340561, 0.02637145295739174, -0.006277905777096748, 0.10078253597021103, -0.012945727445185184, 0.01449866034090519, 0.017524318769574165, -0.04861268773674965, -0.014787840656936169, 0.01737136021256447, 0.05227295309305191, 0.022957341745495796, 0.0016866445075720549, 0.07582615315914154, 0.017705759033560753, -0.0015716258203610778, -0.0029984349384903908, -0.04621938616037369, 0.02433943562209606, -0.04856688529253006, 0.04727041721343994, -0.016229236498475075, -0.0022451658733189106, 0.05938282236456871, 0.04684928432106972, -0.0256496611982584, 0.018178880214691162, -0.04995887354016304, -0.019953856244683266, -0.06652811169624329, 0.0009593124850653112, -0.05363898724317551, -0.018837425857782364, -0.04111715778708458, -0.0021198077592998743, -0.04022648558020592, -0.03390153869986534, 0.017404133453965187, -0.0074494159780442715, 0.013669020496308804, 0.03172920644283295, 0.013172337785363197, -0.00626518065109849, 0.017268115654587746, -0.03328573331236839, 0.08197324723005295, -0.06818145513534546, -0.12145192921161652, -5.1817726876505444e-33, 0.07206416130065918, -0.10233233124017715, -0.039518311619758606, 0.001345106866210699, 0.07297980040311813, 0.04217555746436119, -0.08489370346069336, 0.033417876809835434, -0.03820198029279709, 0.03323644772171974, -0.04697807878255844, -0.0037153393495827913, -0.049164604395627975, -0.028877075761556625, 0.00291626644320786, -0.07700137794017792, 0.03615829348564148, 0.021831398829817772, -0.07132776081562042, -0.05609352886676788, -0.0001289840292884037, 0.1005074679851532, -0.011927727609872818, 0.03246559575200081, 0.03727072477340698, 0.017232665792107582, -0.005268979351967573, 0.02415023371577263, 0.046569306403398514, 0.024385109543800354, -0.12742936611175537, 0.013551429845392704, -0.049475111067295074, -0.029007840901613235, 0.009385472163558006, 0.0610591396689415, 0.005882316268980503, -0.054570648819208145, -0.05731366574764252, -0.09142468124628067, -0.029561292380094528, 0.03298031538724899, -0.09254158288240433, 0.014611317776143551, 0.060211434960365295, 0.03362564370036125, -0.01711602509021759, -0.015008946880698204, -0.0040582711808383465, 0.021523697301745415, -0.018500059843063354, 0.07692428678274155, 0.07741912454366684, 0.05040504038333893, 0.05784924700856209, -0.07141821086406708, 0.00950239971280098, 0.04143376275897026, -0.049271803349256516, 0.009039491415023804, -0.0006089372327551246, 0.03323495015501976, 0.06090978533029556, -0.0555817075073719, -0.0134292496368289, 0.04045293107628822, -0.018049919977784157, -0.07930322736501694, 0.02816927805542946, 0.04188494756817818, -0.023062707856297493, 0.007037443108856678, 0.04865972325205803, 0.060502707958221436, -0.045685648918151855, -0.005206840578466654, -0.006128601264208555, 0.024518238380551338, 0.006941829342395067, -0.04208430275321007, 0.042195942252874374, -0.03775396943092346, 0.01052202470600605, 0.04107959568500519, -0.012251426465809345, -0.0403900109231472, 0.01221493724733591, -0.15064898133277893, 0.010156802833080292, 0.03339752182364464, 0.0025314532686024904, -0.022614028304815292, 0.053126510232686996, -0.044650837779045105, -0.054305292665958405, 1.773386777164997e-33, -0.11583425849676132, 0.010013984516263008, -0.04472271353006363, 0.043904587626457214, -0.06872116774320602, 0.022426342591643333, 0.1263907253742218, 0.05892791971564293, 0.06174798309803009, -0.009931771084666252, 0.02047286368906498, 0.025899656116962433, -0.050425153225660324, 0.013905070722103119, -0.010196041315793991, -0.026411477476358414, 0.12776240706443787, 0.006280264351516962, -0.011610646732151508, 0.021490726619958878, -0.05238820239901543, 0.07279007881879807, -0.07466275990009308, 0.14213714003562927, -0.04015480354428291, 0.058631908148527145, 0.03182351216673851, -0.02055339515209198, 0.0016153063625097275, -0.057800572365522385, -0.03013589233160019, -0.020728088915348053, 0.0449526384472847, 0.0677015632390976, -0.03854568675160408, -0.039688412100076675, -0.016269350424408913, 0.013275796547532082, 0.04998328536748886, -0.05824997276067734, 0.10717656463384628, -0.019735349342226982, 0.004113053437322378, -0.029574330896139145, -0.05759342014789581, 0.0602990947663784, 0.048112671822309494, 0.08564702421426773, -0.07532624900341034, -0.04375803843140602, -0.014880266971886158, 0.050991352647542953, -0.03473405912518501, -0.01971082016825676, -0.004233869258314371, -0.08403671532869339, -0.04963793233036995, -0.008992861025035381, 0.04772939905524254, 0.10661700367927551, -0.027183935046195984, 0.09788491576910019, -0.06811606138944626, 0.020569616928696632, 0.03360128775238991, -0.054027434438467026, -0.049967944622039795, 0.0793352872133255, 0.045689165592193604, -0.031186359003186226, 0.06635914742946625, 0.001481132348999381, -0.06889210641384125, -0.042748890817165375, 0.0004744662146549672, -0.029906732961535454, 0.027813246473670006, -0.006037923041731119, 0.021638456732034683, -0.003204158740118146, 0.04991651698946953, -0.0588565394282341, -0.024569492787122726, 0.03823615610599518, 0.07614949345588684, 0.0620727501809597, 0.10862242430448532, 0.003921707160770893, 0.04708326607942581, -0.0036237789317965508, 0.02968909963965416, 0.08975239098072052, -0.01661710999906063, 0.09280364215373993, 0.07856978476047516, -3.221419930810043e-08, 0.07493720948696136, -0.04131815582513809, -0.009186021983623505, -0.0634353905916214, 0.04685008153319359, 0.06032409146428108, -0.014463100582361221, 0.04866195470094681, -0.11761871725320816, 0.06345325708389282, 0.0496128685772419, 0.03817204758524895, -0.03975909203290939, -0.04137645289301872, -0.06160253658890724, 0.07888321578502655, -0.11216012388467789, -0.01504905242472887, 0.003678105538710952, -0.04987558349967003, 0.013319593854248524, -0.0071268631145358086, -0.05422540754079819, -0.030421456322073936, -0.045886751264333725, 0.07505375891923904, -0.10358119755983353, -0.004367321729660034, -0.009591998532414436, 0.06975298374891281, 0.010770132765173912, -0.07311349362134933, -0.03480401635169983, 0.028354771435260773, -0.013685907237231731, -0.026222318410873413, 0.019258243963122368, 0.029576990753412247, 0.04298372566699982, -0.019282279536128044, 0.033623285591602325, -0.022703809663653374, 0.04089219123125076, 0.01458034198731184, -0.10697578638792038, 0.010506018996238708, -0.03953004628419876, -0.10012616962194443, 0.03301757946610451, -0.04779037833213806, -0.02926359884440899, 0.0006362642743624747, 0.007813302800059319, 0.08117809146642685, 0.04891374334692955, -0.0270573440939188, 0.01279938593506813, 0.07448850572109222, -0.008968266658484936, 0.07786200195550919, 0.07612895965576172, 0.06673555076122284, -0.03327762708067894, 0.11221784353256226], [-0.09077424556016922, -0.004321173764765263, 0.0485813245177269, 0.01600239984691143, 0.06305993348360062, 0.007225281558930874, 0.12898226082324982, 0.03402092307806015, 0.07319033145904541, 0.06843872368335724, 0.004195567220449448, -0.08952050656080246, -0.027654726058244705, -0.05498356372117996, 0.0074511379934847355, -0.005147548392415047, -0.07560398429632187, -0.0010117810452356935, 0.016117917373776436, 0.015321201644837856, 0.06880341470241547, 0.02746446058154106, 0.05821283161640167, -0.013553127646446228, -0.04511800408363342, -0.006279472727328539, -0.006841282360255718, 0.002920766593888402, -0.03365723788738251, -0.011803708970546722, 0.041198574006557465, -0.04805602878332138, -0.00403935881331563, -0.002998538315296173, -0.08565221726894379, -0.009894339367747307, 0.11572092026472092, -0.027901314198970795, 0.03839496523141861, 0.02239576168358326, -0.018354246392846107, 0.02843811921775341, 0.00997850764542818, 0.00012176612654002383, 0.0615004226565361, 0.0629688948392868, -0.02182989940047264, 0.1005239188671112, 0.004260633606463671, -0.09350594878196716, -0.020234012976288795, 0.014279794879257679, -0.04646128788590431, 0.009929380379617214, -0.07782069593667984, 0.06705360859632492, -0.03904205560684204, -0.009611651301383972, 0.046019792556762695, 0.07742096483707428, 0.011429935693740845, -0.0013719447888433933, 0.0148551594465971, 0.04563784971833229, 0.012635409832000732, -0.02511431835591793, -0.05547082796692848, -0.1347910761833191, 0.019971728324890137, 0.043211355805397034, 0.03492512181401253, -0.05289946123957634, 0.010165197774767876, -0.018294651061296463, -0.05712099373340607, -0.14432501792907715, 0.0725279226899147, -0.020182883366942406, -0.006986802909523249, -0.0025313820224255323, -0.05025385692715645, -0.03976425528526306, 0.10722938925027847, 0.014470851048827171, -0.013130499050021172, 0.09413733333349228, -0.09736151248216629, -0.032829102128744125, -0.1397237330675125, 0.022386137396097183, -0.02385745383799076, 0.009897890500724316, 0.09143147617578506, 0.034542016685009, 0.04380429536104202, 0.00093828298849985, -0.06505821645259857, 0.07799242436885834, 0.012868529185652733, 0.1073002889752388, -0.019915813580155373, -0.02175712399184704, 0.04154646024107933, -0.008801407180726528, 0.04062190651893616, -0.009801337495446205, -0.04056771844625473, -0.06910773366689682, 0.03248099610209465, -0.03244445472955704, 0.02478603459894657, -0.019285287708044052, -0.04623979330062866, 0.028794879093766212, 0.00494318176060915, 0.006177326664328575, 0.027298783883452415, -0.05179397389292717, -0.0564415380358696, 0.005550796631723642, 0.017456458881497383, 0.0469675175845623, -0.02875383198261261, -0.08048642426729202, 0.03437250852584839, -0.10702905058860779, -0.05876626819372177, -5.3529015225852376e-33, -0.006287327967584133, -0.01340854074805975, 0.08930911123752594, -0.031461503356695175, 0.07935459911823273, 0.10619645565748215, -0.05945824831724167, 0.008286997675895691, 0.08319014310836792, 0.06524445116519928, -0.042548760771751404, -0.014957514591515064, -0.09232842922210693, 0.07507064193487167, -0.04178767651319504, 0.06916015595197678, 0.008016509935259819, 0.043761998414993286, -0.05831656605005264, -0.010230588726699352, 0.041488099843263626, 0.0444391705095768, -0.003480930346995592, -0.014280259609222412, -0.04202086478471756, 0.04659472033381462, 0.02901593968272209, -0.041872601956129074, 0.07494311034679413, 0.04662926122546196, -0.06684381514787674, -0.015876494348049164, -0.02563110738992691, -0.014308987185359001, -0.05874011665582657, -0.11437094211578369, 0.08765875548124313, -0.08930797129869461, -0.025317495688796043, 0.01094658300280571, -0.004341438412666321, 0.02736738882958889, -0.05235069990158081, 0.08464673161506653, -0.025068702176213264, 0.049463316798210144, 0.06204375624656677, 0.017245281487703323, 0.003648824989795685, 0.08180669695138931, 0.04467298835515976, 0.04281897097826004, 0.1132434606552124, 0.032398711889982224, 0.03701513260602951, -0.01307679619640112, -0.04728211089968681, 0.05446523055434227, -0.008659147657454014, 0.011441857554018497, 0.037619054317474365, -0.007224789820611477, 0.04716648906469345, 0.030068084597587585, 0.052578140050172806, -0.04520203173160553, -0.01209873054176569, -0.01603391207754612, 0.03551926836371422, -0.0020983871072530746, 0.068123959004879, 0.060646988451480865, 0.003167623421177268, -0.06187475845217705, 0.006869093980640173, -0.04608908295631409, 0.009950932115316391, 0.04881925508379936, 0.038603801280260086, 0.016217133030295372, -0.006349007599055767, 0.06591644138097763, 0.014696796424686909, -0.0023292365949600935, -0.02188893035054207, -0.017952026799321175, 0.026980599388480186, -0.015907827764749527, -0.02559836395084858, -0.03220637887716293, 0.08449704200029373, -0.0228938776999712, -0.04050305485725403, -0.0730055421590805, -0.03085247613489628, 1.8073796860678773e-33, -0.05230116844177246, 0.014196504838764668, -0.044968243688344955, 0.017020758241415024, 0.003647125791758299, -0.050432924181222916, 0.06829755753278732, 0.025682980194687843, -0.04125535488128662, 0.0013852869160473347, 0.006468667648732662, -0.07029659301042557, -0.009580670855939388, -0.013528133742511272, 0.023790499195456505, -0.036890678107738495, -0.005132654681801796, 0.027188457548618317, -0.028192199766635895, -0.009005983360111713, 0.02099550887942314, -0.007337902206927538, -0.03633766248822212, 0.01652606576681137, -0.1141233891248703, 0.02655809372663498, 0.020835109055042267, -0.048099782317876816, -0.007241059094667435, 0.0026464080438017845, -0.04493556171655655, 0.0644175112247467, -0.022319432348012924, -0.07922093570232391, 0.06910740584135056, -0.004985511302947998, -0.08609599620103836, -0.003845677012577653, 0.02475804090499878, -0.019033804535865784, 0.04496239125728607, -0.015579498372972012, -0.04503341764211655, -0.04341494292020798, 0.04773916304111481, -0.0810801163315773, -0.024714162573218346, -0.05639687180519104, -0.07916367053985596, 0.01937534660100937, -0.07004601508378983, -0.06934062391519547, -0.041613295674324036, -0.0942700132727623, -0.07260119169950485, -0.036446698009967804, -0.1498660147190094, -0.009570295922458172, 0.04930365830659866, 0.1161232739686966, 0.022939203307032585, -0.041454415768384933, -0.08526185154914856, 0.07278041541576385, -0.03629558905959129, -0.025328034535050392, 0.0063645076006650925, 0.08927848190069199, 0.026802776381373405, -0.05549142509698868, 0.07174713909626007, -0.022975647822022438, -0.012574957683682442, -0.015541980974376202, -0.032734863460063934, 0.01613897643983364, 0.04726499691605568, -0.06111878156661987, 0.05607300624251366, 0.018305474892258644, -0.08866320550441742, -0.08189424127340317, 0.03105001151561737, 0.08009015768766403, -0.003510392038151622, -0.021703651174902916, 0.012455562129616737, 0.03309237211942673, -0.03735202178359032, 0.07492285966873169, 0.06298673152923584, 0.03162553533911705, -0.08552482724189758, 0.05492117255926132, -0.01661377213895321, -2.6253188067926203e-08, -0.0050436872988939285, 0.09730629622936249, 0.053422555327415466, -0.018845759332180023, 0.06553652137517929, 0.09893171489238739, -0.044128742069005966, -0.05685991421341896, -0.04830345883965492, -0.03244015574455261, -0.013385042548179626, -0.006123497150838375, -0.02511126734316349, 0.02951688878238201, -0.0061160544864833355, 0.08838701993227005, -0.1279958188533783, 0.014528905041515827, -0.019665846601128578, 0.03383404761552811, 0.013705705292522907, 0.03742438927292824, -0.013599744066596031, -0.015511981211602688, -0.03373889997601509, 0.0276438407599926, -0.06709184497594833, 0.11620382219552994, -0.028457514941692352, 0.09794427454471588, 0.02507849596440792, 0.051817335188388824, -0.08401425927877426, -0.003711962141096592, 0.03724723681807518, -0.09510155767202377, -0.012896752916276455, -0.008086761459708214, 0.009548495523631573, -0.06013853847980499, 0.00868066493421793, -0.00283022946678102, 0.019954971969127655, 0.055064380168914795, 0.05310448631644249, 0.026363978162407875, -0.021042782813310623, -0.07735627889633179, -0.04009253531694412, -0.027792252600193024, -0.027644194662570953, -0.08522046357393265, -0.025168946012854576, 0.11015945672988892, -0.04527200385928154, -0.047056134790182114, -0.016719428822398186, 0.02236684411764145, -0.03911849856376648, -0.003360390430316329, -0.005907782819122076, 0.01177327148616314, -0.04848776385188103, 0.039522428065538406], [0.008080687373876572, -0.09539895504713058, -0.03077339194715023, -0.02263193391263485, -0.07728616148233414, 0.03247293829917908, 0.0864434763789177, -0.017486222088336945, 0.023008612915873528, 0.026931805536150932, -0.01291318703442812, -0.09954972565174103, -0.025455381721258163, -0.06620167195796967, -0.07048176974058151, 0.04873644933104515, -0.0705658346414566, -0.10960454493761063, -0.009814375080168247, -0.0803423523902893, 0.02983175590634346, 0.021925119683146477, 0.09854543209075928, 0.0009720725938677788, 0.00508861243724823, -0.03540317341685295, 0.01240542158484459, -0.022730454802513123, -0.027505358681082726, -0.002322598360478878, -0.01134045422077179, 0.056228406727313995, 0.02918352000415325, -0.014123535715043545, -0.011095214635133743, -0.03805048391222954, -0.040913745760917664, 0.03215605020523071, -0.005078339483588934, 0.0523952841758728, 0.02833070233464241, -0.06980034708976746, 0.036486219614744186, 0.02715996839106083, 0.04318196699023247, -0.038291025906801224, 0.03963205963373184, -0.027299789711833, 0.018662603572010994, -0.0519438236951828, 0.0167168490588665, 0.07359468191862106, -0.03211061656475067, 0.0520259328186512, -0.003587458748370409, -0.07705038785934448, -0.049508046358823776, 0.03334400802850723, -0.059553928673267365, 0.06343348324298859, 0.07417022436857224, -0.042090486735105515, 0.003046490717679262, -0.011782924644649029, 0.0510990209877491, -0.00013503231457434595, -0.08637797832489014, -0.12473419308662415, -0.04188786819577217, 0.07785393297672272, 0.03680676221847534, 0.04839124530553818, 0.006133830174803734, -0.02402057871222496, -0.027002234011888504, -0.05617440864443779, 0.022975487634539604, -0.06880675256252289, 0.03747402876615524, 0.02555449865758419, -0.05626777932047844, 0.0705253854393959, 0.018991274759173393, 0.041840676218271255, -0.0028655442874878645, -0.0772162452340126, 0.025495853275060654, 0.009666132740676403, -0.07313146442174911, 0.0824606642127037, 0.06467892229557037, 0.010289378464221954, 0.14305511116981506, 0.04800071567296982, -0.03249432146549225, 0.023189915344119072, 0.015835540369153023, -0.03490113094449043, -0.04355061054229736, 0.07404258102178574, 0.03346410021185875, 0.10190630704164505, 0.03602411225438118, 0.034282829612493515, 0.09003107249736786, 0.07280084490776062, 0.024140695109963417, 0.02089696191251278, -0.09953878074884415, 0.009472833015024662, -0.05451291427016258, -0.07355301082134247, 0.06916221231222153, -0.0020786644890904427, -0.03501380607485771, 0.06568209081888199, 0.03277265653014183, 0.04597487673163414, 0.08742590993642807, -0.07478143274784088, 0.1032869964838028, -0.031894002109766006, 0.01201486773788929, 0.007906651124358177, -0.03202249854803085, -0.05379028618335724, -0.0037271413020789623, 1.8279130009828503e-34, 0.005009274464100599, 0.03918767720460892, -0.02847479097545147, 0.04069354012608528, 0.045892223715782166, 0.04610345512628555, -0.026543283835053444, -0.04818358272314072, 0.020885905250906944, -0.002028796821832657, 0.08167649060487747, -0.011398823000490665, 0.00022295571397989988, -0.0018901022849604487, 0.02073175087571144, 0.027863027527928352, 0.01790277659893036, 0.08875709772109985, 0.01471535861492157, -0.07468044757843018, -0.04869888350367546, 0.07993563264608383, -0.06348967552185059, 0.06559175252914429, -0.04011175036430359, -0.0056682368740439415, 0.007279994897544384, -0.050563745200634, -0.01241371501237154, -0.002010038821026683, 0.008123479783535004, 0.04699467122554779, 0.05535563454031944, 0.020346226170659065, -0.0012217145413160324, -0.10184325277805328, 0.10169912129640579, -0.034093622118234634, -0.030902963131666183, 0.008021325804293156, -0.015029323287308216, 0.0008118065888993442, 0.046915698796510696, 0.01217610016465187, 0.054647140204906464, 0.044779833406209946, -0.00925339013338089, -0.008242915384471416, 0.009976963512599468, 0.017530158162117004, 0.010699541307985783, 0.037541769444942474, 0.05122930929064751, 0.06261458992958069, 0.02553327940404415, -0.00370660237967968, 0.06957685202360153, -0.010279889218509197, 0.026150884106755257, 0.002082184888422489, 0.0038684229366481304, -0.02077591046690941, 0.012700356543064117, -0.041210442781448364, -0.08708060532808304, -0.052669450640678406, -0.049883004277944565, 0.0401550754904747, 2.338400463486323e-06, 0.0674474760890007, -0.020083846524357796, 0.04997158423066139, -0.06747264415025711, -0.08824125677347183, -0.03517410159111023, -0.0626448318362236, 0.04031972587108612, -0.022484255954623222, -0.03776632994413376, -0.12003501504659653, -0.01691046543419361, 0.09409453719854355, -0.020762166008353233, 0.05564185231924057, -0.06826724857091904, 0.036217059940099716, -0.032617028802633286, -0.06822937726974487, 0.09408800303936005, -0.03672685846686363, -0.021736174821853638, 0.009966407902538776, 0.023078206926584244, -0.008611306548118591, -0.06199764087796211, 8.941960731121178e-34, 0.0003635684843175113, 0.012690499424934387, 0.026589151471853256, 0.0647854134440422, 0.02548210322856903, 0.02860981412231922, 0.0012570091057568789, 0.056589044630527496, 0.0781116858124733, 0.020748931914567947, 0.016074081882834435, -0.05106383189558983, 0.03079257719218731, -0.056950680911540985, 0.12897659838199615, 0.00837709940969944, 0.051132943481206894, -0.031216761097311974, -0.04527473449707031, -0.004246345721185207, 0.049710001796483994, -0.03990734741091728, -0.028115244582295418, 0.047749023884534836, -0.049286723136901855, 0.07545079290866852, -0.030531425029039383, -0.07274262607097626, 0.015440781600773335, -0.0017576577374711633, 0.03929734230041504, -0.045735444873571396, 0.03634234890341759, 0.035006627440452576, 0.006279601715505123, -0.01770067773759365, 0.07144677639007568, 0.0407305508852005, -0.03948710858821869, 0.07122261077165604, 0.11299609392881393, -0.013451789505779743, 0.017438773065805435, 0.030202750116586685, -0.0057992772199213505, -0.0002120134886354208, 0.044797804206609726, 0.020607180893421173, -0.051776230335235596, 0.039304330945014954, -0.06698288023471832, -0.021190840750932693, -0.08656776696443558, -0.02995225228369236, 0.023406166583299637, -0.010621948167681694, -0.006940520368516445, -0.027040064334869385, 0.07219148427248001, -0.023389726877212524, -0.002373557770624757, -0.018377937376499176, -0.0019894749857485294, 0.032389890402555466, 0.021187711507081985, -0.03936567157506943, -0.08798240125179291, 0.136001855134964, -0.06605537235736847, -0.060855720192193985, -0.04243520274758339, 0.09640294313430786, 0.040865395218133926, -0.09723322093486786, 0.006801824551075697, -0.008721226826310158, -0.09353087842464447, 0.0005397773929871619, 0.007901028729975224, 0.053111523389816284, -0.10052894055843353, -0.03189708665013313, 0.03413191810250282, 0.003766113193705678, -0.045441221445798874, -0.03456147760152817, 0.18371504545211792, -0.0054534305818378925, 0.03959442675113678, -0.02587738446891308, 0.049547191709280014, 0.025953169912099838, -0.029081063345074654, 0.03526005521416664, -0.0056333173997700214, -1.9027329045684382e-08, 0.012451286427676678, 0.053635142743587494, -0.03357088938355446, -0.07271046191453934, -0.008326173759996891, 0.05630640313029289, -0.0463874489068985, 0.08589574694633484, 0.04216993227601051, 0.03112049587070942, 0.0577724426984787, -0.039090339094400406, 0.014506172388792038, 0.022036978974938393, -0.049630746245384216, 0.06102835386991501, 0.01751246117055416, 0.011143133975565434, -0.014998409897089005, 0.020809482783079147, -0.012584014795720577, 0.02053351327776909, -0.016007322818040848, 0.04199579730629921, -0.05905056372284889, 0.08003123849630356, -0.05422866344451904, 0.09618330746889114, -0.044407665729522705, 0.07981470972299576, -0.004549216479063034, 0.028898943215608597, -0.05525489151477814, -0.0499129481613636, 0.01971735805273056, 0.002761144656687975, -0.01193176582455635, -0.05236753821372986, 0.018666550517082214, 0.08221225440502167, 0.01560571976006031, -0.026658572256565094, -0.027583222836256027, -0.050311632454395294, -0.028661256656050682, -0.010513980872929096, -0.07250864803791046, -0.06864107400178909, -0.08933323621749878, -0.13008244335651398, -0.07196878641843796, -0.009834012016654015, 0.022902237251400948, 0.07230094820261002, 0.14229953289031982, -0.0970807820558548, -0.033600106835365295, -0.006086279638111591, -0.10948298871517181, -0.01704154908657074, 0.02816792018711567, 0.034415554255247116, 0.011432654224336147, 0.006576744373887777]]\n",
      "\n",
      "TIME FOR TEXT EMBEDDINGS:  1.8350017070770264\n",
      "\n",
      "- - - - - -\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 111.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.75it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.04it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.74it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.89it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09269339591264725, 0.005343963857740164, -0.021850140765309334, -0.0007701202412135899, 0.04873262345790863, 0.028574742376804352, 0.06419852375984192, -0.009560701437294483, 0.053361136466264725, -0.08790569752454758, -0.014494866132736206, -0.001336785382591188, 0.09828134626150131, -0.01316970493644476, 0.03653036430478096, 0.08743909001350403, 0.05540040507912636, 0.0015357804950326681, -0.03637179359793663, 0.008946479298174381, -0.08317054063081741, 0.093168705701828, -0.04679816961288452, -0.02023383602499962, -0.04995940253138542, -0.041779037564992905, -0.015589887276291847, 0.03397413715720177, -0.014390932396054268, 0.10862997174263, 0.020122425630688667, -0.01385401375591755, 0.046556368470191956, 0.014802731573581696, 0.0009073873516172171, -0.003735911101102829, 0.007216203957796097, 0.047422852367162704, 0.015439030714333057, 0.013636776246130466, 0.012506699189543724, 0.008980418555438519, -0.0021204580552875996, -0.07505076378583908, -0.0013401039177551866, -0.0007081553339958191, -0.03652453422546387, 0.02981887385249138, 0.026721332222223282, -0.0864005908370018, -0.0841793641448021, 0.006127772852778435, -0.05923665687441826, 0.059426795691251755, 0.005135268904268742, 0.005794576369225979, -0.007825948297977448, 0.11261796206235886, -0.020295362919569016, 0.028986873105168343, 0.05184927582740784, -0.027010895311832428, -0.002103075385093689, -0.04580674320459366, 0.012710753828287125, 0.09872579574584961, 0.03393741697072983, 0.06304049491882324, -0.07262536883354187, 0.07092616707086563, -0.046225618571043015, 0.1002376452088356, 0.11196375638246536, 0.018410421907901764, -0.0037345583550632, -0.01472923532128334, 0.02416231296956539, -0.04097253084182739, 0.04131736978888512, -0.022084083408117294, -0.01647752709686756, -0.01220319326967001, -0.049599070101976395, -0.07597590982913971, -0.01710217446088791, -0.002103803213685751, 0.045190729200839996, -0.01898171938955784, 0.013932657428085804, 0.020077990368008614, 0.006097567267715931, 0.037746891379356384, 0.01981831155717373, -0.05266400799155235, 0.06843791902065277, -0.04510204866528511, -0.12242233753204346, 0.0420515313744545, -0.023320375010371208, 0.0456855334341526, 0.017341913655400276, -0.014790228568017483, -0.02733195200562477, -0.01745518669486046, 0.07769669592380524, -0.02074703946709633, -0.0873873382806778, -0.03205585852265358, -0.09111242741346359, 0.03686106950044632, -0.033110618591308594, -0.04013456404209137, -0.01698370836675167, -0.004547453019768, 0.034501608461141586, 0.07777281105518341, 0.0819196030497551, 0.0609041191637516, -0.02624420076608658, 0.008872163482010365, 0.03701310232281685, 0.047796525061130524, -0.026972470805048943, 0.1592535674571991, -0.02717648446559906, -0.07614269852638245, -0.062316883355379105, -2.571606767103848e-33, -0.04442668333649635, 0.007310512941330671, -0.03771059215068817, -0.03247486799955368, 0.019967922940850258, -0.030969128012657166, -0.08463769406080246, -0.05956822633743286, 0.09636646509170532, -0.011295587755739689, -0.03427676111459732, -0.03152259811758995, 0.04891122505068779, 0.00795743241906166, 0.05912664905190468, 0.020901115611195564, -0.039653997868299484, 0.06244846433401108, -0.017728256061673164, -0.06113918498158455, 0.012838946655392647, -0.021642878651618958, -0.023020198568701744, 0.005752484779804945, -0.061735451221466064, -0.14628495275974274, 0.05189298093318939, -0.0004945305990986526, -0.023757480084896088, 0.01143493503332138, -0.09186823666095734, -0.07906462252140045, -0.00751766562461853, -0.007254007272422314, 0.0460025817155838, -0.014169910922646523, 0.0404745377600193, 0.04869122803211212, -0.00924608577042818, -0.07348715513944626, 0.010274073109030724, 0.08677573502063751, -0.0261799618601799, -0.021404141560196877, 0.11437875777482986, 0.06024293228983879, 0.010301555506885052, -0.009255990386009216, -0.06202419847249985, -0.04021848365664482, 0.00999776553362608, -0.009421628899872303, 0.024818118661642075, 0.0540819950401783, -0.040780436247587204, -0.022144027054309845, -0.05682733654975891, 0.040815845131874084, 0.043874047696590424, -0.005246268119663, -0.10526279360055923, 0.11851850152015686, -0.024268455803394318, 0.03979957476258278, 0.013352404348552227, -0.007793786935508251, -0.05901431664824486, -0.014424818567931652, -0.04775921255350113, -0.05760325491428375, 0.1139209121465683, -0.01605174131691456, 0.021209031343460083, 0.04799368977546692, 0.00950566865503788, -0.04933144524693489, -0.023286504670977592, 0.0020122891291975975, 0.11531011760234833, -0.01627112738788128, 0.13887877762317657, 0.03264719247817993, -0.05117451772093773, -0.038163818418979645, 0.003204391337931156, -0.01596914231777191, 0.05639095976948738, -0.03908765688538551, 0.08742183446884155, -0.015590055845677853, -0.030634785071015358, 0.03628954663872719, -0.02492007613182068, -0.056513700634241104, -0.11102378368377686, 6.076948094493691e-34, -0.06599248200654984, -0.008227710612118244, 0.044252246618270874, 0.0838887169957161, -0.11768157035112381, 0.08831872791051865, -0.018532628193497658, 0.005737954750657082, 0.06345856189727783, 0.031013401225209236, 0.02652752213180065, -0.019324084743857384, 0.006759685464203358, 0.009838071651756763, 0.026788435876369476, -0.10548333823680878, -0.026550402864813805, -0.03355967625975609, -0.0779697373509407, -0.02089620940387249, -0.03958207368850708, 0.07856199145317078, -0.020350808277726173, 0.025479495525360107, -0.03126659244298935, -0.07198558747768402, 0.11346635222434998, -0.01793432980775833, 0.10661372542381287, -0.07247735559940338, 0.004973023664206266, 0.028926922008395195, -0.08149190992116928, -0.06507790833711624, -0.006710117217153311, -0.03185900300741196, -0.09547320008277893, 0.07638614624738693, -0.01799030974507332, -0.027803514152765274, -0.0318903774023056, 0.0006965161301195621, -0.03240278735756874, -0.1326095461845398, -0.05504118651151657, -0.04721912369132042, 0.07467763870954514, -0.02727324888110161, 0.025745699182152748, -0.00888131745159626, -0.05085988715291023, -0.02258465066552162, 0.10509804636240005, 0.025873655453324318, -0.029957395046949387, -0.05243976041674614, 0.0764097273349762, -0.016877589747309685, -0.08452364057302475, -0.07604221254587173, 0.003783260937780142, 0.01004392746835947, -0.02630268782377243, -0.05206356197595596, 0.08440501242876053, -0.06404557824134827, -0.09405900537967682, 0.005777649115771055, 0.07394684106111526, -0.03362574800848961, -0.03526247292757034, 0.01594541035592556, -0.047908809036016464, 0.02515123039484024, -0.03931703418493271, 0.015969635918736458, -0.027216076850891113, 0.005253611132502556, 0.0251956507563591, 0.0009437250555492938, 0.05801168456673622, 0.013217540457844734, 0.03778984025120735, 0.0008823121315799654, -0.03796043619513512, -0.06345687061548233, -0.001116667641326785, -0.041566476225852966, 0.023284463211894035, 0.01180362980812788, 0.06208205595612526, -0.023282550275325775, 0.024088574573397636, -0.017585230991244316, 0.01568565145134926, -2.968228685062968e-08, 0.010659657418727875, -0.05632993206381798, 0.07712186127901077, 0.051017243415117264, -0.011079953983426094, -0.000580736028496176, 0.002367486711591482, -0.0185921099036932, 0.03200192749500275, 0.04977913200855255, -0.052581243216991425, -0.04296504333615303, -0.03333304449915886, 0.020073184743523598, -0.08089340478181839, 0.07841378450393677, 0.08487143367528915, -0.1342170089483261, 0.03482994809746742, -0.045340292155742645, 0.008037593215703964, 0.05420054495334625, -0.03073020838201046, 0.024961788207292557, 0.01155844796448946, 0.025057131424546242, 0.04879529029130936, -0.029437700286507607, -0.023896224796772003, 0.024895433336496353, 0.04927956685423851, 0.01198319811373949, -0.12020030617713928, 0.005318841896951199, 0.016125230118632317, 0.004452244378626347, -0.08056430518627167, 0.017642658203840256, 0.03630787134170532, 0.08196454495191574, -0.025738459080457687, 0.04738208279013634, 0.03808916360139847, -0.005146329291164875, -0.0029029264114797115, -0.04597631096839905, 0.00838913768529892, -0.0348191075026989, -0.04001341015100479, 0.013514922931790352, 0.0706583559513092, -0.004429548978805542, 0.04025432467460632, -0.03740037977695465, 0.08259780704975128, 0.02911447361111641, 0.0035791699774563313, -0.008804630488157272, -0.050127062946558, 0.04007802903652191, 0.03561465069651604, 0.0723814144730568, 0.019006803631782532, 0.004914341028779745], [0.042391110211610794, 0.06892799586057663, -0.01282963901758194, 0.022921059280633926, -0.03198853135108948, -0.039736129343509674, -0.013293086551129818, -0.009726453572511673, 0.10041041672229767, -0.04075907915830612, 0.055343955755233765, -0.009186532348394394, 0.10898087918758392, 0.014859226532280445, 0.021831752732396126, 0.07840777933597565, 0.05953110381960869, 0.0903114303946495, -0.0017572504002600908, 0.03334146365523338, 0.03440382704138756, -0.010233321227133274, -0.005508245900273323, 0.06482840329408646, -0.04752602428197861, -0.060290463268756866, -0.035957396030426025, 0.0989808589220047, 0.010417562909424305, 0.09730387479066849, 0.010743101127445698, 0.02495148405432701, -0.05293605476617813, 0.04062078893184662, -0.05360967293381691, -0.04439688101410866, 0.0011170428479090333, 0.08049372583627701, -0.01614558883011341, -0.09232454001903534, 0.005971175618469715, -0.03063441626727581, -0.06689071655273438, -0.015797000378370285, -0.08515523374080658, -0.016341811046004295, 0.05599726736545563, 0.00024008550099097192, -0.0015571662224829197, -0.06100603565573692, -0.0778840109705925, -0.00728037441149354, -0.09796090424060822, -0.0033349026925861835, 0.014183889143168926, -0.007687258068472147, -0.05101575329899788, 0.09605477750301361, -0.004167760256677866, 0.03168299049139023, -0.013458452187478542, 0.040182821452617645, 0.004957696422934532, 0.01872018538415432, 0.039810288697481155, -0.01950615458190441, 0.014330899342894554, 0.01994737982749939, -0.08653590083122253, 0.10226568579673767, -0.02060811035335064, 0.07482320815324783, 0.13921281695365906, 0.04058513045310974, -0.03484804183244705, -0.008807655423879623, -0.015144114382565022, 0.03913534805178642, -0.022510580718517303, -0.004130078013986349, -0.068105548620224, 0.06046317517757416, 0.02317235805094242, 0.0096245426684618, 0.00609600218012929, 0.03591662645339966, 0.020946407690644264, -0.042256299406290054, 0.03185207396745682, -0.03081221505999565, -0.09172669053077698, -0.00982588343322277, 0.11844391375780106, -0.07484181225299835, 0.07271206378936768, -0.09946553409099579, -0.015054541639983654, 0.03605061024427414, -0.051582396030426025, 0.0494629330933094, 0.005246568005532026, 0.002047863556072116, -0.07226938754320145, -0.030025143176317215, 0.0012676991755142808, -0.05315452441573143, -0.08411939442157745, -0.027568230405449867, -0.05262225493788719, -0.09791841357946396, -0.09486532211303711, -0.02286231704056263, 0.019658563658595085, -0.0667785108089447, 0.04014848917722702, -0.026300622150301933, 0.039085857570171356, 0.04480913281440735, 0.08304349333047867, -0.015867801383137703, 0.02567136101424694, -0.034302324056625366, -0.008837074972689152, 0.10396867990493774, 0.060834817588329315, -0.03947168216109276, -0.053554873913526535, -1.6747699642997073e-33, -0.019451923668384552, 0.08432457596063614, 0.02281731553375721, 0.006542837712913752, -0.06658326834440231, -0.004636140074580908, -0.04795597121119499, 0.06363356858491898, 0.0978030264377594, -0.011961827054619789, 0.05763581767678261, -0.010652470402419567, 0.087914377450943, 0.03254757821559906, 0.08301311731338501, 0.02048838697373867, -0.03641009330749512, 0.06959380954504013, 0.02007180079817772, -0.004830503836274147, 0.053324759006500244, -0.01059373002499342, -0.030535465106368065, -0.022521577775478363, -0.032598827034235, -0.0894879400730133, 0.014341423287987709, 0.03988238051533699, 0.01617172174155712, 0.0009915080154314637, -0.0477491095662117, -0.05678429454565048, 0.02211269550025463, -0.01710520312190056, 0.09063855558633804, -0.05110379680991173, 0.04342883825302124, 0.047579459846019745, -0.016882847994565964, -0.054539188742637634, 0.016387881711125374, 0.052501559257507324, -0.055298950523138046, -0.003732291515916586, 0.058746714144945145, 0.09915713965892792, 0.08661483973264694, -0.07828589528799057, 0.021427467465400696, -0.004860812798142433, 0.08748240023851395, 0.0006012074300087988, 0.08024867624044418, 0.05296998471021652, -0.026026949286460876, -0.034695591777563095, -0.055124491453170776, 0.02812461368739605, -0.0020979614928364754, 0.008975391276180744, -0.09244360774755478, 0.06195405125617981, -0.0245231743901968, 0.04758520424365997, 0.01543363370001316, -0.03904944285750389, -0.020271962508559227, 0.07893208414316177, -0.013554195873439312, -0.05646127089858055, 0.08298981189727783, -0.005623143166303635, -0.0380641408264637, 0.0022032891865819693, -0.03374113887548447, -0.053266800940036774, 0.00014691318210680038, 0.08186450600624084, 0.11419805139303207, -0.03600407764315605, 0.03566611558198929, -0.009456805884838104, -0.004530446603894234, -0.06371427327394485, -0.0692603662610054, 0.0032274469267576933, 0.07997224479913712, -0.03826355189085007, 0.061579976230859756, -0.050329457968473434, -0.048905011266469955, 0.00041054017492569983, -0.055720895528793335, 0.03886190056800842, -0.12815143167972565, 7.785829894663803e-34, -0.062047094106674194, -0.09857071936130524, -0.0030745675321668386, 0.10212291032075882, -0.0445769838988781, 0.025363601744174957, -0.021969858556985855, -0.041382502764463425, 0.05155047029256821, 0.07402732968330383, 0.03752948343753815, -0.03085126169025898, -0.07246626168489456, -0.0082863112911582, 0.03220484033226967, -0.07615528255701065, -0.020504271611571312, -0.019862566143274307, -0.1005212664604187, 0.050881367176771164, -0.02343101054430008, 0.09627659618854523, -0.09565428644418716, 0.0169856958091259, 0.009336722083389759, -0.03730775788426399, 0.051705021411180496, 0.00535694882273674, -0.013238709419965744, -0.06268023699522018, 0.0012461217120289803, 0.028150876984000206, -0.024998007342219353, -0.057686153799295425, -0.05247633159160614, -0.04090113565325737, -0.015466161072254181, 0.04910755157470703, 0.0032370747067034245, -0.02469911053776741, 0.037779051810503006, 0.016371123492717743, -0.08349844813346863, -0.03191877156496048, 0.003874062094837427, -0.03890844061970711, -0.008297880180180073, -0.05644873529672623, 0.06771361827850342, 0.0043041459284722805, -0.08550667762756348, -0.07297293096780777, -0.05235593020915985, -0.020406246185302734, -0.030583826825022697, -0.0597553551197052, 0.06802114844322205, -0.04854077100753784, -0.049692895263433456, -0.041422806680202484, 0.021468427032232285, -0.06195485219359398, -0.03828997164964676, 0.05977611243724823, 0.05406433716416359, -0.025965016335248947, -0.05721605569124222, -0.026482343673706055, 0.08805744349956512, -0.01650707982480526, -0.060977738350629807, -0.010216308757662773, -0.03592359647154808, -0.017445407807826996, -0.05415778234601021, -0.04324375465512276, 0.04619666188955307, -0.03868400305509567, -0.033561646938323975, 0.03765644133090973, 0.0053949858993291855, -0.06539969146251678, -0.01468929834663868, -0.006559322588145733, -0.044426318258047104, -0.006270533427596092, -0.03859392926096916, 0.04986262694001198, 0.023595498874783516, -0.008817187510430813, -0.013707580044865608, -0.035784680396318436, 0.03137576952576637, 0.03396354988217354, 0.02421274594962597, -2.8516787153876066e-08, -0.048864223062992096, -0.030507231131196022, 0.10325788706541061, 0.1082780510187149, -0.022523613646626472, 0.014312482438981533, -0.030158106237649918, 0.010144216939806938, 0.013477301225066185, 0.01415900606662035, 0.005809417460113764, -0.0715770497918129, -0.06331880390644073, 0.002155028749257326, -0.05211935564875603, 0.06510467827320099, 0.0931762084364891, -0.006258800160139799, 0.029944920912384987, 0.07327692955732346, -0.05019058659672737, 0.0765952318906784, -0.03198276460170746, 0.02488313987851143, 0.04175811633467674, -0.04858541116118431, 0.03648790344595909, -0.029131151735782623, 0.026969406753778458, -0.03355114907026291, -0.009886210784316063, 0.0021694398019462824, -0.05786554515361786, 0.06101328134536743, 0.02921176515519619, 0.05330384895205498, -0.0892651379108429, 0.07885835319757462, 0.0333186574280262, 0.02566949650645256, -0.029336966574192047, 0.06813660264015198, 0.07020705193281174, -0.01633176952600479, 0.07497339695692062, 0.03991151228547096, -0.014953036792576313, 0.027499690651893616, -0.020935744047164917, -0.05608295649290085, 0.0750143900513649, 0.05228827893733978, -0.02466311864554882, 0.011798178777098656, 0.09885260462760925, -0.028668994084000587, 0.013835448771715164, 0.025909459218382835, -0.037930842489004135, 0.10386265814304352, 0.09220699965953827, -0.02454003132879734, 0.04758813604712486, 0.009064464829862118], [0.039102502167224884, 0.0077433413825929165, -0.02201317436993122, -0.015991581603884697, -0.021642720326781273, -0.048065926879644394, -0.0029944332782179117, -0.042008183896541595, -0.008370066992938519, -0.04118100181221962, 0.018903018906712532, 0.06546349823474884, 0.03265310451388359, -0.049915727227926254, 0.03711463510990143, 0.00011840478691738099, -0.010946930386126041, 0.0014797481708228588, 0.009033869951963425, 0.05914042517542839, 0.13843846321105957, 0.06440233439207077, 0.02037491276860237, -0.03613514080643654, -0.04586120694875717, -0.020132919773459435, 0.048833269625902176, -0.004101743455976248, 0.014790368266403675, 0.07851655781269073, -0.025900335982441902, -0.035648100078105927, -0.032664816826581955, 0.0063012042082846165, -0.08119012415409088, -0.017556026577949524, 0.011737693101167679, 0.0014237741706892848, 0.10744471848011017, 0.027540165930986404, -0.02289850264787674, -0.07348615676164627, -0.06519418209791183, -0.07355978339910507, 0.010595681145787239, 0.0005712237907573581, -0.048158057034015656, 0.0041481321677565575, -0.06206367164850235, -0.05449174344539642, 0.014974880963563919, -0.025528965517878532, 0.05520864203572273, 0.012727903202176094, 0.017470363527536392, 0.01945176161825657, -0.017539912834763527, -0.056711580604314804, 0.011829095892608166, -0.03755927085876465, 0.0006530042155645788, -0.014763103798031807, 0.009313790127635002, -0.012957451865077019, -0.0008688203524798155, -0.0695723369717598, 0.01881404221057892, -0.002321271225810051, -0.038261529058218, 0.11319082230329514, -0.10528169572353363, 0.041124340146780014, 0.042678456753492355, 0.09569409489631653, -0.0909867212176323, -0.011651451699435711, 0.003289845073595643, 0.05039920657873154, 0.057041242718696594, -0.030547719448804855, 0.09166645258665085, -0.012251063250005245, 0.09599253535270691, -0.00030559024889953434, 0.009918387979269028, -0.029934195801615715, -0.01046021655201912, -0.006370866671204567, 0.019265925511717796, 0.05823637172579765, -0.08773975819349289, 0.031304750591516495, 0.08434275537729263, -0.027542375028133392, 0.029967734590172768, -0.13131295144557953, -0.06263110786676407, 0.06378239393234253, -0.11692164093255997, 0.050324659794569016, -0.004831966012716293, 0.06255695968866348, -0.007171456702053547, -0.003189781215041876, 0.034803565591573715, 0.005655182991176844, -0.06753609329462051, -0.0622219480574131, -0.07642146199941635, 0.016786187887191772, -0.10606080293655396, -0.02053491584956646, -0.016881214454770088, 0.0013107951963320374, 0.08634883165359497, -0.015982717275619507, 0.0838278979063034, -0.003068737220019102, -0.1126837432384491, 0.0056236074306070805, -0.04542312026023865, 0.007046782411634922, -0.034118443727493286, 0.0833304226398468, 0.05051173269748688, -0.06212342157959938, -0.056229524314403534, -1.9702429590063667e-33, 0.009972769767045975, 0.034177515655756, -0.026245763525366783, 0.002249404788017273, -0.047032516449689865, 0.03635029494762421, -0.008383549749851227, 0.01550038531422615, 0.027656223624944687, -0.01474906224757433, 0.08827196806669235, 0.007868217304348946, -0.01263421680778265, -0.006944627966731787, -0.04178592190146446, 0.07454120367765427, 0.03881854563951492, 0.07198686897754669, -0.0043084933422505856, -0.02593681961297989, 0.043923504650592804, -0.005510818213224411, 0.024246295914053917, -0.008011441677808762, -0.13651539385318756, -0.08252846449613571, 0.02996542677283287, -0.015615910291671753, 0.051095761358737946, 0.02376297488808632, -0.02001320943236351, -0.03341009095311165, 0.0455818809568882, -0.009435838088393211, 0.058692216873168945, -0.05626879259943962, 0.08645404875278473, -0.007330913096666336, -0.08972913026809692, 0.04107813909649849, 0.014463944360613823, 0.02071603201329708, 0.03614037111401558, -0.039914827793836594, -0.04470888152718544, 0.16271544992923737, 0.07835032790899277, -0.04905466362833977, 0.01732143945991993, 0.04628936946392059, 0.03418669477105141, 0.08108875900506973, 0.020181065425276756, 0.0944700762629509, 0.019679464399814606, -0.028278514742851257, -0.037433020770549774, 0.01393614336848259, 0.002593676559627056, -0.06163511425256729, -0.04462888091802597, 0.006370142102241516, 0.05158288776874542, -0.08064810931682587, -0.005140363238751888, 0.0346807986497879, -0.08304306864738464, 0.061240412294864655, -0.05047179386019707, 0.018608983606100082, -0.006359152030199766, -0.04372197762131691, 0.008152473717927933, 0.09087327867746353, -0.0172126404941082, -0.007383451797068119, 0.01933998614549637, 0.002408672822639346, 0.1494971662759781, 0.01648295857012272, 0.025018595159053802, -0.0322704091668129, -0.05681054666638374, -0.10758623480796814, -0.06724724173545837, -0.0582033209502697, 0.004688500892370939, -0.05842399224638939, 0.054912958294153214, 0.004966565873473883, -0.10002335160970688, 0.04028062894940376, -0.0652826800942421, -0.07129672914743423, -0.08578091859817505, -7.346439289876048e-34, 0.015022045001387596, -0.04380142688751221, -0.03651130571961403, 0.12815694510936737, -0.005193468648940325, 0.040540557354688644, 0.09960875660181046, -0.029112691059708595, 0.045335058122873306, 0.057524193078279495, 0.0025074868462979794, -0.07275514304637909, 0.015495708212256432, -0.040265727788209915, 0.17449897527694702, 0.025539718568325043, 0.001990425633266568, -0.05837472528219223, 0.019099600613117218, -0.0048336125910282135, 0.05545741319656372, 0.07378887385129929, -0.030951766297221184, 0.006717422977089882, -0.0006809103069826961, -0.017392238602042198, 0.025430358946323395, -0.030652470886707306, 0.005129409953951836, -0.052320532500743866, 0.0025147683918476105, 0.04058811068534851, -0.001207880675792694, -0.04978233948349953, -0.021315865218639374, 0.003512029768899083, 0.05702340602874756, 0.11029922962188721, -0.025489794090390205, -0.12624576687812805, -0.010184057056903839, -0.05059882998466492, -0.02831243723630905, 0.10773565620183945, 0.13125748932361603, -0.10685170441865921, -0.0554363913834095, -0.01961083710193634, -0.0022404235787689686, 0.037632815539836884, -0.03856448084115982, -0.08129426836967468, -0.013313677161931992, -0.01635376177728176, -0.02115081064403057, -0.07075013965368271, -0.018759533762931824, -0.01628655567765236, -0.1356329619884491, 0.035967521369457245, 0.034957654774188995, 0.05900725722312927, -0.11144813150167465, -0.016741842031478882, 0.05668899789452553, -0.020417891442775726, 0.013170097954571247, -0.022222384810447693, 0.02981436811387539, -0.030071932822465897, -0.04390143230557442, -0.026948964223265648, -0.021247271448373795, -0.0496060736477375, 0.012206927873194218, -0.0030109123326838017, 0.03446090221405029, 0.018151698634028435, -0.01892654038965702, 0.0077169411815702915, 0.03479904308915138, -0.028601400554180145, 0.03242857754230499, 0.00034195935586467385, -0.00843929685652256, -0.04256342723965645, 0.034708987921476364, 0.00969965010881424, 0.018281636759638786, -0.027963489294052124, -0.061409302055835724, -0.06877729296684265, -0.044503603130578995, 0.021961545571684837, 0.06042836233973503, -2.4255573549680776e-08, -0.06133615970611572, -0.03689973056316376, 0.02163887210190296, 0.04065706208348274, 0.006360011640936136, 0.062295716255903244, -0.030729245394468307, -0.06051264703273773, 0.0353313609957695, -0.01492356974631548, -0.019891545176506042, -0.006494318135082722, -0.01508823037147522, 0.0015393494395539165, -0.02515590749680996, 0.04537798464298248, 0.008658086881041527, 0.008639425039291382, -0.014319688081741333, 0.020847022533416748, -0.018409347161650658, 0.011788720265030861, -0.02062615565955639, 0.11014015972614288, 0.00815326813608408, -0.0036044984590262175, 0.022485654801130295, -0.07278367131948471, -0.00183672655839473, 0.009594516828656197, 0.07062877714633942, 0.029361629858613014, 0.0016945285024121404, 0.03472547233104706, 0.012408244423568249, -0.04924411326646805, -0.05693984404206276, 0.03716433793306351, 0.14618204534053802, 0.03244385868310928, -0.02653387561440468, 0.01275681983679533, 0.03431591019034386, -0.002326775575056672, 0.033377908170223236, 0.05610354617238045, -0.07454033941030502, 0.03579788655042648, -0.03544606268405914, 0.05269218608736992, 0.013791150413453579, 0.004140076693147421, -0.050181515514850616, 0.08103524893522263, 0.07455185800790787, -0.023226916790008545, -0.03892036899924278, 0.005684673320502043, -0.01958771049976349, 0.016143081709742546, 0.0736541897058487, -0.0479019470512867, 0.038674354553222656, -0.051331304013729095]]\n",
      "\n",
      "TIME FOR RESPONSE EMBEDDINGS:  1.5549981594085693\n",
      "\n",
      "- - - - - -\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "content_gab = content_gab[:200]\n",
    "before = time.time()\n",
    "content_gab['text_embeddings'] = content_gab['text'].apply(generate_embeddings)\n",
    "after_text = time.time()\n",
    "print(content_gab.iloc[1]['text_embeddings'])\n",
    "print('\\nTIME FOR TEXT EMBEDDINGS: ', after_text - before)\n",
    "print('\\n- - - - - -\\n')\n",
    "content_gab['response_embeddings'] = content_gab['response'].apply(generate_embeddings)\n",
    "after_response = time.time()\n",
    "print(content_gab.iloc[2]['response_embeddings'])\n",
    "print('\\nTIME FOR RESPONSE EMBEDDINGS: ', after_response - after_text)\n",
    "print('\\n- - - - - -\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for constructing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(row):\n",
    "    text_utterances = row['text_embeddings']\n",
    "    response_utterances = row['response_embeddings']\n",
    "    # text_utterances = row['text']\n",
    "    # response_utterances = row['response']\n",
    "\n",
    "    root = text_utterances[0]\n",
    "    children = text_utterances[1:] + response_utterances\n",
    "    num_nodes = len(children) +1\n",
    "\n",
    "    #for t in text_utterances:\n",
    "     #    print(t)\n",
    "    #print()\n",
    "    #for r in response_utterances:\n",
    "    #     print(r)\n",
    "    # print()\n",
    "    # ids = [[0, i] for i in range(1, num_nodes)]\n",
    "    # print(ids)\n",
    "    # edge_index = torch.tensor(\n",
    "    #     [[0]*num_nodes, list(range(1, num_nodes)\n",
    "    # )], dtype=torch.long)\n",
    "    edge_index = torch.tensor(\n",
    "        [[0, i] for i in range(1, num_nodes)], dtype=torch.long\n",
    "    ).t().contiguous()\n",
    "    # edge_index = torch.tensor(\n",
    "    #     [[0] * len(children), list(range(1, num_nodes))], dtype=torch.long\n",
    "    # )\n",
    "    \n",
    "\n",
    "    # print(row['text_labels_encoded'])\n",
    "    # print()\n",
    "    # print(row['response_labels_encoded'])\n",
    "    # print(type(row['text_labels_encoded']), row['text_labels_encoded'].shape, row['text_labels_encoded'])\n",
    "    # print(type(row['response_labels_encoded']), row['response_labels_encoded'].shape, row['response_labels_encoded'])\n",
    "\n",
    "    # ls = np.concatenate((row['text_labels_encoded'], row['response_labels_encoded']))\n",
    "    ls = np.concatenate((row['text_labels'], row['response_labels'])).astype(int)\n",
    "    \n",
    "    print(ls)\n",
    "\n",
    "    print(ls.shape)\n",
    "    print(type(ls))\n",
    "    print(type(ls[0]))\n",
    "    \n",
    "    labels = torch.tensor(ls, dtype=torch.int32)\n",
    "\n",
    "    print(labels)\n",
    "\n",
    "    node_features = torch.tensor([root] + children, dtype=torch.float)\n",
    "    \n",
    "    # print(node_features.shape)\n",
    "    # print(edge_index.shape)\n",
    "    # print(labels.shape)\n",
    "    # print('sss')\n",
    "    data = Data(x=node_features, edge_index=edge_index, y=labels)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing graphs for all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0]\n",
      "(2,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "(13,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0 0 0]\n",
      "(8,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 0 0 1 0 0 0]\n",
      "(11,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "(18,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       dtype=torch.int32)\n",
      "[1 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(20,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "(18,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 1 0 0 0]\n",
      "(7,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "(12,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0]\n",
      "(16,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0 0 0 0 0 0]\n",
      "(11,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0]\n",
      "(2,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0 1 0 0 0]\n",
      "(10,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "(12,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0 0]\n",
      "(7,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 1 1 0 0 0]\n",
      "(9,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 0]\n",
      "(2,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0 0 0 0]\n",
      "(9,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0 0 0 0 0 0]\n",
      "(11,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(15,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 1 0 0 0 0 0 1 0 0 0 0]\n",
      "(13,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 0 0 1 1 0 0 0]\n",
      "(12,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 1 1 1 0 0 0]\n",
      "(8,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 1, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0]\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0], dtype=torch.int32)\n",
      "[1 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(14,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
      "(14,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "Number of nodes: 4\n",
      "Number of edges: 3\n",
      "\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "graphs = []\n",
    "for index, row in content_gab.iterrows():\n",
    "    graphs.append(construct_graph(row))\n",
    "\n",
    "print(graphs[0])\n",
    "print('\\n- - - - - -\\n')\n",
    "print(f\"Number of nodes: {graphs[0].num_nodes}\")\n",
    "print(f\"Number of edges: {graphs[0].num_edges}\")\n",
    "\n",
    "print()\n",
    "print(len(graphs))\n",
    "#print('Graphs: ')\n",
    "#for i in range(0, 100):\n",
    "#    print(graphs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge to one graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[1103, 384], edge_index=[2, 903], y=[1103])\n",
      "Merged Node Features:\n",
      "tensor([[-0.0227,  0.0438, -0.0208,  ..., -0.0035, -0.1306, -0.0359],\n",
      "        [ 0.0437,  0.0129, -0.0366,  ...,  0.0327,  0.0519, -0.0313],\n",
      "        [ 0.0424,  0.0689, -0.0128,  ..., -0.0245,  0.0476,  0.0091],\n",
      "        ...,\n",
      "        [ 0.0546,  0.0675,  0.0257,  ..., -0.0255, -0.0265,  0.0186],\n",
      "        [-0.0521, -0.0178,  0.0458,  ..., -0.1562,  0.0278, -0.0080],\n",
      "        [ 0.0443,  0.0244, -0.0057,  ...,  0.0071,  0.0421, -0.0769]])\n",
      "Merged Edge Index:\n",
      "tensor([[   0,    0,    0,  ..., 1097, 1097, 1097],\n",
      "        [   1,    2,    3,  ..., 1100, 1101, 1102]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize empty lists to store the merged node features, edge indices, and labels (y)\n",
    "merged_x = []\n",
    "merged_edge_index = []\n",
    "merged_y = []\n",
    "\n",
    "# Keep track of the offset for node indices in subsequent graphs\n",
    "node_offset = 0\n",
    "\n",
    "# Iterate over each graph in the list\n",
    "for graph in graphs:\n",
    "    # Concatenate node features\n",
    "    merged_x.append(graph.x)\n",
    "    \n",
    "    # Adjust edge indices: add the current node_offset to the second row of edge_index\n",
    "    merged_edge_index.append(graph.edge_index + node_offset)\n",
    "    \n",
    "    # Concatenate labels (y), the target labels from each graph\n",
    "    merged_y.append(graph.y)\n",
    "    \n",
    "    # Update node_offset for the next graph\n",
    "    node_offset += graph.x.size(0)\n",
    "\n",
    "# Concatenate all node features, edge indices, and labels\n",
    "merged_x = torch.cat(merged_x, dim=0)\n",
    "merged_edge_index = torch.cat(merged_edge_index, dim=1)\n",
    "merged_y = torch.cat(merged_y, dim=0)\n",
    "\n",
    "# Create a new graph with merged node features, edge indices, and labels (y)\n",
    "merged_graph = Data(x=merged_x, edge_index=merged_edge_index, y=merged_y)\n",
    "\n",
    "print(merged_graph)\n",
    "# Print the merged graph details\n",
    "print(\"Merged Node Features:\")\n",
    "print(merged_graph.x)\n",
    "print(\"Merged Edge Index:\")\n",
    "print(merged_graph.edge_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = []\n",
    "#for index, row in content_gab.iterrows():\n",
    "#    y.append(np.concatenate((row['text_labels'], row['response_labels'])).astype(int))\n",
    "\n",
    "#for i, q in enumerate(y):\n",
    "#    print(q)\n",
    "#    if i >= 5:\n",
    "#       print('\\n- - - -')\n",
    "#       break\n",
    "#for i, q in enumerate(graphs):\n",
    "#    print(q)\n",
    "#    if i >= 5:\n",
    "#       break\n",
    "\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=36851234)\n",
    "#folds = list(rskf.split(graphs, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(384, 16)\n",
      "  (conv2): GCNConv(16, 2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "input_dim = merged_graph.x.shape[1]    # embedding dimensionality\n",
    "data = merged_graph\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(input_dim, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "model.eval()\n",
    "\n",
    "out = model(data.x, data.edge_index)\n",
    "#visualize(out, color=data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "data = merged_graph\n",
    "\n",
    "train_loss_list, train_acc_list, val_loss_list, val_acc_list = [], [], [], []\n",
    "epoch_num = 101\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "    loss = criterion(out, data.y.long())  # Compute the loss for the training dataset.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    pred = out.argmax(dim=1)  # Use the class with the highest probability\n",
    "    train_correct = pred == data.y.long()  # Compare predictions with true labels\n",
    "    train_acc = int(train_correct.sum()) / len(data.y)  # Calculate accuracy\n",
    "\n",
    "    train_loss_list.append(loss.item())  # Append training loss to the list\n",
    "    train_acc_list.append(train_acc)    # Append training accuracy to the list\n",
    "    return loss\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # No gradients for evaluation\n",
    "        out = model(data.x, data.edge_index)  # Forward pass\n",
    "        loss = criterion(out, data.y.long())  # Compute evaluation loss\n",
    "\n",
    "        # Calculate test accuracy\n",
    "        pred = out.argmax(dim=1)  # Use the class with the highest probability\n",
    "        test_correct = pred == data.y.long()  # Compare predictions with true labels\n",
    "        test_acc = int(test_correct.sum()) / len(data.y)  # Calculate accuracy\n",
    "\n",
    "    val_loss_list.append(loss.item())  # Append validation loss to the list\n",
    "    val_acc_list.append(test_acc)     # Append validation accuracy to the list\n",
    "    return test_acc\n",
    "\n",
    "for epoch in range(1, epoch_num):\n",
    "    loss = train()  # Pass the training dataset to train function.\n",
    "    test_acc = test()  # Pass the test dataset to test function.\n",
    "    #print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Plot learning curve\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(train_loss_list, label='Train Loss')\n",
    "plt.plot(val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(train_acc_list, label='Train Accuracy')\n",
    "plt.plot(val_acc_list, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7960\n"
     ]
    }
   ],
   "source": [
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
