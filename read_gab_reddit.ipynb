{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import ast\n",
    "import time\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "# import torch_geometric as tg\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import re\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', 10)  # Limit number of rows displayed\n",
    "pd.set_option('display.width', 1000)  # Set max width for table\n",
    "pd.set_option('display.colheader_justify', 'center')  # Center-align column headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_chars(value):\n",
    "    if isinstance(value, str):  \n",
    "        return value.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').replace('  ', ' ').strip()\n",
    "    return value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading gab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id                                                text                        hate_speech_idx                      response                     \n",
      "0                                    1. 39869714\\r\\n  1. i joined gab to remind myself how retarded ...         [1]      [\"Using words that insult one group while defe...\n",
      "1  1. 39845588\\r\\n2. \\t39848775\\r\\n3. \\t\\t3991101...  1. This is what the left is really scared of. ...         [3]      ['You can disagree with someones opinion witho...\n",
      "2                   1. 37485560\\r\\n2. \\t37528625\\r\\n  1. It makes you an asshole.\\r\\n2. \\tGive it to...         [2]      ['Your argument is more rational if you leave ...\n",
      "3                   1. 39787626\\r\\n2. \\t39794481\\r\\n  1. So they manage to provide a whole lot of da...         [2]      [\"You shouldn't generalize a specific group or...\n",
      "4  1. 37957930\\r\\n2. \\t39953348\\r\\n3. \\t\\t3996521...  1. Hi there, i,m Keith, i hope you are doing w...         [3]      ['If someone is rude it is better to ignore th...\n",
      "5                                    1. 38462712\\r\\n                    1. you sound like a faggot \\r\\n         [1]      [\"Please be careful with the words you choose ...\n",
      "6  1. 38052531\\r\\n2. \\t38103723\\r\\n3. \\t\\t3851658...  1. Hi developers, give us a follow for updates...         [3]      [\"The words you've chosen are hateful and dero...\n",
      "7                   1. 38352488\\r\\n2. \\t38373190\\r\\n  1. Well, you are the fuckers that lit the matc...         [2]      ['Please refrain from using such horrible bigo...\n",
      "8  1. 37238116\\r\\n2. \\t38348543\\r\\n3. \\t\\t3837623...  1. SELF-HATING WHITE CUCKS ON PARADE\\r\\n2. \\tD...      [1, 3]      ['Your words are derogatory and offensive, and...\n",
      "9  1. 37358018\\r\\n2. \\t37359176\\r\\n3. \\t\\t3738104...  1. So after 6 years and nearly 11K followers, ...         [3]      [\"Woah! Please don't use such strong and offen...\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "Index(['id', 'text', 'hate_speech_idx', 'response'], dtype='object')\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "1. 39845588\n",
      "2. \t39848775\n",
      "3. \t\t39911017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_gab = pd.read_csv('gab_reddit_benchmark/gab.csv')\n",
    "\n",
    "content_gab[\"text\"] = content_gab[\"text\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "content_gab[\"response\"] = content_gab[\"response\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\")\n",
    "content_gab[\"hate_speech_idx\"] = content_gab[\"hate_speech_idx\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "\n",
    "# content_gab[\"text\"] = content_gab[\"text\"].apply(clean_special_chars)\n",
    "# content_gab[\"response\"] = content_gab[\"response\"].apply(clean_special_chars)\n",
    "\n",
    "for index, row in content_gab.iterrows():\n",
    "    row['text'] = row['text'].replace(\"'\", '\"')\n",
    "    row['response'] = row['response'].replace(\"'\", '\"')\n",
    "\n",
    "# content_gab = content_gab.applymap(clean_special_chars)\n",
    "print(content_gab.head(n=10))\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab.columns)\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab.iloc[1]['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_first_number(input_string):\n",
    "    match = re.search(r'\\d{2,}', input_string)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None\n",
    "\n",
    "content_gab['extracted_id'] = content_gab['id'].apply(get_first_number)\n",
    "\n",
    "# Find duplicate rows based on 'extracted_id'\n",
    "duplicates = content_gab[content_gab.duplicated(subset=['extracted_id'], keep=False)]\n",
    "filtered_groups = []\n",
    "grouped = content_gab.groupby('extracted_id')\n",
    "for key, group in grouped:\n",
    "    if len(group) > 1:\n",
    "        filtered_groups.append(group)\n",
    "\n",
    "merged_df = pd.concat(filtered_groups, ignore_index=True)\n",
    "\n",
    "merged_df.to_csv('gab_reddit_benchmark/gab_groups.csv', index=False)\n",
    "\n",
    "merged_df = grouped.agg({\n",
    "    'id': ' '.join,\n",
    "    'text': ' '.join,\n",
    "    'hate_speech_idx': ' '.join,\n",
    "    'response': ' '.join\n",
    "}).reset_index()\n",
    "merged_df.to_csv('gab_reddit_benchmark/gab_merged.csv')\n",
    "\n",
    "df = pd.read_csv('gab_reddit_benchmark/gab_merged.csv')\n",
    "df = df.applymap(lambda x: x.replace('] [', ', ') if isinstance(x, str) else x)\n",
    "df = df.applymap(lambda x: x.replace(']  [', ', ') if isinstance(x, str) else x)\n",
    "df = df.applymap(lambda x: 'n/a' if isinstance(x, str) and x.strip() == '' else x)\n",
    "#df = df.applymap(lambda x: x.replace(' ', ',n/a') if isinstance(x, str) else x)\n",
    "df.to_csv('gab_reddit_benchmark/gab_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id                                                text                        hate_speech_idx                      response                     \n",
      "0  1. 37011807\\r\\n2. \\t37012801\\r\\n3. \\t\\t3701333...  1. 64% of Hispanics have IQ scores too low to ...     [2, 2, 3]    ['Racist and hate speech will not be tolerated...\n",
      "1                    1. 37012081\\r\\n 1. 37012081\\r\\n  1. LYING CUNT BITCH !!\\r\\n 1. LYING CUNT BITCH...        [1, 1]    ['Please refrain from using derogatory terms f...\n",
      "2                                    1. 37012105\\r\\n  1. Nigger Kills 2 cops https://www.google.com/...           [1]    ['The N word is highly offensive.   All ethnic...\n",
      "3                   1. 37012146\\r\\n2. \\t37013020\\r\\n  1. STABBED IN NECK   arrested but no descr   #...           [2]    ['Use of the n-word is unacceptable in our dis...\n",
      "4  1. 37012285\\r\\n2. \\t37013809\\r\\n3. \\t37013868\\r\\n  1. Retard 1st Class  w/ Cluster & Ribbon\\r\\n2....           [1]    ['We should try and not use words making fun o...\n",
      "5                                    1. 37012571\\r\\n  1. We should be happy they do fuck their cousi...           [1]    ['I cannot listen to your argument if you keep...\n",
      "6  1. 37012882\\r\\n2. \\t37013415\\r\\n3. \\t\\t3701686...  1. Stolen Valor! Prosecute the scum.\\r\\n2. \\t\\...        [3, 4]    ['Stick to your own beliefs without dismissing...\n",
      "7                                    1. 37013030\\r\\n  1. Yea, here's the scoop.  Fake News! \"We told...                                                                   \n",
      "8                                    1. 37013358\\r\\n           1. Throat-punch that faggot DeNigger\\r\\n           [1]    ['I hope that you can see that you are advocat...\n",
      "9                                    1. 37013922\\r\\n  1. https://www.youtube.com/watch?v=DmNRkp_fuoo...           [1]    ['Gender is a category that shouldn’t be the b...\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "Index(['id', 'text', 'hate_speech_idx', 'response'], dtype='object')\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "1. 37011807\n",
      "2. \t37012801\n",
      "3. \t\t37013338\n",
      "4. \t\t37013511\n",
      "5. \t\t37333801\n",
      " 1. 37011807\n",
      "2. \t37012913\n",
      "3. \t\t37013738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_gab = pd.read_csv('gab_reddit_benchmark/gab_merged.csv')\n",
    "content_gab = content_gab.drop('Unnamed: 0', axis=1)\n",
    "content_gab = content_gab.drop('extracted_id', axis=1)\n",
    "\n",
    "\n",
    "content_gab[\"text\"] = content_gab[\"text\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "content_gab[\"response\"] = content_gab[\"response\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\")\n",
    "content_gab[\"hate_speech_idx\"] = content_gab[\"hate_speech_idx\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "\n",
    "for index, row in content_gab.iterrows():\n",
    "    row['text'] = row['text'].replace(\"'\", '\"')\n",
    "    row['response'] = row['response'].replace(\"'\", '\"')\n",
    "\n",
    "print(content_gab.head(n=10))\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab.columns)\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab.iloc[0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_text_labels(text_utterances_length, labels):\n",
    "    if not labels:\n",
    "        # return ['other'] * text_utterances_length\n",
    "        return [0] * text_utterances_length\n",
    "    new_labels = []\n",
    "    int_list = ast.literal_eval(labels)\n",
    "    for i in range(text_utterances_length):\n",
    "        if i+1 in int_list:\n",
    "            # new_labels.append('hate_speech')\n",
    "            new_labels.append(1)\n",
    "        else:\n",
    "            # new_labels.append('other')\n",
    "            new_labels.append(0)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting 'text' and 'response' into individual rows, so that I can construct a graph from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id                                                text                               text_labels                             response                        response_labels  \n",
      "0  1. 37011807\\r\\n2. \\t37012801\\r\\n3. \\t\\t3701333...  [1. 64% of Hispanics have IQ scores too low to...  [0, 1, 1, 0, 0, 0, 0, 0]  [Racist and hate speech will not be tolerated ...     [0, 0, 0, 0, 0]\n",
      "1                    1. 37012081\\r\\n 1. 37012081\\r\\n   [1. LYING CUNT BITCH !!, 1. LYING CUNT BITCH !!]                    [1, 0]  [Please refrain from using derogatory terms fo...  [0, 0, 0, 0, 0, 0]\n",
      "2                                    1. 37012105\\r\\n  [1. Nigger Kills 2 cops https://www.google.com...                       [1]  [The N word is highly offensive.  All ethnicit...           [0, 0, 0]\n",
      "3                   1. 37012146\\r\\n2. \\t37013020\\r\\n  [1. STABBED IN NECK  arrested but no descr  #D...                    [0, 1]  [Use of the n-word is unacceptable in our disc...              [0, 0]\n",
      "4  1. 37012285\\r\\n2. \\t37013809\\r\\n3. \\t37013868\\r\\n  [1. Retard 1st Class  w/ Cluster & Ribbon, 2. ...                 [1, 0, 0]  [We should try and not use words making fun of...           [0, 0, 0]\n",
      "- - - - \n",
      "Index(['id', 'text', 'text_labels', 'response', 'response_labels'], dtype='object')\n",
      "1. 37011807\n",
      "2. \t37012801\n",
      "3. \t\t37013338\n",
      "4. \t\t37013511\n",
      "5. \t\t37333801\n",
      " 1. 37011807\n",
      "2. \t37012913\n",
      "3. \t\t37013738\n",
      "\n",
      "['1. 64% of Hispanics have IQ scores too low to enter the military. 👈  Source: https://archive.today/o/LRe05/takimag.com/article/frequently_asked_questions_about_the_jason_richwine_brouhaha_steve_sailer/print%23ixzz2TPXmpNgG', \"2. The one white pill about the US becoming brown is that its core racial stock will be too retarded to run a competent military. A Brazil-of-the-North won't be able to police the world.  However, this isn't an endorsement of White Genocide in America. Besides, in this scenario there may be more opportunity for a white ethnostate to break away.\", \"3.  Well it's like any large corporation. Upper management hopes the have the smart people come up with well documented processes that removes the need for smart people so you can hire cheap dumb drones to just follow process. This never works very well in reality, we'll have the Wall Mart of militaries. I'm sure someone will get rich off the whole thing though\", \"4.  A future US military will likely try to adopt a WWII Soviet-like military doctrine of throwing masses of expendable troops at the enemy. The only problem with this is that nonwhites aren't going to be willing to die for ZOG. I foresee major morale problems.\", '5.  there is nothing good about the genocide of white america. This will end with violence. That is one way to sum it up, briefly', '1. 64% of Hispanics have IQ scores too low to enter the military. 👈  Source: https://archive.today/o/LRe05/takimag.com/article/frequently_asked_questions_about_the_jason_richwine_brouhaha_steve_sailer/print%23ixzz2TPXmpNgG', '2. Spic DNA is inferior in every single way you can measure it ALL PARAMETERS, THEY VERY INFERIOR TO THE ARYAN', '3.  Sadly the very best DNA was built across thousands of years of natural selection and survival of the fittest. In a world where every spic maggot lives and has 10 kids, the very best DNA is recessive and fades into history. One of the fundamental rules of Thermodynamics is that all energy systems must fall to there lowest energy state eventually.']\n",
      "[0, 1, 1, 0, 0, 0, 0, 0]\n",
      "['Racist and hate speech will not be tolerated on this platform.', 'This word mocks disabled people and is inappropriate.', 'It invalidates your argument when you use those types of slurs.', 'Pure bred is inbred. Everyone except native americans are an immigrant of some kind. Read your history.', 'I wonder what percentage of all humans would fall into this category. It may not be racial, at all, just a headline to catch attention.']\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "text_column = []\n",
    "text_labels_column = []\n",
    "response_column = []\n",
    "response_labels_column = []\n",
    "\n",
    "for index, row in content_gab.iterrows():\n",
    "    text_utterances = row['text'].split('\\n')\n",
    "    text_utterances = list(filter(None, text_utterances))\n",
    "\n",
    "    for i, t in enumerate(text_utterances):\n",
    "        text_utterances[i] = clean_special_chars(t)\n",
    "    text_labels = mark_text_labels(len(text_utterances), row['hate_speech_idx'])\n",
    "\n",
    "    response_utterances = ast.literal_eval(row['response']) if row['response'] else []\n",
    "    for i, r in enumerate(response_utterances):\n",
    "        response_utterances[i] = clean_special_chars(r)\n",
    "    # response_labels = ['other'] * len(response_utterances)  \n",
    "    response_labels = [0] * len(response_utterances)  \n",
    "\n",
    "    \n",
    "    text_column.append(text_utterances)\n",
    "    text_labels_column.append(text_labels)\n",
    "    response_column.append(response_utterances)\n",
    "    response_labels_column.append(response_labels)\n",
    "\n",
    "content_gab['text'] = text_column\n",
    "content_gab['hate_speech_idx'] = text_labels_column\n",
    "content_gab['response'] = response_column\n",
    "content_gab['response_labels'] = response_labels_column\n",
    "\n",
    "content_gab = content_gab.rename(columns={'hate_speech_idx': 'text_labels'})\n",
    "print(content_gab.head())\n",
    "print('- - - - ')\n",
    "print(content_gab.columns)\n",
    "\n",
    "for index, row in content_gab.iterrows():\n",
    "    if index == 1:\n",
    "        continue\n",
    "    print(row['id'])\n",
    "    print(row['text'])\n",
    "    print(row['text_labels'])\n",
    "    print(row['response'])\n",
    "    print(row['response_labels'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_encoder = LabelEncoder()\n",
    "# content_gab['all_labels'] = content_gab['text_labels'] + content_gab['response_labels']\n",
    "# content_gab['all_labels_encoded'] = content_gab['all_labels'].apply(label_encoder.fit_transform)\n",
    "# print(content_gab.iloc[0])\n",
    "# content_gab['text_labels_encoded'] = content_gab['text_labels'].apply(label_encoder.fit_transform)\n",
    "# content_gab['response_labels_encoded'] = content_gab['response_labels'].apply(label_encoder.fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating BERT encoding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def generate_embeddings(sentences):\n",
    "    if isinstance(sentences, list):\n",
    "        return bert.encode(sentences, show_progress_bar=True).tolist()\n",
    "    elif isinstance(sentences, str):\n",
    "        return bert.encode([sentences], show_progress_bar=True).tolist()\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 72.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 52.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 105.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 326.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 94.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 62.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 277.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 68.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 176.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 54.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 76.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 54.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 67.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 52.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 62.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 64.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 110.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 62.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 53.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 68.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 64.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 94.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 72.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 51.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 67.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 58.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 77.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 111.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 80.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 94.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 51.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 53.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 62.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 66.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 66.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 332.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 71.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 493.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 117.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 52.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 62.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 53.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 67.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 53.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.040111515671014786, -0.019980860874056816, 0.013976861722767353, 0.021806733682751656, -0.013460909947752953, 0.029162606224417686, 0.12978620827198029, -0.03199726715683937, 0.013665660284459591, 0.009298085235059261, 0.027889572083950043, -0.12927503883838654, 0.057423628866672516, -0.03645731136202812, -0.08924495428800583, 0.003100527450442314, -0.08778698742389679, -0.01677405834197998, 0.004921083338558674, 0.01669718511402607, 0.058399613946676254, 0.0006712392787449062, 0.05522868037223816, 0.06297323107719421, -0.0073430766351521015, -0.045072540640830994, 0.01580304279923439, -0.00940280593931675, -0.035581137984991074, -0.043321527540683746, -0.0003930269740521908, 0.08144824951887131, -0.0247819721698761, 0.026637855917215347, 0.01791100762784481, -0.07403568178415298, 0.043707575649023056, -0.005354813765734434, -0.03749185428023338, 0.019640570506453514, 0.0009533570264466107, -0.08591044694185257, 0.0563356839120388, 0.015451686456799507, 0.012978585436940193, 0.013843889348208904, 0.01790759526193142, 0.09729532897472382, 0.0011678653536364436, -0.07746380567550659, 0.007192096207290888, 0.06707136332988739, -0.0475802943110466, 0.014114831574261189, -0.023364581167697906, -0.14599590003490448, -0.08075781166553497, 0.05122145637869835, -0.029158052057027817, 0.06154780089855194, 0.05934493988752365, -0.011493835598230362, 0.013919748365879059, 0.04411112517118454, 0.05024072527885437, -0.0118561377748847, -0.06531890481710434, -0.10701733827590942, -0.03381326049566269, 0.16829092800617218, -0.008968253619968891, 0.01771368272602558, -0.08077249675989151, -0.054106809198856354, -0.04426594451069832, -0.09787891060113907, 0.02920658513903618, -0.11190901696681976, 0.052306562662124634, -0.0013403763296082616, -0.04718188941478729, -0.009266284294426441, -0.010947458446025848, 0.01871548965573311, -0.023012982681393623, -0.08965280652046204, -0.05449749156832695, 0.00585654191672802, -0.02628675475716591, 0.019181953743100166, 0.09221342951059341, -0.05334479734301567, 0.0772247463464737, 0.057111188769340515, -0.05297648906707764, -0.020232656970620155, -0.05923709273338318, -0.029222426936030388, -0.1063842847943306, 0.0338551290333271, -0.031237425282597542, 0.11719315499067307, 0.03926265239715576, 0.038881052285432816, 0.02000211738049984, 0.04364709183573723, 0.08000590652227402, 0.04351244121789932, -0.05459458380937576, 0.022630615159869194, -0.00018089958757627755, -0.05763659626245499, 0.0016649332828819752, -0.012210852466523647, 0.043758660554885864, 0.04527454450726509, 0.07739290595054626, 0.037310607731342316, 0.0250901747494936, -0.03177649527788162, 0.02730264700949192, -0.01907612942159176, 0.009285188280045986, 0.06330647319555283, -0.0531730130314827, -0.15368549525737762, -0.02131630666553974, 5.278872378119304e-34, 0.013963745906949043, 0.019503846764564514, -0.016416912898421288, 0.06060383841395378, 0.06486869603395462, 0.06641621142625809, -0.06501848995685577, 0.01924503594636917, -0.01269327849149704, 0.04242672026157379, -0.030849140137434006, 0.04991858825087547, -0.030706973746418953, -0.00645487941801548, 0.03992359712719917, 0.02742866426706314, -0.011044580489397049, 0.07652553170919418, -0.004601816646754742, -0.03957526013255119, 0.02769160270690918, 0.10170536488294601, 0.003163727466017008, 0.032961226999759674, -0.04595436528325081, -0.02313772216439247, 0.0668853372335434, -0.05477537959814072, -0.06843407452106476, -0.0076514799147844315, -0.031681664288043976, -0.03629857674241066, 0.07056544721126556, 0.0694119930267334, -0.01646053045988083, -0.01547061838209629, 0.0419100783765316, -0.05381028726696968, -0.025412701070308685, -0.027585135772824287, -0.08673509210348129, 0.0026466078124940395, 0.03456755355000496, -0.007929044775664806, 0.045280423015356064, -0.027348162606358528, -0.051258448511362076, 0.039591263979673386, 0.05489296093583107, 0.015311039052903652, 0.002636158838868141, 0.08284692466259003, 0.028859086334705353, 0.009814593940973282, 0.06011432409286499, -0.015664057806134224, 0.07270550727844238, -0.0168948732316494, 0.012817459180951118, 0.04541559889912605, 0.0641687735915184, -0.07141944020986557, -0.03529824689030647, -0.1285087764263153, -0.05714914947748184, -0.023340335115790367, -0.04458729177713394, -0.04705711081624031, 0.007458718493580818, 0.07790287584066391, -0.023804180324077606, 0.044960375875234604, -0.0463760644197464, -0.07306154817342758, -0.010300067253410816, 0.05017390474677086, 0.016431856900453568, 0.04650842770934105, -0.04543733596801758, -0.06840024888515472, 0.006202551070600748, 0.05130220949649811, -0.06910581886768341, 0.03827262669801712, -0.033113956451416016, 0.03819851577281952, -0.0678815245628357, -0.08348270505666733, 0.027531608939170837, -0.010729944333434105, -0.05853388458490372, 0.03953007236123085, 0.07402396202087402, -0.025220036506652832, 0.015599816106259823, -1.2063556693061805e-33, -0.06020955368876457, 0.07276656478643417, 0.012939992360770702, 0.0911172553896904, 0.04825712367892265, -0.0037323436699807644, 0.02870313636958599, -0.01588836871087551, 0.09410853683948517, -0.012761905789375305, -0.0146919721737504, -0.08215449005365372, 0.0026014638133347034, -0.018885117024183273, 0.006506090983748436, -0.015884552150964737, 0.09633733332157135, 0.0227802786976099, -0.06213445961475372, 0.0589606873691082, -0.03560444340109825, 0.048318035900592804, -0.042313963174819946, 0.08661717176437378, -0.07806174457073212, 0.05661145597696304, 0.06675618886947632, -0.02596925012767315, 0.021704629063606262, 0.017314886674284935, 0.04436352849006653, 0.010987799614667892, -0.021645670756697655, 0.045240357518196106, -0.0021713552996516228, -0.03815814107656479, 0.05528810992836952, 0.008784563280642033, -0.024336135014891624, 0.03867531567811966, 0.13091903924942017, 0.016075361520051956, 0.002800402930006385, 0.010984085500240326, 0.0021802037954330444, -0.05402928218245506, -0.04300377890467644, -0.007997690699994564, -0.01719985529780388, 0.016259746626019478, -0.08929973840713501, 0.00498489523306489, -0.03799218684434891, 0.03269173204898834, 0.006741472985595465, -0.02412344701588154, -0.01199487503618002, 0.001355231972411275, 0.06979990005493164, 0.012145213782787323, -0.017063017934560776, 0.051977407187223434, -0.026887746527791023, 0.021980123594403267, 0.039181292057037354, -0.08760755509138107, -0.03241024538874626, 0.1006213054060936, -0.014725979417562485, -0.017326610162854195, 0.038118813186883926, 0.02697169966995716, 0.0010811341926455498, -0.08250278234481812, -0.06707588583230972, 0.04794669523835182, -0.0675421953201294, -0.08735065907239914, 0.006340670865029097, 0.045336224138736725, -0.11353377252817154, -0.02914619632065296, 0.027313239872455597, -0.009244934655725956, -0.020498964935541153, -0.019779687747359276, 0.17267866432666779, 0.0036988118663430214, 0.03492206707596779, 0.02720397152006626, 0.0019705232698470354, 0.023376960307359695, -0.006646905094385147, 0.007091756910085678, 0.0058246576227247715, -1.9545796092756973e-08, -0.05307181179523468, 0.010424039326608181, -0.004733220674097538, -0.11555369943380356, 0.05628947541117668, 0.06417179107666016, -0.07894627749919891, 0.019406750798225403, -0.0064017390832304955, 0.010946490801870823, 0.051129285246133804, 0.015289317816495895, -0.022321874275803566, -0.009139628149569035, -0.02239227294921875, 0.09768322110176086, -0.03265824168920517, -0.027335653081536293, 0.018783407285809517, -0.021211376413702965, 0.05816853418946266, 0.025574058294296265, -0.07158363610506058, -0.030405193567276, -0.09423930197954178, 0.04677167534828186, -0.033257775008678436, 0.09370671957731247, -0.04853816330432892, 0.10074584186077118, 0.051425088196992874, 0.03650866076350212, 0.04013805091381073, -0.030323654413223267, -0.013772699050605297, -0.02908283658325672, 0.07708760350942612, 0.01640721783041954, 0.008350793272256851, 0.04463913291692734, 0.00024834374198690057, -0.011509903706610203, 0.023078229278326035, -0.03585660457611084, -0.018256546929478645, -0.021135104820132256, -0.031101349741220474, -0.04423367232084274, -0.0341482013463974, -0.09501758962869644, -0.06261333078145981, 0.025977982208132744, -0.03540654480457306, 0.08803164213895798, 0.11138269305229187, -0.02321530692279339, -0.00886520091444254, -0.031119024381041527, -0.02749992161989212, 0.006101682782173157, 0.044569239020347595, 0.015275535173714161, -0.021151360124349594, -0.06012168899178505], [-0.040111515671014786, -0.019980860874056816, 0.013976861722767353, 0.021806733682751656, -0.013460909947752953, 0.029162606224417686, 0.12978620827198029, -0.03199726715683937, 0.013665660284459591, 0.009298085235059261, 0.027889572083950043, -0.12927503883838654, 0.057423628866672516, -0.03645731136202812, -0.08924495428800583, 0.003100527450442314, -0.08778698742389679, -0.01677405834197998, 0.004921083338558674, 0.01669718511402607, 0.058399613946676254, 0.0006712392787449062, 0.05522868037223816, 0.06297323107719421, -0.0073430766351521015, -0.045072540640830994, 0.01580304279923439, -0.00940280593931675, -0.035581137984991074, -0.043321527540683746, -0.0003930269740521908, 0.08144824951887131, -0.0247819721698761, 0.026637855917215347, 0.01791100762784481, -0.07403568178415298, 0.043707575649023056, -0.005354813765734434, -0.03749185428023338, 0.019640570506453514, 0.0009533570264466107, -0.08591044694185257, 0.0563356839120388, 0.015451686456799507, 0.012978585436940193, 0.013843889348208904, 0.01790759526193142, 0.09729532897472382, 0.0011678653536364436, -0.07746380567550659, 0.007192096207290888, 0.06707136332988739, -0.0475802943110466, 0.014114831574261189, -0.023364581167697906, -0.14599590003490448, -0.08075781166553497, 0.05122145637869835, -0.029158052057027817, 0.06154780089855194, 0.05934493988752365, -0.011493835598230362, 0.013919748365879059, 0.04411112517118454, 0.05024072527885437, -0.0118561377748847, -0.06531890481710434, -0.10701733827590942, -0.03381326049566269, 0.16829092800617218, -0.008968253619968891, 0.01771368272602558, -0.08077249675989151, -0.054106809198856354, -0.04426594451069832, -0.09787891060113907, 0.02920658513903618, -0.11190901696681976, 0.052306562662124634, -0.0013403763296082616, -0.04718188941478729, -0.009266284294426441, -0.010947458446025848, 0.01871548965573311, -0.023012982681393623, -0.08965280652046204, -0.05449749156832695, 0.00585654191672802, -0.02628675475716591, 0.019181953743100166, 0.09221342951059341, -0.05334479734301567, 0.0772247463464737, 0.057111188769340515, -0.05297648906707764, -0.020232656970620155, -0.05923709273338318, -0.029222426936030388, -0.1063842847943306, 0.0338551290333271, -0.031237425282597542, 0.11719315499067307, 0.03926265239715576, 0.038881052285432816, 0.02000211738049984, 0.04364709183573723, 0.08000590652227402, 0.04351244121789932, -0.05459458380937576, 0.022630615159869194, -0.00018089958757627755, -0.05763659626245499, 0.0016649332828819752, -0.012210852466523647, 0.043758660554885864, 0.04527454450726509, 0.07739290595054626, 0.037310607731342316, 0.0250901747494936, -0.03177649527788162, 0.02730264700949192, -0.01907612942159176, 0.009285188280045986, 0.06330647319555283, -0.0531730130314827, -0.15368549525737762, -0.02131630666553974, 5.278872378119304e-34, 0.013963745906949043, 0.019503846764564514, -0.016416912898421288, 0.06060383841395378, 0.06486869603395462, 0.06641621142625809, -0.06501848995685577, 0.01924503594636917, -0.01269327849149704, 0.04242672026157379, -0.030849140137434006, 0.04991858825087547, -0.030706973746418953, -0.00645487941801548, 0.03992359712719917, 0.02742866426706314, -0.011044580489397049, 0.07652553170919418, -0.004601816646754742, -0.03957526013255119, 0.02769160270690918, 0.10170536488294601, 0.003163727466017008, 0.032961226999759674, -0.04595436528325081, -0.02313772216439247, 0.0668853372335434, -0.05477537959814072, -0.06843407452106476, -0.0076514799147844315, -0.031681664288043976, -0.03629857674241066, 0.07056544721126556, 0.0694119930267334, -0.01646053045988083, -0.01547061838209629, 0.0419100783765316, -0.05381028726696968, -0.025412701070308685, -0.027585135772824287, -0.08673509210348129, 0.0026466078124940395, 0.03456755355000496, -0.007929044775664806, 0.045280423015356064, -0.027348162606358528, -0.051258448511362076, 0.039591263979673386, 0.05489296093583107, 0.015311039052903652, 0.002636158838868141, 0.08284692466259003, 0.028859086334705353, 0.009814593940973282, 0.06011432409286499, -0.015664057806134224, 0.07270550727844238, -0.0168948732316494, 0.012817459180951118, 0.04541559889912605, 0.0641687735915184, -0.07141944020986557, -0.03529824689030647, -0.1285087764263153, -0.05714914947748184, -0.023340335115790367, -0.04458729177713394, -0.04705711081624031, 0.007458718493580818, 0.07790287584066391, -0.023804180324077606, 0.044960375875234604, -0.0463760644197464, -0.07306154817342758, -0.010300067253410816, 0.05017390474677086, 0.016431856900453568, 0.04650842770934105, -0.04543733596801758, -0.06840024888515472, 0.006202551070600748, 0.05130220949649811, -0.06910581886768341, 0.03827262669801712, -0.033113956451416016, 0.03819851577281952, -0.0678815245628357, -0.08348270505666733, 0.027531608939170837, -0.010729944333434105, -0.05853388458490372, 0.03953007236123085, 0.07402396202087402, -0.025220036506652832, 0.015599816106259823, -1.2063556693061805e-33, -0.06020955368876457, 0.07276656478643417, 0.012939992360770702, 0.0911172553896904, 0.04825712367892265, -0.0037323436699807644, 0.02870313636958599, -0.01588836871087551, 0.09410853683948517, -0.012761905789375305, -0.0146919721737504, -0.08215449005365372, 0.0026014638133347034, -0.018885117024183273, 0.006506090983748436, -0.015884552150964737, 0.09633733332157135, 0.0227802786976099, -0.06213445961475372, 0.0589606873691082, -0.03560444340109825, 0.048318035900592804, -0.042313963174819946, 0.08661717176437378, -0.07806174457073212, 0.05661145597696304, 0.06675618886947632, -0.02596925012767315, 0.021704629063606262, 0.017314886674284935, 0.04436352849006653, 0.010987799614667892, -0.021645670756697655, 0.045240357518196106, -0.0021713552996516228, -0.03815814107656479, 0.05528810992836952, 0.008784563280642033, -0.024336135014891624, 0.03867531567811966, 0.13091903924942017, 0.016075361520051956, 0.002800402930006385, 0.010984085500240326, 0.0021802037954330444, -0.05402928218245506, -0.04300377890467644, -0.007997690699994564, -0.01719985529780388, 0.016259746626019478, -0.08929973840713501, 0.00498489523306489, -0.03799218684434891, 0.03269173204898834, 0.006741472985595465, -0.02412344701588154, -0.01199487503618002, 0.001355231972411275, 0.06979990005493164, 0.012145213782787323, -0.017063017934560776, 0.051977407187223434, -0.026887746527791023, 0.021980123594403267, 0.039181292057037354, -0.08760755509138107, -0.03241024538874626, 0.1006213054060936, -0.014725979417562485, -0.017326610162854195, 0.038118813186883926, 0.02697169966995716, 0.0010811341926455498, -0.08250278234481812, -0.06707588583230972, 0.04794669523835182, -0.0675421953201294, -0.08735065907239914, 0.006340670865029097, 0.045336224138736725, -0.11353377252817154, -0.02914619632065296, 0.027313239872455597, -0.009244934655725956, -0.020498964935541153, -0.019779687747359276, 0.17267866432666779, 0.0036988118663430214, 0.03492206707596779, 0.02720397152006626, 0.0019705232698470354, 0.023376960307359695, -0.006646905094385147, 0.007091756910085678, 0.0058246576227247715, -1.9545796092756973e-08, -0.05307181179523468, 0.010424039326608181, -0.004733220674097538, -0.11555369943380356, 0.05628947541117668, 0.06417179107666016, -0.07894627749919891, 0.019406750798225403, -0.0064017390832304955, 0.010946490801870823, 0.051129285246133804, 0.015289317816495895, -0.022321874275803566, -0.009139628149569035, -0.02239227294921875, 0.09768322110176086, -0.03265824168920517, -0.027335653081536293, 0.018783407285809517, -0.021211376413702965, 0.05816853418946266, 0.025574058294296265, -0.07158363610506058, -0.030405193567276, -0.09423930197954178, 0.04677167534828186, -0.033257775008678436, 0.09370671957731247, -0.04853816330432892, 0.10074584186077118, 0.051425088196992874, 0.03650866076350212, 0.04013805091381073, -0.030323654413223267, -0.013772699050605297, -0.02908283658325672, 0.07708760350942612, 0.01640721783041954, 0.008350793272256851, 0.04463913291692734, 0.00024834374198690057, -0.011509903706610203, 0.023078229278326035, -0.03585660457611084, -0.018256546929478645, -0.021135104820132256, -0.031101349741220474, -0.04423367232084274, -0.0341482013463974, -0.09501758962869644, -0.06261333078145981, 0.025977982208132744, -0.03540654480457306, 0.08803164213895798, 0.11138269305229187, -0.02321530692279339, -0.00886520091444254, -0.031119024381041527, -0.02749992161989212, 0.006101682782173157, 0.044569239020347595, 0.015275535173714161, -0.021151360124349594, -0.06012168899178505]]\n",
      "\n",
      "TIME FOR TEXT EMBEDDINGS:  12.000907182693481\n",
      "\n",
      "- - - - - -\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.30it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 54.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 68.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.33it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.81it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 58.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 70.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 51.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.55it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.13it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 61.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.93it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 51.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 70.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 61.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 61.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 62.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.37it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.46it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 70.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.77it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.67it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 53.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.90it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 52.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 51.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.38it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.95it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 66.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.60it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.42it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.39it/s]\n",
      "Batches: 0it [00:00, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 61.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07439517229795456, 0.04188426211476326, -0.09229229390621185, -0.031470734626054764, -0.052862267941236496, -0.020004451274871826, 0.03124356083571911, -0.08033803105354309, -0.010242228396236897, -0.038911741226911545, 0.048023175448179245, -0.05908317118883133, 0.03346498683094978, 0.0034259618259966373, -0.054501939564943314, 0.05408971756696701, 0.013149838894605637, -0.029767796397209167, -0.045929502695798874, -0.0669841393828392, -0.07561828196048737, 0.09951933473348618, 0.14216117560863495, 0.03618994355201721, -0.07632524520158768, -0.05548761039972305, 0.042400769889354706, 0.006625581532716751, 0.03837255761027336, 0.021637508645653725, -0.01584904082119465, 0.017154689878225327, 0.06257234513759613, 0.06082208827137947, -0.07147544622421265, -0.05742684006690979, 0.06866130977869034, 0.0014725130749866366, 0.006498364731669426, -0.04267704859375954, -0.047141071408987045, 0.010848326608538628, -0.016492504626512527, -0.03367657959461212, 0.08141005039215088, 0.053380969911813736, -0.033085208386182785, 0.004110030364245176, -0.026784062385559082, -0.033062685281038284, 0.050176095217466354, -0.024493156000971794, 0.05656621605157852, 0.08710337430238724, -0.04670901224017143, -0.015871236100792885, -0.02822638675570488, 0.009671464562416077, -0.016277385875582695, 0.03113292157649994, 0.03586331382393837, -0.05365530773997307, 0.03586074709892273, -0.018377499654889107, 0.051139913499355316, -0.04359624534845352, 0.018894905224442482, 0.029919534921646118, -0.06049139425158501, 0.024485142901539803, 0.03180629387497902, 0.07686072587966919, 0.08233692497015, 0.03590484708547592, -0.04389408603310585, -0.020272046327590942, 0.038296084851026535, 0.00014711907715536654, 0.030800867825746536, -0.003748226910829544, 0.006293682847172022, 0.1088017001748085, 0.08356574922800064, -0.05613469332456589, 0.04434796795248985, -0.025904685258865356, -0.02734692208468914, 0.030949894338846207, -0.009581279940903187, 0.0403909906744957, -0.07672879099845886, 0.0742403045296669, 0.1789822280406952, -0.05854349955916405, 0.012617920525372028, -0.1070038303732872, -0.018638290464878082, 0.03260781615972519, -0.05237746611237526, 0.028777774423360825, -0.03207947686314583, 0.029079431667923927, -0.07265201210975647, -0.016452815383672714, -0.033225931227207184, -0.049539025872945786, -0.020033342763781548, -0.05262773856520653, -0.07672698050737381, -0.03317368030548096, -0.10545326769351959, -0.000738357484806329, -0.04369155317544937, 0.009846869856119156, 0.021226584911346436, -0.015836073085665703, 0.015209774486720562, -0.018840190023183823, 0.071321502327919, 0.0011545663001015782, -0.08845636248588562, 0.010314462706446648, -0.11260634660720825, 0.09407645463943481, 0.0022133002057671547, -0.027135100215673447, -0.010361319407820702, -1.0108298164621553e-33, 0.006425671279430389, 0.0633036196231842, -0.02495528571307659, -0.04610670357942581, 0.016715243458747864, -0.07102710008621216, -0.03824058920145035, -0.008793476969003677, -0.017122972756624222, -0.03681240603327751, 0.05346335098147392, 0.0027842961717396975, 0.018753167241811752, 0.05254324525594711, 0.043434035032987595, 0.04245004430413246, 0.0004509901627898216, 0.07043227553367615, -0.019360890612006187, -0.041000161319971085, 0.10037098824977875, 0.06492152810096741, -0.09005215018987656, 0.036222126334905624, -0.12026318907737732, -0.0772862359881401, 0.021110890433192253, 0.04867557808756828, 0.011864599771797657, -0.008536637760698795, -0.037403371185064316, -0.030890636146068573, 0.007255591452121735, 0.006097785662859678, 0.04653504118323326, -0.0703529492020607, 0.060190439224243164, 0.0741262286901474, -0.026204334571957588, -0.03949085623025894, 0.010072926059365273, 0.08705651015043259, 0.004875565879046917, 0.022800669074058533, 0.10361339151859283, 0.05204955115914345, -0.0075820256024599075, -0.07028576731681824, -0.008707815781235695, 0.06758180260658264, 0.04375225305557251, -0.03105391561985016, 0.04943268746137619, 0.07763952761888504, -0.021529296413064003, -0.004785994067788124, -0.03787214681506157, 0.09111668914556503, -0.007230667397379875, -0.054580871015787125, -0.022354083135724068, -0.03756427392363548, 0.0471220426261425, -0.03657401725649834, 0.06841103732585907, -0.006533159874379635, -0.02348695695400238, 0.09719198942184448, -0.028360750526189804, -0.029283149167895317, 0.07429251074790955, 0.003943238407373428, -0.03102937340736389, 0.08680849522352219, -0.04037850722670555, 0.0006089853122830391, 0.05114004760980606, -0.034088052809238434, 0.09174999594688416, -0.03351818025112152, -0.010697576217353344, 0.04840092360973358, -0.04873768240213394, -0.07698981463909149, -0.10675092786550522, -0.028191756457090378, 0.0320933498442173, -0.016807816922664642, 0.10381986200809479, -0.04173029586672783, -0.010273199528455734, 0.040919747203588486, 0.04353337734937668, 0.015330497175455093, -0.1620090752840042, 1.468069200941193e-33, -0.007648384664207697, -0.02641652710735798, -0.04914168640971184, 0.07848764210939407, -0.031351905316114426, 0.017200978472828865, 0.01611904986202717, 0.05132991448044777, 0.05249735713005066, 0.062226954847574234, 0.02224966324865818, -0.08824194222688675, 0.05080326646566391, 0.049063790589571, 0.08012791723012924, -0.07281723618507385, 0.05432187393307686, 0.15687435865402222, 0.030615633353590965, 0.03486071527004242, 0.04241473227739334, 0.006669576279819012, -0.08299964666366577, 0.013216626830399036, 0.003462171880528331, 0.05241582170128822, -0.0007283706800080836, -0.0013673059875145555, 0.00639972323551774, -0.011624053120613098, 0.01922415755689144, 0.03257377818226814, -0.039689477533102036, 0.007560453377664089, 0.07459099590778351, -0.057624418288469315, 0.052982792258262634, 0.057829104363918304, -0.0015331691829487681, -0.027938786894083023, -0.01760435663163662, 0.07309404015541077, -0.09089033305644989, 0.08231722563505173, 0.016288386657834053, 0.047665297985076904, -0.019829997792840004, -0.055410221219062805, -0.05666714534163475, -0.014981292188167572, -0.01764562353491783, -0.0017475486965849996, 0.018694300204515457, 0.007087919395416975, 0.0802711546421051, -0.02047879435122013, 0.025895820930600166, -0.05618567392230034, -0.08594831079244614, -0.01724625937640667, 0.014249906875193119, -0.017771771177649498, -0.059376753866672516, 0.0408301018178463, 0.018991267308592796, -0.03519204258918762, -0.07750684022903442, 0.012133929878473282, 0.06473132222890854, 0.020538903772830963, 0.02029728703200817, 0.0426352322101593, -0.036703694611787796, -0.04617413133382797, -0.06205795705318451, -0.08055044710636139, 0.07079177349805832, -0.01235036738216877, 0.009693811647593975, 0.021990502253174782, 0.017483022063970566, -0.06975125521421432, -0.018216300755739212, 0.007790304254740477, 0.011550516821444035, -0.022828085348010063, 0.02428378537297249, 0.013941359706223011, 0.0359758697450161, 0.0320754200220108, 0.032121144235134125, -0.1103883683681488, -0.06766674667596817, -0.0002715228183660656, 0.0005020666285417974, -2.185561776002487e-08, -0.012483572587370872, -0.03508196398615837, -0.03914012387394905, 0.037246353924274445, 0.04003312811255455, 0.04497312009334564, -0.0227237269282341, -0.1066153347492218, -0.027202919125556946, 0.01771475188434124, -0.017461908981204033, -0.00019373420218471438, -0.08074506372213364, 0.04050460830330849, -0.07581333816051483, 0.018564244732260704, 0.12310442328453064, -0.07309773564338684, 0.00488853408023715, 0.046191636472940445, -0.04567277804017067, 0.030903352424502373, 0.018863415345549583, -0.0462908111512661, 0.009401461109519005, 0.004207309801131487, -0.066860131919384, -0.04633047431707382, 0.05888558179140091, -0.049111541360616684, 0.033966802060604095, -0.014694086275994778, -0.06109841912984848, -0.003255422692745924, -0.023160411044955254, 0.03066270239651203, -0.07810329645872116, 0.042485494166612625, 0.09877948462963104, 0.05035511031746864, 0.01243493240326643, -0.00976369995623827, 0.06870091706514359, -0.0299884844571352, 0.005302450153976679, -0.02146567590534687, -0.07924561202526093, 0.01057079154998064, -0.0966225415468216, -0.014881155453622341, 0.03292703256011009, -0.01745549403131008, -0.01822563260793686, 0.036429546773433685, -0.015367597341537476, -0.033413395285606384, 0.00286050233989954, 0.07581701874732971, 0.029092764481902122, 0.03024645522236824, 0.09714531153440475, -0.027787379920482635, 0.035921141505241394, -0.02358429692685604], [0.09606626629829407, 0.05096124857664108, -0.016991177573800087, 0.048853661864995956, -0.0001912148727569729, -0.02591744437813759, -0.05123588815331459, -0.09465376287698746, -0.026228521019220352, 0.04913102462887764, -0.013334313407540321, -0.0624752976000309, 0.001251943875104189, 0.03243833780288696, -0.033594053238630295, -0.026569349691271782, 0.031146863475441933, -0.034716539084911346, -0.0536702498793602, 0.013060086406767368, -0.05227266997098923, -0.0511578805744648, 0.06516531109809875, 0.03542781621217728, 0.012991336174309254, -0.1392158418893814, 0.06048164516687393, 0.03535613790154457, -0.049693673849105835, 0.0034236989449709654, -0.07441728562116623, -0.03708481416106224, 0.03363460302352905, -0.014794816263020039, 0.014759467914700508, -0.03387105092406273, 0.003780316561460495, 0.05160091817378998, -0.049797121435403824, -0.0012085245689377189, 0.07241345942020416, 0.026401277631521225, 0.03011421486735344, 0.010937973856925964, 0.12577638030052185, -0.052716273814439774, 4.047360926051624e-05, 0.0020028315484523773, -0.10055738687515259, -0.02898009493947029, 0.020725451409816742, 0.013023002073168755, 0.10618198662996292, -0.016071084886789322, -0.015372770838439465, -0.016762245446443558, -0.06075069308280945, 0.007291082292795181, 0.05579402670264244, -0.02359623648226261, -0.0008217141148634255, -0.021011801436543465, -0.057366397231817245, 0.030485302209854126, -0.038579002022743225, -0.023456847295165062, -0.006992406211793423, 0.04488460347056389, 0.016926169395446777, -0.06466805934906006, 0.04482022300362587, 0.06872032582759857, -0.053135573863983154, 0.06838317215442657, 0.008988074958324432, -0.007480782922357321, 0.10034314543008804, -0.024748975411057472, -0.03792296722531319, -0.059232451021671295, -0.03905769810080528, 0.01055829506367445, 0.05168762803077698, -0.024669146165251732, 0.03391542285680771, -0.11521567404270172, -0.007668266538530588, 0.018620049580931664, -0.059380654245615005, 0.026050329208374023, 0.002396967262029648, 0.037883054465055466, 0.018474163487553596, -0.07753800600767136, 0.023765742778778076, -0.043098852038383484, -0.008399097248911858, 0.06556452810764313, -0.005919792223721743, 0.0016497793840244412, -0.004003072157502174, 0.0038904554676264524, -0.03420165926218033, 0.07000888139009476, 0.025983717292547226, -0.06759589910507202, -0.06708347797393799, -0.0075883022509515285, -0.07144185900688171, -0.05414257571101189, -0.024520933628082275, 0.05784563347697258, -0.008315129205584526, 0.07261469960212708, 0.05335180461406708, -0.07138329744338989, 0.03783117234706879, 0.014086778275668621, -0.02519841119647026, 0.07228142768144608, -0.06297479569911957, 0.01922927424311638, -0.10601763427257538, 0.020121244713664055, -0.0005900668329559267, -0.08360151946544647, 0.03996668756008148, -1.961943234205592e-33, 0.02628525160253048, -0.08006194233894348, -0.001226023305207491, 0.012481508776545525, 0.07803099602460861, -0.02621171437203884, -0.001749325543642044, 0.061437904834747314, -0.028391150757670403, -0.000419358053477481, -0.009130536578595638, 0.10900174081325531, -0.02265322022140026, -0.0026117940433323383, -0.028918059542775154, 0.12813197076320648, -0.1472313553094864, 0.04185900092124939, -0.060571372509002686, 0.052473511546850204, 0.03634140267968178, 0.054129429161548615, -0.06887154281139374, 0.03313305228948593, -0.0806468203663826, -0.0028728845063596964, 0.05883371829986572, 0.030809495598077774, -0.0013581805396825075, 0.022698605433106422, -0.0749373510479927, -0.08153532445430756, 0.021625636145472527, -0.014969994314014912, 0.02981247939169407, 0.0778270959854126, 0.014585742726922035, -0.03226304426789284, -0.07139771431684494, 0.08041487634181976, 0.014620057307183743, -0.054625410586595535, 0.006130589172244072, -0.02620958536863327, 0.021423369646072388, -0.006309964694082737, 0.016803862527012825, -0.06750420480966568, -0.11537312716245651, 0.03933925926685333, -0.04622199386358261, 0.08984264731407166, 0.03205261752009392, 0.03457634896039963, 0.04765557870268822, 0.07692471891641617, 0.019346265122294426, -0.030477844178676605, 0.057007644325494766, 0.040661271661520004, -0.0027191033586859703, 0.07722746580839157, -0.08374612033367157, -0.062479157000780106, -0.013142794370651245, 0.02560611069202423, 0.04023145139217377, -0.0338955782353878, -0.09811673313379288, -0.022616231814026833, 0.08443046361207962, -0.02541159838438034, -0.06777453422546387, 0.03562065213918686, -0.056101858615875244, 0.08155560493469238, -0.023571867495775223, -0.0037592495791614056, 0.03821491822600365, -0.07738235592842102, 0.06106683984398842, 0.01820324920117855, -0.015365098603069782, -0.0330304354429245, -0.05967918783426285, 0.014847583137452602, 0.005677474662661552, -0.07187917083501816, 0.06551980972290039, 0.00912251602858305, -0.013086286373436451, -0.02393009327352047, 0.045069385319948196, -0.03711117058992386, -0.10353566706180573, -3.0391123039451436e-34, -0.05955975875258446, 0.031636741012334824, 0.03942602872848511, 0.00739933829754591, 0.01793401688337326, -0.022777734324336052, 0.04154285416007042, 0.01713932864367962, 0.06177690997719765, 0.0033868958707898855, 0.030802058055996895, -0.02305702120065689, 0.10406170040369034, 0.12488406151533127, -0.012828275561332703, -0.06102108955383301, 0.005905053578317165, 0.08224885165691376, -0.04644389450550079, 0.024816716089844704, 0.035753872245550156, 0.07826370745897293, -0.07537214457988739, 0.053909506648778915, 0.01191213633865118, 0.0260989461094141, 0.009298183023929596, 0.020008163526654243, -0.07014646381139755, -0.024314068257808685, -0.017562678083777428, 0.07294656336307526, -0.00891813449561596, -0.04966561868786812, 0.053232841193675995, 0.08287328481674194, 0.019530488178133965, 0.05837566778063774, 0.018036440014839172, 0.003683418268337846, -0.014078323729336262, -0.06106632947921753, -0.014239628799259663, 0.005145404487848282, 0.024549074470996857, 0.029494240880012512, 0.04376950114965439, 0.026939520612359047, 0.018856720998883247, -0.0745585635304451, -0.09336472302675247, 0.05478251725435257, 0.00747326435521245, 0.09474775195121765, 0.031248487532138824, -0.09508969634771347, 0.06397591531276703, -0.03160025179386139, -0.04155789688229561, 0.06990159302949905, -0.05404065549373627, 0.01304285041987896, -0.03251352533698082, -0.05485885590314865, -0.05755738541483879, -0.1470613330602646, -0.03426142781972885, -0.012745414860546589, 0.047647081315517426, -0.04246991127729416, 0.03338700532913208, -0.11892978101968765, -0.04940800741314888, 0.01441209390759468, -0.026885585859417915, 0.03412894904613495, -0.031792499125003815, 0.009363963268697262, -0.008606036193668842, 0.06803183257579803, 0.004527343902736902, -0.034079890698194504, 0.09390869736671448, -0.006654359865933657, -0.00926261581480503, 0.08466543257236481, -0.03449495881795883, -0.03756675124168396, 0.028368594124913216, 0.035161685198545456, -0.06068282946944237, -0.012278076261281967, 0.035229362547397614, 0.03585569187998772, -0.07837014645338058, -2.4230191186802585e-08, 0.00594351114705205, 0.04900767654180527, 0.09386416524648666, 0.07803063094615936, -0.08567968010902405, 0.08260183036327362, -0.06136341392993927, -0.011406046338379383, -0.06745854020118713, 0.03179599717259407, -0.09160968661308289, 0.02565491385757923, 0.001360552734695375, 0.014832566492259502, 0.03832361474633217, 0.006414550356566906, 0.1271379590034485, -0.049917303025722504, 0.055454693734645844, 0.010138502344489098, 0.034451983869075775, 0.06410364806652069, 0.04613678902387619, -0.05595358833670616, -0.007912407629191875, 0.0010086044203490019, -0.06979620456695557, -0.031133055686950684, -0.01006515696644783, -0.013669956475496292, -0.048093508929014206, 0.02957054413855076, -0.08506479859352112, -0.0022323697339743376, 0.04139229282736778, -0.03207872435450554, 0.021218804642558098, 0.11344936490058899, 0.041770320385694504, 0.005231182556599379, -0.00018839261610992253, -0.004717672243714333, 0.01601797714829445, 0.02661590278148651, -0.023382417857646942, -0.028987452387809753, -0.03106301836669445, 0.05187750235199928, -0.042397938668727875, 0.05109312757849693, -0.02042124606668949, -0.03962772712111473, -0.019815634936094284, 0.01955004781484604, 0.013296159915626049, -0.11997226625680923, -0.03989474102854729, -0.0030049660708755255, 0.01873285137116909, 0.03754880279302597, 0.09432602673768997, -0.020780516788363457, -0.04800991714000702, 0.008232343941926956], [0.032020825892686844, 0.06610872596502304, -0.018421348184347153, -0.06056195870041847, -0.0724724680185318, -0.009956827387213707, 0.044563133269548416, -0.09112268686294556, -0.023054929450154305, -0.057879287749528885, -0.001589495805092156, -0.17991752922534943, -0.04086504876613617, -0.005223070736974478, -0.04083407670259476, -0.0008952039061114192, -0.005838806740939617, 0.030753709375858307, -0.07629502564668655, -0.006499418988823891, -0.0005283115315251052, -0.0441797599196434, 0.043195147067308426, 0.019877780228853226, -0.06045529991388321, -0.049050964415073395, 0.023193109780550003, 0.022686026990413666, 0.03192245960235596, 0.089900903403759, -0.0018350346945226192, -0.038984812796115875, -0.009598921053111553, 0.06305183470249176, -0.08252399414777756, -0.0580514632165432, -0.00012302128016017377, 0.059431105852127075, -0.019211485981941223, -0.012601334601640701, 0.015395216643810272, 0.05993684008717537, -0.011068952269852161, 0.019964346662163734, 0.0868854820728302, -2.037482408923097e-05, 0.05817573890089989, 0.030660293996334076, -0.05609435960650444, -0.013638759963214397, 0.030644075945019722, 0.014804291538894176, 0.047981709241867065, 0.006299151107668877, -0.06496501713991165, 0.05130329355597496, 0.005129467695951462, -0.004539405461400747, 0.005051962099969387, 0.01871732994914055, 0.011755510233342648, -0.03515919670462608, 0.04056253656744957, 0.040476080030202866, -0.008846272714436054, -0.024998383596539497, 0.01808238960802555, 0.04874043166637421, -0.04998045414686203, -0.031047021970152855, 0.023159049451351166, -0.004597610794007778, 0.0058749765157699585, 0.08166816830635071, 0.05676921829581261, 0.11220454424619675, -0.007634501904249191, 0.034240249544382095, -0.07195769995450974, 0.009473823942244053, 0.047822192311286926, 0.02584436535835266, 0.022562455385923386, -0.10791152715682983, 0.05506487935781479, -0.090715192258358, -0.015843501314520836, -0.03380464389920235, -0.033166561275720596, 0.025317352265119553, -0.05265553295612335, 0.07459738850593567, 0.14321942627429962, -0.07204742729663849, 0.012724366039037704, -0.11212235689163208, 0.01678185723721981, 0.08745172619819641, 0.05185281112790108, 0.047819968312978745, -0.09947304427623749, -0.02276727370917797, 0.01243973895907402, 0.08042644709348679, 0.052765026688575745, -0.03750261664390564, -0.022668585181236267, -0.03742910176515579, -0.0681152418255806, -0.08049744367599487, -0.05513852462172508, -0.0032529758755117655, 0.013755964115262032, 0.07066188007593155, 0.0858851745724678, 0.005917282775044441, -0.015408516861498356, -0.02751799114048481, -0.007524192798882723, 0.0026944789569824934, -0.024874094873666763, -0.0387226864695549, -0.027595486491918564, 0.07191608101129532, 0.025396879762411118, -0.09831371903419495, 0.03625888377428055, -2.3743450397114446e-33, 0.01721501350402832, -0.04887809231877327, 0.040077805519104004, -0.01391794253140688, 0.04397176578640938, -0.04974845051765442, 0.013300049118697643, -0.03899296000599861, -0.019362086430191994, -0.06839212030172348, -0.007778260391205549, 0.00920723844319582, -0.027160044759511948, -0.034182045608758926, 0.01986061967909336, 0.16438764333724976, -0.08687544614076614, 0.003686790820211172, -0.02893594279885292, -0.03181177377700806, 0.09875218570232391, 0.030194412916898727, -0.04540827497839928, 0.033442262560129166, -0.09422829002141953, 0.016836097463965416, 0.032096389681100845, -0.005928354803472757, 0.013264870271086693, -0.008081382140517235, -0.06680532544851303, -0.0716199055314064, 0.049527607858181, -0.010710330680012703, 0.04343502223491669, 0.002592558041214943, 0.07272853702306747, -0.029998408630490303, -0.04686048999428749, 0.048478808254003525, 0.023453889414668083, -0.00866260938346386, -0.04225071519613266, 0.029048524796962738, -0.029234781861305237, 0.029217684641480446, 0.02878492884337902, -0.03459895774722099, -0.07027978450059891, 0.10009033977985382, -0.017985539510846138, 0.0012417194666340947, 0.12572436034679413, 0.04368576034903526, 0.040130455046892166, 0.026314441114664078, -0.021685898303985596, -0.03348278999328613, 0.000950313697103411, -0.0038243266753852367, -0.017194505780935287, 0.05653214827179909, -0.040920477360486984, 0.019219961017370224, 0.017906054854393005, -0.0525507926940918, 0.07719862461090088, 0.06189350038766861, -0.11982856690883636, -0.04582624137401581, 0.09626180678606033, -0.040476761758327484, -0.06491662561893463, -0.0012837153626605868, -0.03723590448498726, 0.018476000055670738, 0.024032363668084145, -0.07492472976446152, 0.05003181844949722, -0.0427638478577137, -0.0003370610938873142, -0.041192568838596344, -0.05988631024956703, -0.06636055558919907, -0.051460277289152145, -0.05894516780972481, 0.06361784785985947, 0.008929003961384296, 0.042810916900634766, 0.03740644082427025, -0.003465545829385519, 0.026394447311758995, 0.023999014869332314, 0.016862357035279274, -0.08391520380973816, 6.1419639523537025e-34, -0.07709309458732605, -0.026744576171040535, -0.06186828017234802, 0.052827488631010056, 0.007412730250507593, 0.029115451499819756, 0.0015032188966870308, -0.06305737048387527, 0.028382545337080956, 0.07528726756572723, 0.003186271758750081, -0.09051664173603058, 0.0654837116599083, 0.08383554965257645, 0.01877027563750744, -0.05186225101351738, 0.022030523046851158, 0.09462614357471466, -0.04829225316643715, 0.036626383662223816, 0.02963775023818016, 0.11281884461641312, -0.046591635793447495, 0.006131874397397041, -0.09530503302812576, 0.012008463963866234, 0.07551166415214539, 0.009649110026657581, -0.07389457523822784, 0.029789822176098824, -0.02266407385468483, 0.04724600166082382, -0.030043892562389374, -0.015824174508452415, 0.05634009465575218, -0.07228035479784012, 0.07272635400295258, 0.023399611935019493, 0.013170997612178326, 0.03560495004057884, -0.043458689004182816, -0.024380739778280258, -0.08507270365953445, -0.051430974155664444, 0.07657035440206528, 0.055194124579429626, 0.08902441710233688, -0.06424803286790848, -0.028046924620866776, -0.06833740323781967, -0.0239753108471632, -0.028720535337924957, 0.039507411420345306, 0.05304677039384842, 0.029257170855998993, -0.08104594796895981, 0.07575917989015579, 0.00522954436019063, -0.07662832736968994, -0.008809786289930344, 0.03404669836163521, -0.003148789284750819, 0.008065869100391865, -0.08148163557052612, 0.01429812889546156, -0.07892826944589615, -0.03894242271780968, -0.01364810485392809, 0.027887336909770966, 0.011058100499212742, 0.06037005037069321, -0.03107128106057644, -0.0032500759698450565, -9.709578444017097e-05, -0.0813891589641571, -0.03853217884898186, -0.011763153597712517, 0.017229897901415825, 0.042759399861097336, -0.0047879908233881, -0.014880513772368431, -0.04066919907927513, 0.08205977082252502, -0.003681737929582596, -0.02625546231865883, 0.023808052763342857, 0.0006895939004607499, 0.020032022148370743, -0.008105241693556309, 0.06083535775542259, 0.06485976278781891, 0.049053020775318146, -0.007657570764422417, 0.04766211658716202, -0.0728951245546341, -2.7223919119023776e-08, -0.002908776281401515, 0.01477931346744299, -0.04483889415860176, 0.0401216596364975, -0.053581804037094116, 0.12869201600551605, -0.054626721888780594, -0.06946323066949844, 0.04171561077237129, 0.005527243949472904, -0.021553747355937958, -0.03930853307247162, 0.05253814905881882, 0.016972709447145462, -0.026681622490286827, 0.0024203122593462467, 0.06450150161981583, -0.07243907451629639, 0.020075270906090736, 0.07907252758741379, -0.07712830603122711, 0.055752355605363846, 0.053026095032691956, -0.06229894235730171, -0.0721023678779602, -0.002285007620230317, -0.01799783483147621, -0.02397545613348484, 0.02352319285273552, -0.0021470822393894196, -0.01849595084786415, 0.01714630238711834, -0.0441485270857811, 0.06099912151694298, 0.06393889337778091, -0.09602038562297821, -0.044792093336582184, 0.08880391716957092, 0.07083870470523834, 0.05681910365819931, -0.090086430311203, 0.030293604359030724, 0.034194376319646835, 0.04542301222681999, -0.017564011737704277, 0.0026487295981496572, -0.07730897516012192, 0.022421889007091522, -0.08230321109294891, 0.01252508070319891, 0.008320651948451996, -0.06693592667579651, -0.08159185200929642, 0.04856126010417938, 0.057362161576747894, -0.10146161168813705, -0.019563980400562286, 0.04543671756982803, 0.020973628386855125, 0.04064115136861801, 0.03647468239068985, 0.005722453352063894, 0.013801936991512775, 0.018641997128725052]]\n",
      "\n",
      "TIME FOR RESPONSE EMBEDDINGS:  6.426454067230225\n",
      "\n",
      "- - - - - -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_gab = content_gab[:200]\n",
    "before = time.time()\n",
    "content_gab['text_embeddings'] = content_gab['text'].apply(generate_embeddings)\n",
    "after_text = time.time()\n",
    "print(content_gab.iloc[1]['text_embeddings'])\n",
    "print('\\nTIME FOR TEXT EMBEDDINGS: ', after_text - before)\n",
    "print('\\n- - - - - -\\n')\n",
    "content_gab['response_embeddings'] = content_gab['response'].apply(generate_embeddings)\n",
    "after_response = time.time()\n",
    "print(content_gab.iloc[2]['response_embeddings'])\n",
    "print('\\nTIME FOR RESPONSE EMBEDDINGS: ', after_response - after_text)\n",
    "print('\\n- - - - - -\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for constructing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(row):\n",
    "    text_utterances = row['text_embeddings']\n",
    "    response_utterances = row['response_embeddings']\n",
    "    # text_utterances = row['text']\n",
    "    # response_utterances = row['response']\n",
    "\n",
    "    root = text_utterances[0]\n",
    "    children = text_utterances[1:] + response_utterances\n",
    "    num_nodes = len(children) +1\n",
    "\n",
    "    # for t in text_utterances:\n",
    "    #     print(t)\n",
    "    # print()\n",
    "    # for r in response_utterances:\n",
    "    #     print(r)\n",
    "    # print()\n",
    "    # ids = [[0, i] for i in range(1, num_nodes)]\n",
    "    # print(ids)\n",
    "    # edge_index = torch.tensor(\n",
    "    #     [[0]*num_nodes, list(range(1, num_nodes)\n",
    "    # )], dtype=torch.long)\n",
    "    edge_index = torch.tensor(\n",
    "        [[0, i] for i in range(1, num_nodes)], dtype=torch.long\n",
    "    ).t().contiguous()\n",
    "    # edge_index = torch.tensor(\n",
    "    #     [[0] * len(children), list(range(1, num_nodes))], dtype=torch.long\n",
    "    # )\n",
    "    \n",
    "\n",
    "    # print(row['text_labels_encoded'])\n",
    "    # print()\n",
    "    # print(row['response_labels_encoded'])\n",
    "    # print(type(row['text_labels_encoded']), row['text_labels_encoded'].shape, row['text_labels_encoded'])\n",
    "    # print(type(row['response_labels_encoded']), row['response_labels_encoded'].shape, row['response_labels_encoded'])\n",
    "\n",
    "    # ls = np.concatenate((row['text_labels_encoded'], row['response_labels_encoded']))\n",
    "    ls = np.concatenate((row['text_labels'], row['response_labels'])).astype(int)\n",
    "    \n",
    "    print(ls)\n",
    "\n",
    "    print(ls.shape)\n",
    "    print(type(ls))\n",
    "    print(type(ls[0]))\n",
    "    \n",
    "    labels = torch.tensor(ls, dtype=torch.int32)\n",
    "\n",
    "    print(labels)\n",
    "\n",
    "    node_features = torch.tensor([root] + children, dtype=torch.float)\n",
    "    \n",
    "    # print(node_features.shape)\n",
    "    # print(edge_index.shape)\n",
    "    # print(labels.shape)\n",
    "    # print('sss')\n",
    "    data = Data(x=node_features, edge_index=edge_index, y=labels)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing graphs for all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "(13,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0 0 0]\n",
      "(8,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 1 0 0 0]\n",
      "(7,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0]\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 1 0 1 0 0 0]\n",
      "(9,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 1, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0 0 0 0]\n",
      "(9,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 1 0 1 0 0 0 0]\n",
      "(10,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0]\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0], dtype=torch.int32)\n",
      "[0 1 1 1 0 0 0]\n",
      "(7,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0]\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "(13,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0 0 0 0 0]\n",
      "(10,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(15,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0 0 0 0]\n",
      "(9,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0 0 0 0 0 0]\n",
      "(11,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 1 0 1 0 1 0 1 1 0 0 0]\n",
      "(15,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0 0 0 0 0 0]\n",
      "(10,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0]\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 1 0 0 0 1 0 0 0]\n",
      "(12,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0 0 0 0]\n",
      "(9,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0]\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0], dtype=torch.int32)\n",
      "[0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "(13,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0 0 0 0]\n",
      "(9,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 1 0 0 0 0]\n",
      "(7,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "(12,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(27,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0 0 0 0 0]\n",
      "(10,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 1 0 0 0]\n",
      "(7,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0 0 0 0 0]\n",
      "(10,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "(14,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0 0 0 0 0]\n",
      "(10,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(26,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0]\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0]\n",
      "(20,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       dtype=torch.int32)\n",
      "[1 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0]\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0], dtype=torch.int32)\n",
      "[1 1 1 0 0 0 0 0 0 0 0]\n",
      "(11,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0]\n",
      "(18,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "       dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0]\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0], dtype=torch.int32)\n",
      "[0 0 0]\n",
      "(3,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0 0]\n",
      "(7,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0 0 0 0 0]\n",
      "(10,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "(12,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0]\n",
      "(2,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0]\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0], dtype=torch.int32)\n",
      "[1 0 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0 0]\n",
      "(7,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(18,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       dtype=torch.int32)\n",
      "[1 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 1 0 0 0]\n",
      "(7,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0]\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(22,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 0]\n",
      "(6,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0]\n",
      "(2,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0]\n",
      "(2,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(22,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 1 0 0 0]\n",
      "(7,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 0 1 0 0 0]\n",
      "(10,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      "(16,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(14,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 1 1 0 0 0 0 0 0 0]\n",
      "(11,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0]\n",
      "(16,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[1 0 0 0]\n",
      "(4,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([1, 0, 0, 0], dtype=torch.int32)\n",
      "[0 1 0 0 0]\n",
      "(5,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.int32)\n",
      "Data(x=[13, 384], edge_index=[2, 12], y=[13])\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "Number of nodes: 13\n",
      "Number of edges: 12\n",
      "\n",
      "200\n",
      "Graphs: \n",
      "Data(x=[13, 384], edge_index=[2, 12], y=[13])\n",
      "Data(x=[8, 384], edge_index=[2, 7], y=[8])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[6, 384], edge_index=[2, 5], y=[6])\n",
      "Data(x=[3, 384], edge_index=[2, 2], y=[3])\n",
      "Data(x=[7, 384], edge_index=[2, 6], y=[7])\n",
      "Data(x=[1, 384], edge_index=[0], y=[1])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[9, 384], edge_index=[2, 8], y=[9])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[3, 384], edge_index=[2, 2], y=[3])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[9, 384], edge_index=[2, 8], y=[9])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[3, 384], edge_index=[2, 2], y=[3])\n",
      "Data(x=[3, 384], edge_index=[2, 2], y=[3])\n",
      "Data(x=[6, 384], edge_index=[2, 5], y=[6])\n",
      "Data(x=[10, 384], edge_index=[2, 9], y=[10])\n",
      "Data(x=[1, 384], edge_index=[0], y=[1])\n",
      "Data(x=[7, 384], edge_index=[2, 6], y=[7])\n",
      "Data(x=[3, 384], edge_index=[2, 2], y=[3])\n",
      "Data(x=[6, 384], edge_index=[2, 5], y=[6])\n",
      "Data(x=[1, 384], edge_index=[0], y=[1])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[13, 384], edge_index=[2, 12], y=[13])\n",
      "Data(x=[10, 384], edge_index=[2, 9], y=[10])\n",
      "Data(x=[15, 384], edge_index=[2, 14], y=[15])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[9, 384], edge_index=[2, 8], y=[9])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[6, 384], edge_index=[2, 5], y=[6])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[11, 384], edge_index=[2, 10], y=[11])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[15, 384], edge_index=[2, 14], y=[15])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[6, 384], edge_index=[2, 5], y=[6])\n",
      "Data(x=[10, 384], edge_index=[2, 9], y=[10])\n",
      "Data(x=[3, 384], edge_index=[2, 2], y=[3])\n",
      "Data(x=[6, 384], edge_index=[2, 5], y=[6])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[1, 384], edge_index=[0], y=[1])\n",
      "Data(x=[6, 384], edge_index=[2, 5], y=[6])\n",
      "Data(x=[6, 384], edge_index=[2, 5], y=[6])\n",
      "Data(x=[12, 384], edge_index=[2, 11], y=[12])\n",
      "Data(x=[3, 384], edge_index=[2, 2], y=[3])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[3, 384], edge_index=[2, 2], y=[3])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[9, 384], edge_index=[2, 8], y=[9])\n",
      "Data(x=[6, 384], edge_index=[2, 5], y=[6])\n",
      "Data(x=[1, 384], edge_index=[0], y=[1])\n",
      "Data(x=[13, 384], edge_index=[2, 12], y=[13])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[9, 384], edge_index=[2, 8], y=[9])\n",
      "Data(x=[7, 384], edge_index=[2, 6], y=[7])\n",
      "Data(x=[3, 384], edge_index=[2, 2], y=[3])\n",
      "Data(x=[3, 384], edge_index=[2, 2], y=[3])\n",
      "Data(x=[12, 384], edge_index=[2, 11], y=[12])\n",
      "Data(x=[27, 384], edge_index=[2, 26], y=[27])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[3, 384], edge_index=[2, 2], y=[3])\n",
      "Data(x=[10, 384], edge_index=[2, 9], y=[10])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[7, 384], edge_index=[2, 6], y=[7])\n",
      "Data(x=[6, 384], edge_index=[2, 5], y=[6])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[10, 384], edge_index=[2, 9], y=[10])\n",
      "Data(x=[14, 384], edge_index=[2, 13], y=[14])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[3, 384], edge_index=[2, 2], y=[3])\n",
      "Data(x=[10, 384], edge_index=[2, 9], y=[10])\n",
      "Data(x=[5, 384], edge_index=[2, 4], y=[5])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n",
      "Data(x=[26, 384], edge_index=[2, 25], y=[26])\n",
      "Data(x=[4, 384], edge_index=[2, 3], y=[4])\n"
     ]
    }
   ],
   "source": [
    "graphs = []\n",
    "for index, row in content_gab.iterrows():\n",
    "    graphs.append(construct_graph(row))\n",
    "\n",
    "print(graphs[0])\n",
    "print('\\n- - - - - -\\n')\n",
    "print(f\"Number of nodes: {graphs[0].num_nodes}\")\n",
    "print(f\"Number of edges: {graphs[0].num_edges}\")\n",
    "\n",
    "print()\n",
    "print(len(graphs))\n",
    "print('Graphs: ')\n",
    "for i in range(0, 100):\n",
    "    print(graphs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge to one graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[1198, 384], edge_index=[2, 998], y=[1198])\n",
      "Merged Node Features:\n",
      "tensor([[ 0.0813, -0.0214, -0.0582,  ...,  0.0492,  0.0206,  0.0012],\n",
      "        [-0.0027,  0.0063, -0.0170,  ..., -0.0939, -0.0490,  0.0221],\n",
      "        [ 0.0185, -0.0804,  0.0782,  ..., -0.0679, -0.0118,  0.0497],\n",
      "        ...,\n",
      "        [ 0.0485,  0.0435, -0.0563,  ..., -0.0356,  0.0521, -0.0419],\n",
      "        [ 0.0144,  0.0081, -0.0431,  ..., -0.0029,  0.0659, -0.0579],\n",
      "        [ 0.1278,  0.0149, -0.0109,  ...,  0.0264,  0.0013, -0.0679]])\n",
      "Merged Edge Index:\n",
      "tensor([[   0,    0,    0,  ..., 1193, 1193, 1193],\n",
      "        [   1,    2,    3,  ..., 1195, 1196, 1197]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize empty lists to store the merged node features, edge indices, and labels (y)\n",
    "merged_x = []\n",
    "merged_edge_index = []\n",
    "merged_y = []\n",
    "\n",
    "# Keep track of the offset for node indices in subsequent graphs\n",
    "node_offset = 0\n",
    "\n",
    "# Iterate over each graph in the list\n",
    "for graph in graphs:\n",
    "    # Concatenate node features\n",
    "    merged_x.append(graph.x)\n",
    "    \n",
    "    # Adjust edge indices: add the current node_offset to the second row of edge_index\n",
    "    merged_edge_index.append(graph.edge_index + node_offset)\n",
    "    \n",
    "    # Concatenate labels (y), the target labels from each graph\n",
    "    merged_y.append(graph.y)\n",
    "    \n",
    "    # Update node_offset for the next graph\n",
    "    node_offset += graph.x.size(0)\n",
    "\n",
    "# Concatenate all node features, edge indices, and labels\n",
    "merged_x = torch.cat(merged_x, dim=0)\n",
    "merged_edge_index = torch.cat(merged_edge_index, dim=1)\n",
    "merged_y = torch.cat(merged_y, dim=0)\n",
    "\n",
    "# Create a new graph with merged node features, edge indices, and labels (y)\n",
    "merged_graph = Data(x=merged_x, edge_index=merged_edge_index, y=merged_y)\n",
    "\n",
    "print(merged_graph)\n",
    "# Print the merged graph details\n",
    "print(\"Merged Node Features:\")\n",
    "print(merged_graph.x)\n",
    "print(\"Merged Edge Index:\")\n",
    "print(merged_graph.edge_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node level to graph level labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Aggregating node-level labels into graph-level labels\n",
    "#def convert_to_graph_level(dataset):\n",
    " #   new_dataset = []\n",
    " #   for data in dataset:\n",
    " #       # Example: Majority vote for classification\n",
    "  #      #graph_label = data.y.mode()[0]  # Use the most frequent label\n",
    "   #     graph_label = 1 if (data.y == 1).sum().item() > 0 else 0\n",
    "    #    #data.y = graph_label.unsqueeze(0)  # Ensure shape [1]\n",
    "     #   data.y = graph_label\n",
    "      #  new_dataset.append(data)\n",
    "    #return new_dataset\n",
    "\n",
    "# Convert dataset\n",
    "#new_dataset = convert_to_graph_level(graphs)\n",
    "#for i in new_dataset:\n",
    " #   print(i.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given percentage of shuffled dataset is test fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(graphs)\n",
    "\n",
    "size_train = len(graphs) - len(graphs) // 10 # 10% test dataset\n",
    "\n",
    "train_dataset = graphs[:size_train]\n",
    "test_dataset = graphs[size_train:]\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = []\n",
    "#for index, row in content_gab.iterrows():\n",
    "#    y.append(np.concatenate((row['text_labels'], row['response_labels'])).astype(int))\n",
    "\n",
    "#for i, q in enumerate(y):\n",
    "#    print(q)\n",
    "#    if i >= 5:\n",
    "#       print('\\n- - - -')\n",
    "#       break\n",
    "#for i, q in enumerate(graphs):\n",
    " #   print(q)\n",
    " #   if i >= 5:\n",
    " #      break\n",
    "\n",
    "#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=36851234)\n",
    "#folds = list(rskf.split(graphs, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batching of graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[411, 384], edge_index=[2, 347], y=[411], batch=[411], ptr=[65])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(x=[368, 384], edge_index=[2, 304], y=[368], batch=[368], ptr=[65])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 52\n",
      "DataBatch(x=[317, 384], edge_index=[2, 265], y=[317], batch=[317], ptr=[53])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(384, 16)\n",
      "  (conv2): GCNConv(16, 2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "input_dim = merged_graph.x.shape[1]    # embedding dimensionality\n",
    "data = merged_graph\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(input_dim, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "model.eval()\n",
    "\n",
    "out = model(data.x, data.edge_index)\n",
    "#visualize(out, color=data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6894, Test Accuracy: 0.7922\n",
      "Epoch: 002, Loss: 0.6319, Test Accuracy: 0.7922\n",
      "Epoch: 003, Loss: 0.5803, Test Accuracy: 0.7922\n",
      "Epoch: 004, Loss: 0.5405, Test Accuracy: 0.7922\n",
      "Epoch: 005, Loss: 0.5318, Test Accuracy: 0.7922\n",
      "Epoch: 006, Loss: 0.5238, Test Accuracy: 0.7922\n",
      "Epoch: 007, Loss: 0.5417, Test Accuracy: 0.7922\n",
      "Epoch: 008, Loss: 0.5459, Test Accuracy: 0.7922\n",
      "Epoch: 009, Loss: 0.5421, Test Accuracy: 0.7922\n",
      "Epoch: 010, Loss: 0.5371, Test Accuracy: 0.7922\n",
      "Epoch: 011, Loss: 0.5251, Test Accuracy: 0.7922\n",
      "Epoch: 012, Loss: 0.5132, Test Accuracy: 0.7922\n",
      "Epoch: 013, Loss: 0.5104, Test Accuracy: 0.7922\n",
      "Epoch: 014, Loss: 0.5037, Test Accuracy: 0.7922\n",
      "Epoch: 015, Loss: 0.4979, Test Accuracy: 0.7922\n",
      "Epoch: 016, Loss: 0.5026, Test Accuracy: 0.7922\n",
      "Epoch: 017, Loss: 0.4963, Test Accuracy: 0.7922\n",
      "Epoch: 018, Loss: 0.5015, Test Accuracy: 0.7922\n",
      "Epoch: 019, Loss: 0.4988, Test Accuracy: 0.7922\n",
      "Epoch: 020, Loss: 0.4973, Test Accuracy: 0.7922\n",
      "Epoch: 021, Loss: 0.5009, Test Accuracy: 0.7922\n",
      "Epoch: 022, Loss: 0.4950, Test Accuracy: 0.7922\n",
      "Epoch: 023, Loss: 0.4913, Test Accuracy: 0.7922\n",
      "Epoch: 024, Loss: 0.4878, Test Accuracy: 0.7922\n",
      "Epoch: 025, Loss: 0.4851, Test Accuracy: 0.7922\n",
      "Epoch: 026, Loss: 0.4838, Test Accuracy: 0.7922\n",
      "Epoch: 027, Loss: 0.4807, Test Accuracy: 0.7922\n",
      "Epoch: 028, Loss: 0.4838, Test Accuracy: 0.7922\n",
      "Epoch: 029, Loss: 0.4807, Test Accuracy: 0.7922\n",
      "Epoch: 030, Loss: 0.4834, Test Accuracy: 0.7922\n",
      "Epoch: 031, Loss: 0.4754, Test Accuracy: 0.7922\n",
      "Epoch: 032, Loss: 0.4736, Test Accuracy: 0.7922\n",
      "Epoch: 033, Loss: 0.4727, Test Accuracy: 0.7922\n",
      "Epoch: 034, Loss: 0.4714, Test Accuracy: 0.7922\n",
      "Epoch: 035, Loss: 0.4709, Test Accuracy: 0.7922\n",
      "Epoch: 036, Loss: 0.4680, Test Accuracy: 0.7922\n",
      "Epoch: 037, Loss: 0.4648, Test Accuracy: 0.7922\n",
      "Epoch: 038, Loss: 0.4693, Test Accuracy: 0.7922\n",
      "Epoch: 039, Loss: 0.4672, Test Accuracy: 0.7922\n",
      "Epoch: 040, Loss: 0.4652, Test Accuracy: 0.7922\n",
      "Epoch: 041, Loss: 0.4633, Test Accuracy: 0.7922\n",
      "Epoch: 042, Loss: 0.4546, Test Accuracy: 0.7922\n",
      "Epoch: 043, Loss: 0.4537, Test Accuracy: 0.7922\n",
      "Epoch: 044, Loss: 0.4588, Test Accuracy: 0.7922\n",
      "Epoch: 045, Loss: 0.4522, Test Accuracy: 0.7922\n",
      "Epoch: 046, Loss: 0.4519, Test Accuracy: 0.7922\n",
      "Epoch: 047, Loss: 0.4491, Test Accuracy: 0.7922\n",
      "Epoch: 048, Loss: 0.4471, Test Accuracy: 0.7922\n",
      "Epoch: 049, Loss: 0.4477, Test Accuracy: 0.7922\n",
      "Epoch: 050, Loss: 0.4470, Test Accuracy: 0.7922\n",
      "Epoch: 051, Loss: 0.4444, Test Accuracy: 0.7922\n",
      "Epoch: 052, Loss: 0.4458, Test Accuracy: 0.7922\n",
      "Epoch: 053, Loss: 0.4397, Test Accuracy: 0.7922\n",
      "Epoch: 054, Loss: 0.4384, Test Accuracy: 0.7922\n",
      "Epoch: 055, Loss: 0.4339, Test Accuracy: 0.7922\n",
      "Epoch: 056, Loss: 0.4350, Test Accuracy: 0.7922\n",
      "Epoch: 057, Loss: 0.4301, Test Accuracy: 0.7922\n",
      "Epoch: 058, Loss: 0.4339, Test Accuracy: 0.7922\n",
      "Epoch: 059, Loss: 0.4311, Test Accuracy: 0.7922\n",
      "Epoch: 060, Loss: 0.4248, Test Accuracy: 0.7922\n",
      "Epoch: 061, Loss: 0.4240, Test Accuracy: 0.7922\n",
      "Epoch: 062, Loss: 0.4260, Test Accuracy: 0.7922\n",
      "Epoch: 063, Loss: 0.4197, Test Accuracy: 0.7922\n",
      "Epoch: 064, Loss: 0.4194, Test Accuracy: 0.7922\n",
      "Epoch: 065, Loss: 0.4156, Test Accuracy: 0.7922\n",
      "Epoch: 066, Loss: 0.4193, Test Accuracy: 0.7922\n",
      "Epoch: 067, Loss: 0.4191, Test Accuracy: 0.7922\n",
      "Epoch: 068, Loss: 0.4132, Test Accuracy: 0.7922\n",
      "Epoch: 069, Loss: 0.4152, Test Accuracy: 0.7922\n",
      "Epoch: 070, Loss: 0.4135, Test Accuracy: 0.7922\n",
      "Epoch: 071, Loss: 0.4151, Test Accuracy: 0.7922\n",
      "Epoch: 072, Loss: 0.4104, Test Accuracy: 0.7922\n",
      "Epoch: 073, Loss: 0.4089, Test Accuracy: 0.7922\n",
      "Epoch: 074, Loss: 0.4109, Test Accuracy: 0.7922\n",
      "Epoch: 075, Loss: 0.4063, Test Accuracy: 0.7922\n",
      "Epoch: 076, Loss: 0.4031, Test Accuracy: 0.7922\n",
      "Epoch: 077, Loss: 0.4042, Test Accuracy: 0.7922\n",
      "Epoch: 078, Loss: 0.4018, Test Accuracy: 0.7922\n",
      "Epoch: 079, Loss: 0.4030, Test Accuracy: 0.7922\n",
      "Epoch: 080, Loss: 0.3962, Test Accuracy: 0.7922\n",
      "Epoch: 081, Loss: 0.3971, Test Accuracy: 0.7922\n",
      "Epoch: 082, Loss: 0.3985, Test Accuracy: 0.7922\n",
      "Epoch: 083, Loss: 0.3948, Test Accuracy: 0.7922\n",
      "Epoch: 084, Loss: 0.3915, Test Accuracy: 0.7922\n",
      "Epoch: 085, Loss: 0.3937, Test Accuracy: 0.7922\n",
      "Epoch: 086, Loss: 0.3944, Test Accuracy: 0.7922\n",
      "Epoch: 087, Loss: 0.3893, Test Accuracy: 0.7922\n",
      "Epoch: 088, Loss: 0.3886, Test Accuracy: 0.7922\n",
      "Epoch: 089, Loss: 0.3874, Test Accuracy: 0.7922\n",
      "Epoch: 090, Loss: 0.3867, Test Accuracy: 0.7922\n",
      "Epoch: 091, Loss: 0.3853, Test Accuracy: 0.7922\n",
      "Epoch: 092, Loss: 0.3857, Test Accuracy: 0.7922\n",
      "Epoch: 093, Loss: 0.3890, Test Accuracy: 0.7922\n",
      "Epoch: 094, Loss: 0.3831, Test Accuracy: 0.7922\n",
      "Epoch: 095, Loss: 0.3838, Test Accuracy: 0.7922\n",
      "Epoch: 096, Loss: 0.3797, Test Accuracy: 0.7922\n",
      "Epoch: 097, Loss: 0.3751, Test Accuracy: 0.7922\n",
      "Epoch: 098, Loss: 0.3755, Test Accuracy: 0.7922\n",
      "Epoch: 099, Loss: 0.3764, Test Accuracy: 0.7922\n",
      "Epoch: 100, Loss: 0.3776, Test Accuracy: 0.7922\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "data = merged_graph\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "    loss = criterion(out, data.y.long())  # Compute the loss for the training dataset.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    test_correct = pred == data.y.long()  # Check against ground-truth labels.\n",
    "    test_acc = int(test_correct.sum()) / len(data.y)  # Derive ratio of correct predictions.\n",
    "    return test_acc\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()  # Pass the training dataset to train function.\n",
    "    test_acc = test()  # Pass the test dataset to test function.\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Test Accuracy: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7922\n"
     ]
    }
   ],
   "source": [
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphNN model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GraphNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for training the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data):\n",
    "    for train_idx, test_idx in folds:\n",
    "        train_graphs = [graphs[i] for i in train_idx]\n",
    "        test_graphs = [graphs[i] for i in test_idx]\n",
    "\n",
    "        model.train()\n",
    "        for data in train_graphs:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in test_graphs:\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += int((pred == data.y).sum())\n",
    "            total += len(pred)\n",
    "    print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m graphs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]    \u001b[38;5;66;03m# embedding dimensionality\u001b[39;00m\n\u001b[0;32m      2\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m----> 3\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m GraphNN(input_dim, hidden_dim, output_dim)\n\u001b[0;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "input_dim = graphs[0].x.shape[1]    # embedding dimensionality\n",
    "hidden_dim = 64\n",
    "output_dim = len(label_encoder.classes_)\n",
    "\n",
    "model = GraphNN(input_dim, hidden_dim, output_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train(model, optimizer, criterion, graphs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
