{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we're creating a CNN to compare it with GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import necessary stuff for\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', 10)  # Limit number of rows displayed\n",
    "pd.set_option('display.width', 1000)  # Set max width for table\n",
    "pd.set_option('display.colheader_justify', 'center')  # Center-align column headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_chars(value):\n",
    "    if isinstance(value, str):  \n",
    "        return value.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').replace('  ', ' ').strip()\n",
    "    return value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading gab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id                                                text                        hate_speech_idx                      response                     \n",
      "0                                    1. 39869714\\r\\n  1. i joined gab to remind myself how retarded ...         [1]      [\"Using words that insult one group while defe...\n",
      "1  1. 39845588\\r\\n2. \\t39848775\\r\\n3. \\t\\t3991101...  1. This is what the left is really scared of. ...         [3]      ['You can disagree with someones opinion witho...\n",
      "2                   1. 37485560\\r\\n2. \\t37528625\\r\\n  1. It makes you an asshole.\\r\\n2. \\tGive it to...         [2]      ['Your argument is more rational if you leave ...\n",
      "3                   1. 39787626\\r\\n2. \\t39794481\\r\\n  1. So they manage to provide a whole lot of da...         [2]      [\"You shouldn't generalize a specific group or...\n",
      "4  1. 37957930\\r\\n2. \\t39953348\\r\\n3. \\t\\t3996521...  1. Hi there, i,m Keith, i hope you are doing w...         [3]      ['If someone is rude it is better to ignore th...\n",
      "5                                    1. 38462712\\r\\n                    1. you sound like a faggot \\r\\n         [1]      [\"Please be careful with the words you choose ...\n",
      "6  1. 38052531\\r\\n2. \\t38103723\\r\\n3. \\t\\t3851658...  1. Hi developers, give us a follow for updates...         [3]      [\"The words you've chosen are hateful and dero...\n",
      "7                   1. 38352488\\r\\n2. \\t38373190\\r\\n  1. Well, you are the fuckers that lit the matc...         [2]      ['Please refrain from using such horrible bigo...\n",
      "8  1. 37238116\\r\\n2. \\t38348543\\r\\n3. \\t\\t3837623...  1. SELF-HATING WHITE CUCKS ON PARADE\\r\\n2. \\tD...      [1, 3]      ['Your words are derogatory and offensive, and...\n",
      "9  1. 37358018\\r\\n2. \\t37359176\\r\\n3. \\t\\t3738104...  1. So after 6 years and nearly 11K followers, ...         [3]      [\"Woah! Please don't use such strong and offen...\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "Index(['id', 'text', 'hate_speech_idx', 'response'], dtype='object')\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "1. 39845588\n",
      "2. \t39848775\n",
      "3. \t\t39911017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_gab = pd.read_csv('gab_reddit_benchmark/gab.csv')\n",
    "\n",
    "content_gab[\"text\"] = content_gab[\"text\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "content_gab[\"response\"] = content_gab[\"response\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\")\n",
    "content_gab[\"hate_speech_idx\"] = content_gab[\"hate_speech_idx\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "\n",
    "# content_gab[\"text\"] = content_gab[\"text\"].apply(clean_special_chars)\n",
    "# content_gab[\"response\"] = content_gab[\"response\"].apply(clean_special_chars)\n",
    "\n",
    "for index, row in content_gab.iterrows():\n",
    "    row['text'] = row['text'].replace(\"'\", '\"')\n",
    "    row['response'] = row['response'].replace(\"'\", '\"')\n",
    "\n",
    "# content_gab = content_gab.applymap(clean_special_chars)\n",
    "print(content_gab.head(n=10))\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab.columns)\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab.iloc[1]['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wrobl\\AppData\\Local\\Temp\\ipykernel_13228\\2213304540.py:30: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace('] [', ', ') if isinstance(x, str) else x)\n",
      "C:\\Users\\Wrobl\\AppData\\Local\\Temp\\ipykernel_13228\\2213304540.py:31: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace(']  [', ', ') if isinstance(x, str) else x)\n",
      "C:\\Users\\Wrobl\\AppData\\Local\\Temp\\ipykernel_13228\\2213304540.py:32: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: 'n/a' if isinstance(x, str) and x.strip() == '' else x)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_first_number(input_string):\n",
    "    match = re.search(r'\\d{2,}', input_string)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None\n",
    "\n",
    "content_gab['extracted_id'] = content_gab['id'].apply(get_first_number)\n",
    "\n",
    "# Find duplicate rows based on 'extracted_id'\n",
    "duplicates = content_gab[content_gab.duplicated(subset=['extracted_id'], keep=False)]\n",
    "filtered_groups = []\n",
    "grouped = content_gab.groupby('extracted_id')\n",
    "for key, group in grouped:\n",
    "    if len(group) > 1:\n",
    "        filtered_groups.append(group)\n",
    "\n",
    "merged_df = pd.concat(filtered_groups, ignore_index=True)\n",
    "\n",
    "merged_df.to_csv('gab_reddit_benchmark/gab_groups.csv', index=False)\n",
    "\n",
    "merged_df = grouped.agg({\n",
    "    'id': ' '.join,\n",
    "    'text': ' '.join,\n",
    "    'hate_speech_idx': ' '.join,\n",
    "    'response': ' '.join\n",
    "}).reset_index()\n",
    "merged_df.to_csv('gab_reddit_benchmark/gab_merged.csv')\n",
    "\n",
    "df = pd.read_csv('gab_reddit_benchmark/gab_merged.csv')\n",
    "df = df.applymap(lambda x: x.replace('] [', ', ') if isinstance(x, str) else x)\n",
    "df = df.applymap(lambda x: x.replace(']  [', ', ') if isinstance(x, str) else x)\n",
    "df = df.applymap(lambda x: 'n/a' if isinstance(x, str) and x.strip() == '' else x)\n",
    "df.to_csv('gab_reddit_benchmark/gab_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id                                                text                        hate_speech_idx                      response                     \n",
      "0  1. 37011807\\r\\n2. \\t37012801\\r\\n3. \\t\\t3701333...  1. 64% of Hispanics have IQ scores too low to ...     [2, 2, 3]    ['Racist and hate speech will not be tolerated...\n",
      "1                    1. 37012081\\r\\n 1. 37012081\\r\\n  1. LYING CUNT BITCH !!\\r\\n 1. LYING CUNT BITCH...        [1, 1]    ['Please refrain from using derogatory terms f...\n",
      "2                                    1. 37012105\\r\\n  1. Nigger Kills 2 cops https://www.google.com/...           [1]    ['The N word is highly offensive.   All ethnic...\n",
      "3                   1. 37012146\\r\\n2. \\t37013020\\r\\n  1. STABBED IN NECK   arrested but no descr   #...           [2]    ['Use of the n-word is unacceptable in our dis...\n",
      "4  1. 37012285\\r\\n2. \\t37013809\\r\\n3. \\t37013868\\r\\n  1. Retard 1st ClassÂ  w/ Cluster & Ribbon\\r\\n2....           [1]    ['We should try and not use words making fun o...\n",
      "5                                    1. 37012571\\r\\n  1. We should be happy they do fuck their cousi...           [1]    ['I cannot listen to your argument if you keep...\n",
      "6  1. 37012882\\r\\n2. \\t37013415\\r\\n3. \\t\\t3701686...  1. Stolen Valor! Prosecute the scum.\\r\\n2. \\t\\...        [3, 4]    ['Stick to your own beliefs without dismissing...\n",
      "7                                    1. 37013030\\r\\n  1. Yea, here's the scoop.  Fake News! \"We told...                                                                   \n",
      "8                                    1. 37013358\\r\\n           1. Throat-punch that faggot DeNigger\\r\\n           [1]    ['I hope that you can see that you are advocat...\n",
      "9                                    1. 37013922\\r\\n  1. https://www.youtube.com/watch?v=DmNRkp_fuoo...           [1]    ['Gender is a category that shouldnât be the b...\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "Index(['id', 'text', 'hate_speech_idx', 'response'], dtype='object')\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "1. 37011807\n",
      "2. \t37012801\n",
      "3. \t\t37013338\n",
      "4. \t\t37013511\n",
      "5. \t\t37333801\n",
      " 1. 37011807\n",
      "2. \t37012913\n",
      "3. \t\t37013738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_gab_m = pd.read_csv('gab_reddit_benchmark/gab_merged.csv')\n",
    "content_gab_m = content_gab_m.drop('Unnamed: 0', axis=1)\n",
    "content_gab_m = content_gab_m.drop('extracted_id', axis=1)\n",
    "\n",
    "\n",
    "content_gab_m[\"text\"] = content_gab_m[\"text\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "content_gab_m[\"response\"] = content_gab_m[\"response\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\")\n",
    "content_gab_m[\"hate_speech_idx\"] = content_gab_m[\"hate_speech_idx\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "\n",
    "for index, row in content_gab_m.iterrows():\n",
    "    row['text'] = row['text'].replace(\"'\", '\"')\n",
    "    row['response'] = row['response'].replace(\"'\", '\"')\n",
    "\n",
    "print(content_gab_m.head(n=10))\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab_m.columns)\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab_m.iloc[0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_text_labels(text_utterances_length, labels):\n",
    "    if not labels:\n",
    "        # return ['other'] * text_utterances_length\n",
    "        return [0] * text_utterances_length\n",
    "    new_labels = []\n",
    "    int_list = ast.literal_eval(labels)\n",
    "    for i in range(text_utterances_length):\n",
    "        if i+1 in int_list:\n",
    "            # new_labels.append('hate_speech')\n",
    "            new_labels.append(1)\n",
    "        else:\n",
    "            # new_labels.append('other')\n",
    "            new_labels.append(0)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting 'text' and 'response'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id                                                text                        text_labels                      response                       extracted_id response_labels\n",
      "0                                    1. 39869714\\r\\n  [1. i joined gab to remind myself how retarded...         [1]  [Using words that insult one group while defen...    39869714       [0, 0, 0]  \n",
      "1  1. 39845588\\r\\n2. \\t39848775\\r\\n3. \\t\\t3991101...  [1. This is what the left is really scared of....   [0, 0, 1]  [You can disagree with someones opinion withou...    39845588       [0, 0, 0]  \n",
      "2                   1. 37485560\\r\\n2. \\t37528625\\r\\n  [1. It makes you an asshole., 2. Give it to a ...      [0, 1]  [Your argument is more rational if you leave y...    37485560       [0, 0, 0]  \n",
      "3                   1. 39787626\\r\\n2. \\t39794481\\r\\n  [1. So they manage to provide a whole lot of d...      [0, 1]  [You shouldn't generalize a specific group or ...    39787626       [0, 0, 0]  \n",
      "4  1. 37957930\\r\\n2. \\t39953348\\r\\n3. \\t\\t3996521...  [1. Hi there, i,m Keith, i hope you are doing ...   [0, 0, 1]  [If someone is rude it is better to ignore the...    37957930       [0, 0, 0]  \n",
      "- - - - \n",
      "Index(['id', 'text', 'text_labels', 'response', 'extracted_id', 'response_labels'], dtype='object')\n",
      "1. 39869714\n",
      " [\"1. i joined gab to remind myself how retarded jew haters are. You wouldn't be typing on your abacus without them you retard.\"] [1] [\"Using words that insult one group while defending another group doesn't come across as helpful.\", 'You can make the same point more effectively without the use of hateful terminology.', 'Use of the r-word is unacceptable in our discourse as it demeans and insults people with mental disabilities.'] [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "text_column = []\n",
    "text_labels_column = []\n",
    "response_column = []\n",
    "response_labels_column = []\n",
    "\n",
    "for index, row in content_gab.iterrows():\n",
    "    text_utterances = row['text'].split('\\n')\n",
    "    text_utterances = list(filter(None, text_utterances))\n",
    "\n",
    "    for i, t in enumerate(text_utterances):\n",
    "        text_utterances[i] = clean_special_chars(t)\n",
    "    text_labels = mark_text_labels(len(text_utterances), row['hate_speech_idx'])\n",
    "\n",
    "    response_utterances = ast.literal_eval(row['response']) if row['response'] else []\n",
    "    for i, r in enumerate(response_utterances):\n",
    "        response_utterances[i] = clean_special_chars(r)\n",
    "    # response_labels = ['other'] * len(response_utterances)  \n",
    "    response_labels = [0] * len(response_utterances)  \n",
    "\n",
    "    \n",
    "    text_column.append(text_utterances)\n",
    "    text_labels_column.append(text_labels)\n",
    "    response_column.append(response_utterances)\n",
    "    response_labels_column.append(response_labels)\n",
    "\n",
    "content_gab['text'] = text_column\n",
    "content_gab['hate_speech_idx'] = text_labels_column\n",
    "content_gab['response'] = response_column\n",
    "content_gab['response_labels'] = response_labels_column\n",
    "\n",
    "content_gab = content_gab.rename(columns={'hate_speech_idx': 'text_labels'})\n",
    "print(content_gab.head())\n",
    "print('- - - - ')\n",
    "print(content_gab.columns)\n",
    "\n",
    "for index, row in content_gab.iterrows():\n",
    "    if index == 1:\n",
    "        continue\n",
    "    print(row['id'], row['text'], row['text_labels'], row['response'], row['response_labels'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"1. i joined gab to remind myself how retarded jew haters are. You wouldn't be typing on your abacus without them you retard.\", '1. This is what the left is really scared of.  https://redd.it/9rfkts', '2. That literally looks like a monkey. Why are we supposed to pretend itâs a person bc itâs wearing a red hat?', '3.  Dumb Cunt', '1. It makes you an asshole.', '2. Give it to a soldier who has defend it , then try to burn it . Show me what a badass you really are. The flag is helpless to stop you or fight back . Completely unnecessary act , its proves nothing and accomplishes nothing. It only shows your a ignorant weak faggot . Like kicking a puppy ,,, it shows how weak and worthless you are', '1. So they manage to provide a whole lot of data, except a critical little nugget.  Gay men are well over half the patients who present at STI clinics, and are probably the source of the emerging antibiotic-resistant pathogens.  https://www.dailymail.co.uk/health/article-6315871/One-young-person-England-told-STI-FOUR-MINUTES.html', \"2. yup. i cant smoke in a bar because it's harmful to some faggot that's getting drunk, doing drugs & having an STD potluck w/ the boys later...... seems legit.\", '1. Hi there, i,m Keith, i hope you are doing well, i would be very honored to get to know you if you would be interested? @PugLife', '2. Fuck off']\n",
      "[1, 0, 0, 1, 0, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# get text, first label, response, first response label for each row\n",
    "texts = []\n",
    "text_labels = []\n",
    "for index, row in content_gab.iterrows():\n",
    "    for i, t in enumerate(row['text']):\n",
    "        texts.append(t)\n",
    "        text_labels.append(row['text_labels'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def generate_embeddings(sentences):\n",
    "    if isinstance(sentences, list):\n",
    "        return bert.encode(sentences, show_progress_bar=True).tolist()\n",
    "    elif isinstance(sentences, str):\n",
    "        return bert.encode([sentences], show_progress_bar=True).tolist()\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|ââââââââââ| 1056/1056 [00:10<00:00, 102.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TIME FOR TEXT EMBEDDINGS:  10.999762773513794\n",
      "\n",
      "- - - - - -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# content_gab = content_gab[:200]\n",
    "# before = time.time()\n",
    "# content_gab['text_embeddings'] = content_gab['text'].apply(generate_embeddings)\n",
    "# after_text = time.time()\n",
    "# print(content_gab.iloc[1]['text_embeddings'])\n",
    "# print('\\nTIME FOR TEXT EMBEDDINGS: ', after_text - before)\n",
    "# print('\\n- - - - - -\\n')\n",
    "# content_gab['response_embeddings'] = content_gab['response'].apply(generate_embeddings)\n",
    "# after_response = time.time()\n",
    "# print(content_gab.iloc[2]['response_embeddings'])\n",
    "# print('\\nTIME FOR RESPONSE EMBEDDINGS: ', after_response - after_text)\n",
    "# print('\\n- - - - - -\\n')\n",
    "# embed texts\n",
    "before = time.time()\n",
    "text_embeddings = generate_embeddings(texts)\n",
    "after_text = time.time()\n",
    "print('\\nTIME FOR TEXT EMBEDDINGS: ', after_text - before)\n",
    "print('\\n- - - - - -\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 27020 27020\n",
      "Test data: 6756 6756\n",
      "([-0.03443263843655586, -0.021835561841726303, 0.07337596267461777, 0.03134050965309143, -0.016652841120958328, -0.040243592113256454, 0.12395364046096802, -0.05407875403761864, 0.11736534535884857, 0.034889619797468185, 0.04762837663292885, -0.13469630479812622, 0.06200395151972771, -0.06488172709941864, 0.0009588395478203893, 0.03084147348999977, -0.0319390706717968, -0.05509456992149353, -0.09645666182041168, 0.004767337813973427, -0.010027318261563778, -0.10109984129667282, -0.028984393924474716, -0.062055639922618866, -0.10067390650510788, -0.06287593394517899, 0.020131118595600128, 0.004611480049788952, -0.045968376100063324, 0.004295308142900467, -0.006558979861438274, 0.058647893369197845, 0.014986323192715645, -0.005212636198848486, 0.005807189270853996, -0.03227309510111809, -0.006039989180862904, 0.040627527981996536, -0.0164304468780756, 0.02992333471775055, -0.0905553475022316, -0.023228392004966736, 0.03351351618766785, 0.07705956697463989, 0.018041357398033142, 0.03548351302742958, 0.07969237864017487, 0.0717390701174736, -0.16249863803386688, -0.06567282229661942, -0.08810938149690628, 0.03672642633318901, -0.10527325421571732, 0.07620197534561157, 0.007373425643891096, 0.0005725855589844286, -0.06368528306484222, -0.012272561900317669, 0.050526026636362076, 0.061160843819379807, -0.008706647902727127, -0.010748264379799366, -0.009740187786519527, 0.03334035351872444, 0.017275400459766388, 0.050342585891485214, -0.0034850703086704016, -0.0029112088959664106, -0.01924171857535839, 0.05584002658724785, 0.035904306918382645, 0.08753057569265366, -0.058950431644916534, 0.020817093551158905, -0.02321472391486168, 0.009823083877563477, 0.08750840276479721, -0.014176003634929657, -0.012425712309777737, -0.05200239643454552, -0.05026919022202492, -0.032092832028865814, 0.059452418237924576, 0.07868681848049164, 0.05118924006819725, 0.04008707404136658, -0.041123878210783005, -0.1047246977686882, -0.1911236047744751, 0.05969138815999031, -0.056895919144153595, 0.06068665534257889, 0.019800743088126183, -0.01802949048578739, 3.054088892895379e-06, -0.040018171072006226, 0.015615265816450119, -0.014814246445894241, 0.02188805118203163, 0.042755454778671265, -0.00522179389372468, 0.01030917838215828, 0.010681616142392159, 0.06997591257095337, -0.02336590178310871, -0.09685231000185013, -0.02938985265791416, -0.005260974168777466, 0.007251089904457331, -0.004259201232343912, 0.04962491989135742, -0.034729477018117905, -0.017216546460986137, -0.014359072782099247, 0.021745072677731514, -0.025757640600204468, 0.07737250626087189, 0.028635047376155853, 0.05398959293961525, -0.004835707135498524, -0.028702473267912865, 0.06133361905813217, -0.023951830342411995, -0.04239235818386078, -0.054899685084819794, -0.09824705123901367, -0.024739645421504974, -2.7841802267706736e-33, 0.04521802440285683, 0.021925440058112144, 0.03064519166946411, 0.03634929656982422, 0.014376135542988777, 0.020509440451860428, -0.07047495245933533, 0.008445306681096554, 0.028096340596675873, 0.00017067990847863257, 0.04355064034461975, 0.07823610305786133, -0.11038042604923248, 0.02014978788793087, 0.07335198670625687, 0.10846143960952759, 0.055157843977212906, 0.014830294996500015, -0.08419737964868546, 0.03188122436404228, 0.07951751351356506, 0.07712610810995102, 0.012123341672122478, -0.055037155747413635, -0.06565096974372864, -0.06430333107709885, 0.08540883660316467, 0.021079078316688538, -0.00820060446858406, -0.01667124591767788, -0.016352009028196335, -0.022423192858695984, 0.06733882427215576, 0.09588216990232468, -0.016614552587270737, -0.04007209092378616, -0.005679939407855272, -0.0593210868537426, -0.02590174786746502, 0.052829474210739136, -0.012295562773942947, 0.0008008923614397645, 0.024580566212534904, 0.10464666038751602, -0.008270437829196453, 0.02509700506925583, 0.06207534670829773, -0.00769661134108901, -0.01903718151152134, 0.05561455339193344, -0.07925817370414734, 0.060900263488292694, 0.030064743012189865, 0.03911164402961731, -0.03528113663196564, 0.022685088217258453, 0.017934545874595642, -0.06498488038778305, 0.012896433472633362, 0.02561623603105545, -0.007251144852489233, -0.018446702510118484, -0.003211061703041196, -0.038331951946020126, -0.022738592699170113, -0.021005354821681976, -0.04562236741185188, -0.07062971591949463, 0.01368451863527298, -0.013903814367949963, 0.03583940491080284, 0.05081737041473389, -0.026961782947182655, -0.015418528579175472, 0.008343655616044998, -0.03774113953113556, 0.08595289289951324, 0.03049430064857006, 0.009508360177278519, -0.03769141808152199, -0.0031287160236388445, 0.07824540883302689, -0.0767531618475914, 0.027452338486909866, -0.10996630787849426, -0.02958701364696026, 0.07649892568588257, -0.06737414002418518, 0.01556557696312666, -0.03379909321665764, -0.05719408020377159, -0.010551346465945244, -0.011366273276507854, -0.07522324472665787, -0.06345419585704803, 1.4863626481048726e-33, -0.023888424038887024, -0.0207985769957304, -0.005213231313973665, 0.08142352849245071, 0.08766429126262665, -0.04855015501379967, 0.024663472548127174, -0.005875728093087673, 0.06601478904485703, 0.03817879036068916, -0.0172264501452446, -0.05697540566325188, -0.00919315591454506, 0.059875186532735825, 0.011041805148124695, -0.06228666752576828, 0.04413599148392677, -0.00034738925751298666, 0.0006378020043484867, 0.04787612706422806, -0.014746093191206455, 0.07165438681840897, -0.01390252634882927, 0.06703414022922516, -0.08236022293567657, 0.11535999178886414, 0.07990514487028122, 0.002507508499547839, 0.0012827219907194376, 0.031950246542692184, 0.03477348014712334, 0.06888236105442047, -0.09168078005313873, 0.0019195547793060541, 0.08262952417135239, -0.04520833492279053, -0.022716650739312172, 0.016968589276075363, 0.03925751522183418, 0.04487989842891693, -0.048434529453516006, -0.030351422727108, -0.0517473965883255, 0.039437245577573776, 0.06978273391723633, -0.03125254809856415, 0.059856705367565155, -0.019484400749206543, 0.012035666033625603, -0.05389595031738281, -0.05020672455430031, 0.0036340109072625637, -0.060730885714292526, 0.01482296921312809, 0.019564012065529823, -0.10628526657819748, 0.059652917087078094, 0.093034528195858, 0.06772546470165253, 0.11805973201990128, 0.011720010079443455, 0.024817081168293953, -0.020174408331513405, 0.007344041485339403, -0.053590234369039536, -0.035113897174596786, -0.030001813545823097, 0.027965229004621506, 0.03812438249588013, -0.05351711064577103, -0.02135229855775833, -0.04658467322587967, -0.03336252644658089, -0.0138921607285738, -0.07300926744937897, -0.046916257590055466, -0.05323836952447891, -0.09190230071544647, -0.018314912915229797, 0.06441851705312729, -0.05935414880514145, -0.009245840832591057, 0.12831217050552368, 0.03013094700872898, -0.0012848546029999852, -0.011146985925734043, 0.02235053852200508, 0.06782136857509613, 0.06958065181970596, -0.03593119978904724, 0.09996647387742996, 0.024628475308418274, -0.053330421447753906, 0.0027206577360630035, -0.07543856650590897, -1.9510926207999546e-08, 0.012657422572374344, 0.05125351995229721, 0.042466647922992706, -0.040463171899318695, -0.04310125857591629, 0.04641896113753319, 0.011926078237593174, -0.06240390986204147, 0.014680241234600544, 0.0007835633587092161, -0.025638192892074585, 0.016590872779488564, -0.03825189545750618, -0.03573165461421013, -0.006657409947365522, 0.014382816851139069, -0.04418764263391495, -0.07794319838285446, 0.034005604684352875, 0.07111836969852448, -0.0014589380007237196, 0.029989905655384064, -0.09324625134468079, -0.03951430693268776, -0.06467239558696747, 0.08217480778694153, -0.00012417681864462793, -0.038949307054281235, -0.014478331431746483, 0.08226016908884048, 0.05005715414881706, 0.03639032691717148, 0.000932216236833483, 0.019156845286488533, -0.019967559725046158, -0.017045758664608, -0.025301415473222733, 0.028732260689139366, -0.02964656427502632, -0.015154621563851833, -0.028692705556750298, -0.0593634732067585, -0.016592662781476974, 0.03252880647778511, 0.015957757830619812, -0.08186101913452148, -0.03462430462241173, -0.022656304761767387, -0.04088420420885086, 0.0401172935962677, -0.04628799855709076, 0.008782063610851765, 0.010296895168721676, 0.029169943183660507, -0.008673125877976418, -0.08843038231134415, -0.023603806272149086, 0.0607488714158535, 0.0027229704428464174, 0.0262287687510252, 0.08098200708627701, -0.0033973839599639177, -0.016796685755252838, -0.07459524273872375], 0)\n",
      "([-0.06819602847099304, 0.07411690801382065, -0.054076727479696274, -0.04481951519846916, 0.04061410576105118, 0.05046503618359566, 0.013317925855517387, -0.001099849701859057, -0.038111742585897446, 0.06721091270446777, -0.0570887066423893, 0.031195584684610367, -0.016624463722109795, -0.06825833767652512, -0.07495582848787308, -0.0006868764758110046, 0.007121558301150799, -0.10555940121412277, -0.019578412175178528, 0.008670855313539505, 0.034668680280447006, 0.04453149065375328, 0.08375544846057892, -0.014136044308543205, 0.02031189575791359, -0.04237334057688713, 0.02095438912510872, -0.028160858899354935, -0.0179612934589386, 0.08672895282506943, 0.02893909439444542, 0.008081257343292236, 0.01462748646736145, 0.06081126630306244, 0.03087909333407879, -0.08572059124708176, 0.10280554741621017, -0.05518491193652153, 0.02351435460150242, -0.011062480509281158, -0.008203312754631042, -0.09240960329771042, -0.03901553899049759, -0.012781591154634953, 0.04262301325798035, -0.02928275242447853, -0.012940619140863419, -0.015125147067010403, -0.014919635839760303, -0.010593022219836712, 0.05759621784090996, 0.0450129397213459, -0.04050450399518013, 0.0810791552066803, -0.01847279816865921, -0.03366643935441971, -0.07640353590250015, -0.07000076770782471, 0.020554279908537865, -0.04556352645158768, -0.04657507315278053, -0.004885649308562279, -0.032395292073488235, 0.054711319506168365, -0.027070315554738045, -0.04449110105633736, 0.030604751780629158, -0.038728903979063034, -0.05522735416889191, 0.05429369956254959, -0.0020325519144535065, 0.024795599281787872, -0.0318957194685936, -0.0180889293551445, -0.027238138020038605, -0.03527254983782768, 0.04430309683084488, -0.01134425774216652, 0.003296223469078541, -0.05475667119026184, -0.05345994606614113, 0.02427583932876587, 0.0020524796564131975, -0.004462386481463909, -0.042535729706287384, -0.08039401471614838, 0.004327008966356516, 0.0849854126572609, 0.008903982117772102, 0.017383182421326637, 0.003967405762523413, 0.033453185111284256, -0.025335783138871193, 0.03881128504872322, 0.010927366092801094, -0.023993147537112236, 0.009332047775387764, 0.06248883903026581, -0.10180172324180603, -0.0016808761283755302, -0.04645204544067383, -0.013317647390067577, 0.021764295175671577, 0.033287398517131805, -0.033040981739759445, 0.02033526822924614, -0.024232279509305954, 0.011445587500929832, -0.002073204144835472, -0.09563414752483368, -0.04689808189868927, -0.03561786562204361, -0.0031572638545185328, 0.06694218516349792, 0.001295409514568746, -0.04180173575878143, -0.005480200983583927, -0.009281295351684093, 0.08607950806617737, 0.08894391357898712, 0.0049400972202420235, 0.04538312181830406, -0.11634021997451782, 0.06994041800498962, -0.12093394994735718, -0.08837360143661499, -0.009365972131490707, -4.22365993573193e-33, -0.0031195126939564943, -0.06809977442026138, 0.010935216210782528, 0.08596538007259369, -0.05609171837568283, 0.09462037682533264, -0.0519958958029747, 0.017261216416954994, -0.05452999845147133, 0.06574305146932602, 0.016611022874712944, -0.08326774835586548, -0.018923863768577576, 0.039078228175640106, -0.019908158108592033, -0.000588955997955054, -0.08719906955957413, 0.0031966862734407187, 0.07071739435195923, -0.04756931960582733, -0.0018188157118856907, 0.016867533326148987, -0.020718203857541084, -0.008879901841282845, -0.04598841443657875, -0.008649139665067196, -0.015491417609155178, 0.012347117066383362, 0.08237212896347046, 0.024889126420021057, 0.03328225016593933, -0.04635583609342575, 0.046346940100193024, -0.008671155199408531, -0.06910626590251923, 0.02472042478621006, 0.15821212530136108, 0.011746224947273731, -0.04737940430641174, -0.11631982028484344, 0.03688720613718033, -0.009455343708395958, 0.04498963803052902, 0.016198135912418365, 0.06964728981256485, -0.018237700685858727, 0.08160382509231567, -0.07542217522859573, 0.043467927724123, 0.05377067252993584, -0.025782080367207527, 0.03890945762395859, 0.04810011386871338, -0.04989918693900108, -0.009349032305181026, -0.01168975792825222, -0.013758771121501923, 0.032680124044418335, -0.022943643853068352, 0.0209730826318264, 0.008676383644342422, -0.05457917973399162, 0.10823210328817368, 0.015461019240319729, 0.0043571689166128635, 0.030803756788372993, 0.10220730304718018, 0.003299952484667301, 0.009704021736979485, 0.013861656188964844, 0.03657729551196098, -0.06061916798353195, -0.12845733761787415, -0.020949671044945717, 0.020692966878414154, 0.04814576357603073, 0.11582391709089279, -0.007384708151221275, 0.09672459959983826, -0.019248725846409798, 0.0054753911681473255, 0.018355850130319595, -0.02609441801905632, 0.07227107137441635, 0.055432021617889404, 0.0013148419093340635, -0.03901715576648712, -0.07694003731012344, 0.005573217291384935, -0.03879069536924362, 0.03335457667708397, -0.08549341559410095, 0.07242230325937271, 0.0008207475766539574, 0.025629937648773193, 5.531043007842051e-34, -0.009775004349648952, 0.014980093576014042, -0.06373696774244308, 0.0027909863274544477, -0.03182534500956535, -0.04176915064454079, 0.02387036569416523, -0.0036365995183587074, -0.006016155704855919, 0.042948395013809204, 0.05418440327048302, -0.028290102258324623, 0.10803838819265366, 0.056618645787239075, 0.0006655529141426086, 0.00311132799834013, -0.0066514997743070126, 0.007816583849489689, 0.016551878303289413, 0.012291398830711842, 0.018609318882226944, 0.056218378245830536, -0.03868646174669266, 0.0030260304920375347, -0.0326906256377697, 0.04399272799491882, -0.024479921907186508, -0.005205553025007248, -0.05746375024318695, -0.05693906918168068, -0.04975438490509987, 0.005066679324954748, -0.07259304076433182, 0.047784674912691116, -0.014197212643921375, -0.04471505060791969, 0.0715317353606224, 0.054611168801784515, -0.01731717959046364, -0.015652967616915703, 0.030381932854652405, -0.021913085132837296, -0.09657963365316391, 0.04488443583250046, -0.02967749908566475, -0.008450697176158428, 0.018605053424835205, 0.03083840012550354, 0.05299851670861244, 0.07727697491645813, -0.05456412211060524, 0.06851145625114441, 0.018622510135173798, -0.04068559780716896, -0.01881285570561886, -0.05111957713961601, 0.005728293675929308, 0.018512247130274773, 0.11969629675149918, 0.07866889238357544, 0.034184835851192474, 0.11146121472120285, -0.011139512993395329, -0.06280475854873657, -0.003045587334781885, 0.048395413905382156, 0.0013394036795943975, 0.034739796072244644, -0.05235763639211655, 0.06164352595806122, 0.03453439846634865, -0.022807126864790916, -0.09452148526906967, -0.006616711616516113, -0.007949738763272762, -0.06949085742235184, 0.08046412467956543, 0.02821587398648262, -0.05009450763463974, -0.01289429422467947, -0.03581387549638748, -0.0260976143181324, 0.01628117822110653, 0.038366977125406265, 0.01277480274438858, -0.10208256542682648, 0.10771219432353973, 0.06176795810461044, 0.08278709650039673, 0.08234124630689621, 0.008005822077393532, -0.10905823856592178, -0.08095217496156693, -0.06180566921830177, -0.06337667256593704, -2.6115269946558328e-08, -0.0033732820302248, 0.09508127719163895, -0.002927088411524892, -0.037634532898664474, 0.051216647028923035, 0.06741039454936981, -0.02921788953244686, 0.01782038062810898, 0.04983704537153244, -0.008812138810753822, 0.009036430157721043, -0.007899878546595573, 0.04363773390650749, 0.060888636857271194, 0.01271704863756895, 0.046229299157857895, 0.00753874471411109, -0.07512165606021881, -0.03908245638012886, -0.013175022788345814, 0.014669684693217278, 0.10656759142875671, 0.07030025869607925, -0.0535576269030571, -0.02838819846510887, -0.022771669551730156, -0.030600404366850853, -0.01438969373703003, -0.07037011533975601, -0.06444963067770004, 0.03679444640874863, 0.021063227206468582, -0.1259000301361084, -0.05702533572912216, -0.027769794687628746, 0.0620078481733799, -0.0735907107591629, 0.04346633329987526, -0.024495376273989677, -0.11271264404058456, -0.07187552750110626, 0.04873296990990639, -0.019821809604763985, -0.035487640649080276, 0.012469563633203506, -0.03575634956359863, -0.12858538329601288, 0.018603725358843803, -0.010392880067229271, -0.006424917373806238, -0.062193118035793304, 0.018615497276186943, 0.034424763172864914, 0.015641245990991592, 0.12158685177564621, -0.1057024747133255, -0.002576845232397318, 0.018896430730819702, -0.05463138222694397, 0.05449957400560379, 0.10162745416164398, -0.0172722265124321, -0.09485642611980438, -0.03120497614145279], 0)\n"
     ]
    }
   ],
   "source": [
    "# make dataset from embeddings and labels\n",
    "class GabDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_embeddings, text_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Train data:', len(X_train), len(y_train))\n",
    "print('Test data:', len(X_test), len(y_test))\n",
    "\n",
    "# create dataset\n",
    "train_dataset = GabDataset(X_train, y_train)\n",
    "test_dataset = GabDataset(X_test, y_test)\n",
    "\n",
    "print(train_dataset[0])\n",
    "print(test_dataset[0])\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv1d(384, 16, kernel_size=(3,), stride=(1,))\n",
      "  (conv2): Conv1d(16, 2, kernel_size=(3,), stride=(1,))\n",
      "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "[tensor([ 0.0350, -0.0202,  0.0380, -0.0634, -0.1068,  0.0410, -0.0333, -0.0141,\n",
      "         0.0583, -0.0194, -0.0268, -0.0018, -0.1009, -0.0170, -0.0212, -0.0364],\n",
      "       dtype=torch.float64), tensor([-0.0286, -0.0361, -0.0023, -0.0298,  0.1116,  0.0657, -0.0559, -0.0165,\n",
      "        -0.0147,  0.0043, -0.0494, -0.0138, -0.0825,  0.0532, -0.0623,  0.1072],\n",
      "       dtype=torch.float64), tensor([ 0.0253, -0.0370, -0.0631, -0.0424, -0.0204, -0.0169, -0.0207,  0.0264,\n",
      "         0.0337, -0.0352,  0.0094,  0.0680, -0.0138, -0.0222,  0.0446,  0.0757],\n",
      "       dtype=torch.float64), tensor([ 0.0645,  0.0229, -0.0187,  0.0466,  0.0184,  0.0263,  0.0467,  0.0380,\n",
      "         0.0062,  0.0148,  0.0697, -0.0132, -0.0129,  0.0038, -0.0417, -0.0099],\n",
      "       dtype=torch.float64), tensor([ 0.1480, -0.0332,  0.0362,  0.0016, -0.0363,  0.0268,  0.0994,  0.0146,\n",
      "         0.0168, -0.0094, -0.0689,  0.0163,  0.0591, -0.0088,  0.0212,  0.0709],\n",
      "       dtype=torch.float64), tensor([ 0.0979, -0.0495,  0.0581, -0.0225, -0.0244,  0.0390,  0.0086, -0.0063,\n",
      "        -0.0047, -0.0090,  0.0238,  0.0047,  0.1034, -0.0284,  0.0658, -0.0513],\n",
      "       dtype=torch.float64), tensor([ 0.0276,  0.0442,  0.0479,  0.0698, -0.0747,  0.0913,  0.1420,  0.1057,\n",
      "         0.1196,  0.1557,  0.1392,  0.0840,  0.0766,  0.0838,  0.1452, -0.0724],\n",
      "       dtype=torch.float64), tensor([-0.0701,  0.0423,  0.0519, -0.0346, -0.0902, -0.0399, -0.0336, -0.0498,\n",
      "         0.0505, -0.0266,  0.0350,  0.0072, -0.0112, -0.0369, -0.0219, -0.0603],\n",
      "       dtype=torch.float64), tensor([ 0.0385, -0.0116,  0.0070, -0.0226, -0.0001,  0.0507, -0.0189,  0.1130,\n",
      "         0.0774,  0.0452,  0.0768,  0.0116, -0.0415, -0.0315,  0.0902,  0.1301],\n",
      "       dtype=torch.float64), tensor([ 0.0337,  0.0101, -0.0570,  0.0416,  0.0705, -0.0232,  0.0823, -0.0074,\n",
      "        -0.0526,  0.0234, -0.0275,  0.0075,  0.0064,  0.0109, -0.0187,  0.0042],\n",
      "       dtype=torch.float64), tensor([ 0.0544,  0.0150,  0.0233,  0.1240, -0.0349,  0.0008,  0.0285,  0.0395,\n",
      "         0.0018, -0.0704, -0.0026, -0.0556, -0.0325, -0.0230,  0.0017, -0.0099],\n",
      "       dtype=torch.float64), tensor([ 0.0110,  0.0285, -0.0388, -0.0061, -0.0383, -0.1153,  0.0398, -0.0767,\n",
      "        -0.0243, -0.1248, -0.0359, -0.0727, -0.0158, -0.1228, -0.0939, -0.0113],\n",
      "       dtype=torch.float64), tensor([ 0.0150,  0.0086, -0.0096,  0.0742, -0.0058, -0.0101,  0.0923, -0.0434,\n",
      "         0.0048, -0.0269,  0.0819,  0.0445,  0.0996,  0.0466,  0.0447,  0.0254],\n",
      "       dtype=torch.float64), tensor([-0.0863,  0.0487,  0.0571, -0.0270, -0.0142,  0.0094, -0.0534, -0.0456,\n",
      "        -0.1248, -0.0444,  0.0292, -0.0030, -0.0773, -0.0544, -0.0563, -0.0255],\n",
      "       dtype=torch.float64), tensor([ 0.0191, -0.0291, -0.0708, -0.1030, -0.0858, -0.0190, -0.0991, -0.0301,\n",
      "         0.0009, -0.0161, -0.0514, -0.0869, -0.0978, -0.0482, -0.0280,  0.0235],\n",
      "       dtype=torch.float64), tensor([-0.0321,  0.0757,  0.0214,  0.0403, -0.0468,  0.0183,  0.0759, -0.0063,\n",
      "         0.0247,  0.0416,  0.0575,  0.0981,  0.0224,  0.0560,  0.0327, -0.0176],\n",
      "       dtype=torch.float64), tensor([ 0.0138,  0.0155, -0.0460,  0.0663, -0.1115, -0.0050, -0.0326, -0.0327,\n",
      "         0.0444, -0.0048, -0.0281, -0.0114, -0.0079,  0.0488, -0.0115, -0.0556],\n",
      "       dtype=torch.float64), tensor([-0.0498, -0.0035, -0.0242,  0.0341, -0.0113, -0.0187,  0.0162,  0.0116,\n",
      "        -0.0805, -0.0614, -0.0979, -0.0336, -0.0834, -0.0323, -0.0824, -0.0476],\n",
      "       dtype=torch.float64), tensor([-0.0101, -0.0573, -0.1228, -0.0253,  0.0424, -0.0571, -0.0004, -0.0371,\n",
      "        -0.0614, -0.0450, -0.0365, -0.0397, -0.0028, -0.0829,  0.0335,  0.0593],\n",
      "       dtype=torch.float64), tensor([ 0.0217,  0.0925,  0.0366,  0.0218, -0.0347, -0.0211,  0.0015,  0.0620,\n",
      "        -0.0025, -0.1025, -0.0448,  0.0174, -0.0274,  0.0150,  0.0053, -0.0864],\n",
      "       dtype=torch.float64), tensor([-0.0034, -0.0363,  0.0436, -0.0120,  0.0316, -0.0689, -0.0495,  0.0478,\n",
      "         0.0687, -0.0066,  0.0966,  0.0327, -0.0420, -0.0048,  0.0702,  0.0436],\n",
      "       dtype=torch.float64), tensor([ 0.0629, -0.0026,  0.0446,  0.0239,  0.0006, -0.0280,  0.0485,  0.0138,\n",
      "         0.0413,  0.0487,  0.0685,  0.0612,  0.0369, -0.0232, -0.0294,  0.1093],\n",
      "       dtype=torch.float64), tensor([ 0.1178,  0.0409,  0.0470,  0.0431,  0.1159,  0.0508,  0.0440, -0.0216,\n",
      "         0.0527,  0.1071,  0.0239,  0.0485,  0.0240,  0.1159,  0.1106,  0.0701],\n",
      "       dtype=torch.float64), tensor([ 0.0794,  0.0925, -0.0143,  0.0198, -0.0020, -0.0105,  0.0556,  0.0479,\n",
      "         0.0091, -0.0304,  0.0464,  0.0053,  0.0805,  0.0211,  0.0224, -0.0158],\n",
      "       dtype=torch.float64), tensor([ 0.0618, -0.0356,  0.0455,  0.0826,  0.0989, -0.0123,  0.0239,  0.0313,\n",
      "        -0.0410, -0.0318,  0.0651, -0.0371,  0.0445, -0.0493,  0.0047,  0.0985],\n",
      "       dtype=torch.float64), tensor([-0.0637, -0.0454, -0.0974, -0.0104, -0.0070, -0.0065, -0.0345,  0.0845,\n",
      "         0.0268, -0.0344, -0.0104, -0.0419, -0.0482, -0.0740, -0.0111,  0.0752],\n",
      "       dtype=torch.float64), tensor([ 0.0015, -0.0559,  0.0385, -0.0126,  0.0433,  0.0320,  0.0042,  0.0281,\n",
      "         0.0008, -0.0387, -0.0539, -0.0309,  0.1105,  0.0123,  0.0269, -0.0537],\n",
      "       dtype=torch.float64), tensor([ 0.0356,  0.0439, -0.0130, -0.1101,  0.0063,  0.0593,  0.0090, -0.0280,\n",
      "         0.0194, -0.0040,  0.0842, -0.0188,  0.0247,  0.0154,  0.0356, -0.0005],\n",
      "       dtype=torch.float64), tensor([-0.0161, -0.0248,  0.0142, -0.0802, -0.0002, -0.0242, -0.0122, -0.0307,\n",
      "         0.0745,  0.0150,  0.0194, -0.0143, -0.1013, -0.0258, -0.1019,  0.0626],\n",
      "       dtype=torch.float64), tensor([ 0.0901, -0.1000,  0.0177, -0.0135, -0.0610, -0.0428,  0.0395, -0.0501,\n",
      "        -0.0132, -0.0328,  0.0374, -0.0165, -0.0563,  0.0264, -0.0117, -0.0415],\n",
      "       dtype=torch.float64), tensor([ 0.0334, -0.0298,  0.0039,  0.0263, -0.0772, -0.0790,  0.0140,  0.0029,\n",
      "         0.0392,  0.0296, -0.0169, -0.0282,  0.0914, -0.0130,  0.0037, -0.0203],\n",
      "       dtype=torch.float64), tensor([-0.0448, -0.0495,  0.0121,  0.0008,  0.0678,  0.0403, -0.0747,  0.0261,\n",
      "         0.0103, -0.0323,  0.1077,  0.0888, -0.0346, -0.0084,  0.0798,  0.0296],\n",
      "       dtype=torch.float64), tensor([-0.0842, -0.0694, -0.0809,  0.0061, -0.0016,  0.0132, -0.0344, -0.1312,\n",
      "        -0.0109,  0.0023, -0.0381, -0.0367, -0.0226,  0.0043,  0.0441, -0.0187],\n",
      "       dtype=torch.float64), tensor([ 0.0610, -0.0133,  0.0367,  0.0822, -0.0539,  0.0007,  0.0693, -0.0601,\n",
      "        -0.0212, -0.0260, -0.0268,  0.0077, -0.0320,  0.1036,  0.0192, -0.0252],\n",
      "       dtype=torch.float64), tensor([-0.0277,  0.0391, -0.0844,  0.1307,  0.0444, -0.0342, -0.0078,  0.0384,\n",
      "         0.0425, -0.0176, -0.0038, -0.0475,  0.0558, -0.0506, -0.0553,  0.0188],\n",
      "       dtype=torch.float64), tensor([-0.0197, -0.0425, -0.0822,  0.0456,  0.0645,  0.0256, -0.0067, -0.0140,\n",
      "         0.0393, -0.1391, -0.0132, -0.0361, -0.0760, -0.1104, -0.0443,  0.0105],\n",
      "       dtype=torch.float64), tensor([0.0238, 0.0614, 0.0404, 0.0996, 0.0485, 0.0667, 0.0597, 0.0725, 0.0672,\n",
      "        0.0419, 0.0126, 0.0429, 0.1063, 0.1168, 0.0282, 0.1105],\n",
      "       dtype=torch.float64), tensor([ 0.0272,  0.0027, -0.0396,  0.0548,  0.0383,  0.0643,  0.0801,  0.0191,\n",
      "         0.0954,  0.0067, -0.0252,  0.0366, -0.0490,  0.0153,  0.0771,  0.0628],\n",
      "       dtype=torch.float64), tensor([ 0.0303, -0.0286, -0.0218, -0.0290,  0.0169,  0.0211,  0.0192,  0.0182,\n",
      "         0.0111, -0.0614, -0.0114, -0.0040,  0.0792, -0.0095,  0.0156,  0.0431],\n",
      "       dtype=torch.float64), tensor([-0.0613,  0.0025,  0.0930,  0.0374, -0.0192, -0.0739,  0.0047, -0.0116,\n",
      "        -0.0649,  0.0382,  0.0163,  0.0716, -0.0294, -0.0059,  0.0585, -0.0227],\n",
      "       dtype=torch.float64), tensor([ 0.0179, -0.0294,  0.0223, -0.0620,  0.0234,  0.0651,  0.0222,  0.0563,\n",
      "        -0.0166,  0.0504,  0.0539,  0.0800,  0.0438,  0.0512,  0.0062,  0.0454],\n",
      "       dtype=torch.float64), tensor([-9.0261e-02,  7.6866e-03, -6.0275e-04, -2.4649e-02,  2.6317e-02,\n",
      "         3.4087e-02, -5.2267e-02, -9.7386e-02,  5.7365e-06, -2.2883e-02,\n",
      "        -6.7919e-02, -4.1961e-02, -5.6837e-02,  7.0779e-03, -1.2951e-01,\n",
      "         4.0709e-02], dtype=torch.float64), tensor([ 0.0271,  0.0298,  0.0017,  0.1209,  0.0506,  0.0020,  0.0565, -0.0215,\n",
      "        -0.0421,  0.0155, -0.0122, -0.0128, -0.0056, -0.0288,  0.0359, -0.0503],\n",
      "       dtype=torch.float64), tensor([-0.0387,  0.0387, -0.0033, -0.0332, -0.0605, -0.0178, -0.0441, -0.0212,\n",
      "        -0.0029,  0.0025,  0.0041, -0.0485, -0.0036,  0.0207,  0.0241, -0.0209],\n",
      "       dtype=torch.float64), tensor([ 0.0010,  0.1065,  0.0617, -0.1375, -0.0413, -0.0203,  0.0110,  0.0276,\n",
      "         0.0427,  0.0384, -0.0026,  0.0186, -0.0174,  0.0751,  0.0149,  0.0202],\n",
      "       dtype=torch.float64), tensor([ 0.0305,  0.0450, -0.0130, -0.0294,  0.0913,  0.0545, -0.0084,  0.0337,\n",
      "        -0.0459,  0.0344,  0.0854, -0.0485,  0.0103, -0.0352, -0.0148,  0.0107],\n",
      "       dtype=torch.float64), tensor([-0.0142, -0.0123, -0.0307, -0.0918, -0.0031, -0.0028,  0.0218, -0.0266,\n",
      "        -0.0208, -0.0242, -0.0557, -0.0018, -0.0367,  0.0095,  0.0302, -0.0362],\n",
      "       dtype=torch.float64), tensor([ 0.0571,  0.0006,  0.0360,  0.0127, -0.0004,  0.0649,  0.0748,  0.0240,\n",
      "        -0.0004,  0.0865,  0.1303,  0.0216, -0.0004, -0.0242,  0.0809,  0.0441],\n",
      "       dtype=torch.float64), tensor([-1.4174e-01, -5.8105e-03,  6.5926e-02,  4.2693e-02, -5.3389e-02,\n",
      "        -7.0635e-02, -2.3866e-02, -2.1805e-05, -9.7606e-02, -2.2309e-02,\n",
      "        -9.4833e-02,  7.0786e-02, -8.1187e-02, -1.5755e-02,  1.4933e-02,\n",
      "        -2.7940e-02], dtype=torch.float64), tensor([-0.0767,  0.0151,  0.0455, -0.0915,  0.0915,  0.0570, -0.0853, -0.0555,\n",
      "        -0.0792, -0.0758, -0.1159, -0.0661, -0.0435, -0.0440, -0.1067,  0.0197],\n",
      "       dtype=torch.float64), tensor([-0.0591, -0.0170, -0.0062, -0.0963, -0.0547, -0.0905, -0.0194, -0.1084,\n",
      "        -0.1571, -0.0280,  0.0034, -0.0487, -0.0361,  0.0364, -0.0139, -0.0883],\n",
      "       dtype=torch.float64), tensor([ 0.0224,  0.0507, -0.0115, -0.0797,  0.0155,  0.0536, -0.0365,  0.0255,\n",
      "        -0.0078, -0.0416,  0.0076,  0.0076, -0.0584,  0.0016,  0.0030,  0.0548],\n",
      "       dtype=torch.float64), tensor([-0.0288, -0.0313, -0.0508, -0.0329,  0.0416, -0.0106, -0.0650, -0.0887,\n",
      "        -0.1305, -0.0205,  0.0183, -0.0269, -0.0846,  0.0048, -0.1231,  0.0113],\n",
      "       dtype=torch.float64), tensor([-0.0694,  0.0433,  0.0662,  0.0611,  0.0379,  0.0018, -0.0153,  0.0641,\n",
      "        -0.0328,  0.0746,  0.1242,  0.0258,  0.0670,  0.0804,  0.0599, -0.0414],\n",
      "       dtype=torch.float64), tensor([-0.0197,  0.0446,  0.0411,  0.0202,  0.0426, -0.0042,  0.0170,  0.0717,\n",
      "         0.0017,  0.0165, -0.0102, -0.0093,  0.0967,  0.0039, -0.0208,  0.0286],\n",
      "       dtype=torch.float64), tensor([-0.0885, -0.0803, -0.0846, -0.0578, -0.1080, -0.0849, -0.0888,  0.0148,\n",
      "        -0.0138, -0.1108, -0.1582, -0.0892, -0.0460, -0.0346, -0.0849, -0.1154],\n",
      "       dtype=torch.float64), tensor([ 0.0030, -0.0723, -0.0313, -0.0050, -0.1238, -0.0622, -0.0896, -0.0338,\n",
      "        -0.0319, -0.0727, -0.0082, -0.0304, -0.0465, -0.0465, -0.1023, -0.0487],\n",
      "       dtype=torch.float64), tensor([ 0.1129,  0.0414, -0.0421,  0.0373, -0.1007,  0.0560,  0.1003,  0.0549,\n",
      "         0.0643, -0.0354, -0.0051, -0.0207,  0.0402,  0.0013,  0.0646,  0.0004],\n",
      "       dtype=torch.float64), tensor([ 0.0697,  0.0444, -0.0630, -0.0223,  0.0029, -0.0219,  0.0316,  0.0183,\n",
      "         0.0175, -0.0004,  0.0262,  0.0019,  0.0437,  0.0439, -0.0253,  0.0119],\n",
      "       dtype=torch.float64), tensor([ 0.1189,  0.0766, -0.0705,  0.0096, -0.0251,  0.0316,  0.1319,  0.0596,\n",
      "         0.0487,  0.1042,  0.0038, -0.0108,  0.0644,  0.0372,  0.0937, -0.0441],\n",
      "       dtype=torch.float64), tensor([-0.0002,  0.0941, -0.0617, -0.0269,  0.0941,  0.0087,  0.0572,  0.0627,\n",
      "         0.0247,  0.0216, -0.0189, -0.0312,  0.0150,  0.0042,  0.0217,  0.0057],\n",
      "       dtype=torch.float64), tensor([-0.0032,  0.0560,  0.0451,  0.0422, -0.0393, -0.0838, -0.0131,  0.0395,\n",
      "        -0.0773,  0.0295,  0.0709,  0.0369,  0.0291, -0.0104,  0.0523,  0.0174],\n",
      "       dtype=torch.float64), tensor([ 0.0484, -0.0501, -0.0340, -0.0239, -0.0285, -0.0075,  0.0250,  0.0048,\n",
      "         0.0150, -0.0124,  0.0082, -0.0301,  0.0484, -0.0289,  0.0518,  0.0211],\n",
      "       dtype=torch.float64), tensor([-0.0044,  0.0869,  0.0894,  0.0022, -0.0186,  0.0255,  0.0033,  0.0586,\n",
      "         0.0455, -0.0214, -0.0216,  0.0153,  0.0434,  0.0165,  0.0393,  0.0044],\n",
      "       dtype=torch.float64), tensor([-0.0258, -0.0239, -0.0090, -0.0265, -0.0350,  0.0073,  0.0245,  0.0692,\n",
      "         0.0871,  0.1157, -0.0253, -0.0326, -0.0487,  0.0442,  0.0447,  0.0727],\n",
      "       dtype=torch.float64), tensor([ 0.0364,  0.0247,  0.0168,  0.0724,  0.0515, -0.0357,  0.0269,  0.0028,\n",
      "         0.0470, -0.0186, -0.0051,  0.0779,  0.0364,  0.0629, -0.0049, -0.0914],\n",
      "       dtype=torch.float64), tensor([ 0.0311,  0.0294,  0.0016, -0.1014, -0.0183,  0.0091, -0.0017,  0.0606,\n",
      "         0.0366, -0.0154, -0.0558, -0.0539, -0.0141, -0.0326, -0.0454,  0.0361],\n",
      "       dtype=torch.float64), tensor([ 0.0282,  0.0070, -0.1095, -0.0914, -0.0586, -0.0134,  0.0053,  0.0233,\n",
      "         0.0188, -0.0734, -0.0160, -0.1052,  0.0387, -0.0322, -0.0944, -0.0671],\n",
      "       dtype=torch.float64), tensor([-0.0198,  0.0141, -0.0510,  0.1318, -0.0440, -0.1229, -0.0714, -0.0144,\n",
      "        -0.0378, -0.0153,  0.0196, -0.0429, -0.0595, -0.0272, -0.0226, -0.0642],\n",
      "       dtype=torch.float64), tensor([ 0.0734,  0.1016,  0.1079, -0.0516, -0.0452,  0.0895,  0.1188,  0.0778,\n",
      "         0.0387,  0.1594,  0.0904,  0.1535,  0.0509,  0.0398,  0.0408, -0.0134],\n",
      "       dtype=torch.float64), tensor([ 0.0154, -0.0647,  0.0821,  0.0743,  0.0276, -0.0615,  0.1061, -0.0205,\n",
      "         0.0311,  0.0050, -0.0623, -0.0316, -0.0545, -0.0146, -0.0505, -0.0193],\n",
      "       dtype=torch.float64), tensor([ 0.0136, -0.0095, -0.0354,  0.0280,  0.0551,  0.0596,  0.0180,  0.0722,\n",
      "         0.0031,  0.0181, -0.0399,  0.0314,  0.0432, -0.0377,  0.0613,  0.0190],\n",
      "       dtype=torch.float64), tensor([ 0.0126, -0.0323, -0.0351, -0.0778, -0.0218,  0.0160, -0.0080, -0.0863,\n",
      "         0.0275, -0.0532,  0.0296,  0.0253, -0.0357, -0.0139, -0.0563, -0.0132],\n",
      "       dtype=torch.float64), tensor([-0.0255,  0.0096, -0.0347,  0.0427, -0.0209, -0.0116,  0.0335, -0.0858,\n",
      "        -0.0129, -0.0251, -0.0150, -0.0485, -0.0506,  0.0322, -0.0698, -0.0047],\n",
      "       dtype=torch.float64), tensor([-0.0582,  0.0437, -0.0995,  0.0151, -0.0434, -0.0209,  0.0052,  0.0227,\n",
      "         0.0864,  0.0053, -0.0455, -0.0802, -0.0109, -0.0031, -0.0186, -0.0398],\n",
      "       dtype=torch.float64), tensor([-0.0378,  0.0033, -0.0045, -0.0439, -0.0765, -0.0962, -0.0785, -0.1332,\n",
      "        -0.0813, -0.0210, -0.0898, -0.0098, -0.0770,  0.0232, -0.0592, -0.0302],\n",
      "       dtype=torch.float64), tensor([ 0.0510, -0.0532,  0.0101,  0.0571, -0.0260,  0.0076,  0.0634, -0.0257,\n",
      "        -0.1007,  0.0996,  0.0109, -0.0313, -0.0282,  0.0300, -0.0001, -0.0111],\n",
      "       dtype=torch.float64), tensor([ 0.0328, -0.0099, -0.1051,  0.0429, -0.0601, -0.0311,  0.1088, -0.0796,\n",
      "        -0.1028,  0.0089,  0.0539, -0.1005,  0.0763, -0.0608, -0.0939, -0.1107],\n",
      "       dtype=torch.float64), tensor([ 0.0235, -0.0166, -0.0076,  0.0677, -0.1051, -0.0105,  0.0562,  0.0479,\n",
      "        -0.0226,  0.0074, -0.0168,  0.0493,  0.0735, -0.0069, -0.0012, -0.2050],\n",
      "       dtype=torch.float64), tensor([-0.0316, -0.0408,  0.0241, -0.0393, -0.0930, -0.0427, -0.0870,  0.0401,\n",
      "         0.0491, -0.0679, -0.0693, -0.0025, -0.0245, -0.0822, -0.0048, -0.0703],\n",
      "       dtype=torch.float64), tensor([-0.0196, -0.0771, -0.0017, -0.0550,  0.0343,  0.0377, -0.0608,  0.0671,\n",
      "         0.0152,  0.0036,  0.0528, -0.0274,  0.0818, -0.0142, -0.0820, -0.0116],\n",
      "       dtype=torch.float64), tensor([-0.1115, -0.0439, -0.0597, -0.0699, -0.0315, -0.0275,  0.0015, -0.0026,\n",
      "         0.0442,  0.0459, -0.0034,  0.0887,  0.0110, -0.0030, -0.0213,  0.0037],\n",
      "       dtype=torch.float64), tensor([ 0.0008, -0.0140,  0.0429,  0.0235,  0.0492,  0.0339, -0.0381, -0.0100,\n",
      "        -0.0441,  0.0938,  0.0123,  0.0323,  0.0266,  0.0623,  0.0589,  0.0559],\n",
      "       dtype=torch.float64), tensor([ 0.0228, -0.0098, -0.0132, -0.0281,  0.0297,  0.0082, -0.0595, -0.0033,\n",
      "         0.0185,  0.0595,  0.0053,  0.0317,  0.0323, -0.0367,  0.0597,  0.0208],\n",
      "       dtype=torch.float64), tensor([-0.0057,  0.0283, -0.0637, -0.0629, -0.0448,  0.0198, -0.0021, -0.0812,\n",
      "        -0.1178, -0.0108, -0.0215, -0.0837,  0.0632, -0.0397, -0.0425, -0.0786],\n",
      "       dtype=torch.float64), tensor([-0.0087,  0.0459, -0.0381,  0.0574,  0.0259,  0.0428, -0.0669,  0.0407,\n",
      "         0.0223, -0.0564, -0.0080, -0.1060, -0.0053, -0.0906, -0.0848, -0.0420],\n",
      "       dtype=torch.float64), tensor([-0.0552,  0.0602,  0.0878, -0.0850,  0.0156,  0.0067, -0.0245,  0.0055,\n",
      "         0.0256, -0.0298, -0.0279, -0.0080, -0.0048, -0.0349, -0.0816, -0.0033],\n",
      "       dtype=torch.float64), tensor([ 0.0232,  0.0127,  0.0963,  0.0094, -0.0063,  0.0565,  0.0455, -0.0196,\n",
      "         0.0259,  0.0042, -0.0021,  0.0314,  0.0183, -0.0573, -0.0624, -0.0187],\n",
      "       dtype=torch.float64), tensor([ 0.0149, -0.0357, -0.0581,  0.0531, -0.0363, -0.0284,  0.0789, -0.0124,\n",
      "         0.0409, -0.0494,  0.0231,  0.0748,  0.0205, -0.0922, -0.0256, -0.0317],\n",
      "       dtype=torch.float64), tensor([ 0.0814,  0.0442,  0.0585, -0.0834, -0.0211, -0.0121,  0.0189,  0.0101,\n",
      "         0.0235,  0.0562,  0.0005,  0.0037, -0.0228,  0.0194,  0.0059,  0.0053],\n",
      "       dtype=torch.float64), tensor([ 0.0684,  0.0182, -0.0262,  0.0156,  0.0391, -0.0121, -0.0159, -0.0448,\n",
      "        -0.0340, -0.0476, -0.0512,  0.0386,  0.0009,  0.0339, -0.0273,  0.0444],\n",
      "       dtype=torch.float64), tensor([ 0.0577,  0.1084,  0.0466,  0.0181, -0.0520,  0.0025, -0.0239, -0.0271,\n",
      "        -0.0972,  0.0839,  0.0388,  0.1195,  0.0184,  0.0513,  0.0237, -0.0660],\n",
      "       dtype=torch.float64), tensor([ 0.0399,  0.0351,  0.0273,  0.0253,  0.1092,  0.1842,  0.0994,  0.0307,\n",
      "        -0.0712,  0.0808,  0.1334,  0.0705,  0.1070,  0.0426,  0.0205,  0.0661],\n",
      "       dtype=torch.float64), tensor([ 0.0414,  0.0597,  0.0159, -0.0270, -0.0163, -0.0249, -0.0305,  0.0831,\n",
      "        -0.0081,  0.0263,  0.0203,  0.0161,  0.0963, -0.0378,  0.0132,  0.0401],\n",
      "       dtype=torch.float64), tensor([-0.0561, -0.0264, -0.0656,  0.0150,  0.0579,  0.0462, -0.0177, -0.0021,\n",
      "         0.0595, -0.0462,  0.0847, -0.0395,  0.0417,  0.0023,  0.0445,  0.1095],\n",
      "       dtype=torch.float64), tensor([-0.0850,  0.0214,  0.1144,  0.0022,  0.0177, -0.0411, -0.0423,  0.0061,\n",
      "         0.0049,  0.0377, -0.1001, -0.0341,  0.0344, -0.0137,  0.0340,  0.0608],\n",
      "       dtype=torch.float64), tensor([-0.0548, -0.1074, -0.0076, -0.0798, -0.0376, -0.0231,  0.0060, -0.0433,\n",
      "        -0.0041, -0.0554,  0.0554, -0.0467, -0.0488,  0.0179, -0.0587, -0.0353],\n",
      "       dtype=torch.float64), tensor([-0.0767,  0.0512,  0.0247,  0.0579,  0.1065,  0.0604, -0.0530, -0.0005,\n",
      "        -0.0288, -0.0019, -0.0715, -0.0439, -0.0701, -0.0097, -0.0285,  0.0065],\n",
      "       dtype=torch.float64), tensor([-0.0346, -0.1053, -0.1045, -0.0600,  0.0834, -0.0133, -0.0436, -0.0063,\n",
      "        -0.0023, -0.0476, -0.0483, -0.1055, -0.0597, -0.0173, -0.0224,  0.0896],\n",
      "       dtype=torch.float64), tensor([-0.0089,  0.0528,  0.0884,  0.0605,  0.0075,  0.0357,  0.0291,  0.0721,\n",
      "        -0.0020,  0.1222,  0.0080,  0.0713,  0.0436,  0.0780,  0.0737,  0.0733],\n",
      "       dtype=torch.float64), tensor([-0.0644, -0.0232, -0.0405, -0.0724,  0.0686, -0.0069, -0.0641, -0.0084,\n",
      "         0.0365, -0.0362, -0.0046,  0.0345, -0.0182, -0.0556,  0.0128, -0.0186],\n",
      "       dtype=torch.float64), tensor([-0.0381, -0.0250, -0.0379, -0.0192, -0.0607, -0.0309,  0.0016,  0.0335,\n",
      "         0.0054,  0.0044,  0.0332, -0.0123,  0.0132,  0.0169,  0.0236,  0.0516],\n",
      "       dtype=torch.float64), tensor([-0.0094,  0.0728, -0.0155, -0.0266, -0.0597, -0.0031,  0.0231,  0.0073,\n",
      "        -0.0262,  0.0024, -0.0581, -0.0208, -0.0673,  0.1378,  0.0561,  0.0685],\n",
      "       dtype=torch.float64), tensor([ 0.0031,  0.0572,  0.1009, -0.0070, -0.0197,  0.0559,  0.1020,  0.0094,\n",
      "         0.0100,  0.0448, -0.0529,  0.0580,  0.0270,  0.1011, -0.0297, -0.0647],\n",
      "       dtype=torch.float64), tensor([-0.0368,  0.0811,  0.0513,  0.0253, -0.0124,  0.0047,  0.0692, -0.0044,\n",
      "         0.0232, -0.0118,  0.0228, -0.0420,  0.0016,  0.0305,  0.0680, -0.0236],\n",
      "       dtype=torch.float64), tensor([-0.0693, -0.0062,  0.0318,  0.0090, -0.0044, -0.0787, -0.0658,  0.0585,\n",
      "         0.0743,  0.0175, -0.0481, -0.0025,  0.0250, -0.1035,  0.0313,  0.0092],\n",
      "       dtype=torch.float64), tensor([-0.0517, -0.0611,  0.0316, -0.0433, -0.0375, -0.0817, -0.0238,  0.0135,\n",
      "        -0.0137, -0.0228, -0.0133,  0.0081, -0.0668,  0.0363,  0.0413, -0.1338],\n",
      "       dtype=torch.float64), tensor([-0.0501, -0.0330,  0.0651,  0.0097, -0.0018,  0.0097, -0.0249, -0.0079,\n",
      "         0.0197,  0.0352,  0.0404,  0.0569,  0.0023,  0.1031,  0.0406, -0.0291],\n",
      "       dtype=torch.float64), tensor([-0.0579, -0.0143, -0.0112, -0.0317, -0.0026, -0.0678, -0.0727, -0.0224,\n",
      "        -0.0197, -0.0347,  0.0527, -0.1001, -0.0760, -0.0359, -0.0513, -0.0737],\n",
      "       dtype=torch.float64), tensor([ 0.0487,  0.0201, -0.0438, -0.0282, -0.0181, -0.0629, -0.0426, -0.0209,\n",
      "         0.0446,  0.0464, -0.0788, -0.0507,  0.0240, -0.0071, -0.0049,  0.0200],\n",
      "       dtype=torch.float64), tensor([-0.0073,  0.0044,  0.0430,  0.0534,  0.0039, -0.0678, -0.0161, -0.0421,\n",
      "        -0.0898, -0.0020, -0.0255, -0.0116,  0.0051,  0.0116, -0.0042, -0.0343],\n",
      "       dtype=torch.float64), tensor([ 0.0459,  0.0256,  0.0060, -0.0205,  0.0344, -0.0385, -0.0003, -0.0445,\n",
      "        -0.0600, -0.0182, -0.0767, -0.0028,  0.0141,  0.0295, -0.0241,  0.0092],\n",
      "       dtype=torch.float64), tensor([-0.0211,  0.0070, -0.0265, -0.0277,  0.0025, -0.0349,  0.0253, -0.0320,\n",
      "         0.0162, -0.0387,  0.0179, -0.0452, -0.0533,  0.0348,  0.0218, -0.0398],\n",
      "       dtype=torch.float64), tensor([-0.0535,  0.0233,  0.0164,  0.0781,  0.0776, -0.0276, -0.0071,  0.0051,\n",
      "        -0.0149,  0.0469, -0.0952, -0.0102, -0.0237, -0.0061, -0.0064, -0.0305],\n",
      "       dtype=torch.float64), tensor([ 0.0800,  0.0040,  0.0270,  0.0852,  0.0687,  0.0938,  0.0457,  0.0142,\n",
      "         0.0150,  0.0158, -0.0347,  0.1012,  0.0255,  0.0137,  0.0281,  0.0004],\n",
      "       dtype=torch.float64), tensor([-0.0155,  0.0289, -0.0142,  0.0184, -0.0525, -0.0927, -0.0526,  0.0942,\n",
      "         0.0552, -0.0519,  0.0148,  0.0180, -0.1037, -0.1008,  0.0148, -0.0275],\n",
      "       dtype=torch.float64), tensor([ 0.0483,  0.0344, -0.0550,  0.0489,  0.0564,  0.1278,  0.0553,  0.1002,\n",
      "        -0.0015,  0.0552, -0.0084,  0.0537,  0.0435,  0.1159,  0.1029,  0.0361],\n",
      "       dtype=torch.float64), tensor([ 0.0068,  0.0908,  0.0381,  0.0406,  0.0411, -0.0032,  0.0485,  0.0713,\n",
      "        -0.0019,  0.0420,  0.0056,  0.0313,  0.0139,  0.0603,  0.0377, -0.0336],\n",
      "       dtype=torch.float64), tensor([ 0.0377, -0.0109,  0.0212, -0.0556, -0.0397,  0.0589,  0.0058,  0.0748,\n",
      "         0.0427,  0.0289, -0.0573,  0.0722, -0.0597,  0.0525,  0.0542, -0.0507],\n",
      "       dtype=torch.float64), tensor([ 0.0551,  0.0862,  0.0133, -0.1379,  0.0041,  0.0141, -0.0221, -0.0160,\n",
      "        -0.0233,  0.0024, -0.0101, -0.0310,  0.0032, -0.0080, -0.0128,  0.0625],\n",
      "       dtype=torch.float64), tensor([-0.0445,  0.0308,  0.0558,  0.0142, -0.0504, -0.0566, -0.0401, -0.0370,\n",
      "         0.0433,  0.0162, -0.0432,  0.0461, -0.0338, -0.0407,  0.0912, -0.0777],\n",
      "       dtype=torch.float64), tensor([-0.0443, -0.0155, -0.0598,  0.0580,  0.0612,  0.0467,  0.0023,  0.0335,\n",
      "         0.0034,  0.0414,  0.0144,  0.0023,  0.0698,  0.0551, -0.0278,  0.0672],\n",
      "       dtype=torch.float64), tensor([-0.0248,  0.0198,  0.0621, -0.0579,  0.0344, -0.0554, -0.0768,  0.0709,\n",
      "         0.0215,  0.0402, -0.1072, -0.0202, -0.0817,  0.0063,  0.1345,  0.1276],\n",
      "       dtype=torch.float64), tensor([ 0.0053, -0.0645,  0.0538, -0.0391,  0.0667,  0.0432,  0.0163,  0.0487,\n",
      "         0.0141, -0.0320,  0.0297,  0.0630,  0.0046, -0.0171,  0.0372,  0.0833],\n",
      "       dtype=torch.float64), tensor([ 0.0560, -0.0649, -0.0757,  0.0336, -0.0383, -0.0697, -0.0045, -0.0186,\n",
      "        -0.0918, -0.1245,  0.0708, -0.0772,  0.1019, -0.0398, -0.0291, -0.0034],\n",
      "       dtype=torch.float64), tensor([-0.0852, -0.0725,  0.0215, -0.0849, -0.0228,  0.0346, -0.1210, -0.0816,\n",
      "        -0.0522, -0.0604, -0.1049, -0.0759, -0.0925, -0.1571, -0.0690, -0.0254],\n",
      "       dtype=torch.float64), tensor([-0.1126, -0.0901,  0.0578, -0.0356, -0.0525, -0.1035, -0.1026, -0.0056,\n",
      "        -0.0290,  0.0263, -0.1099, -0.0021, -0.0919, -0.0351, -0.0050, -0.0103],\n",
      "       dtype=torch.float64), tensor([-2.2833e-33, -3.2053e-33, -6.0974e-33, -2.1103e-33, -2.9721e-33,\n",
      "        -2.1492e-33, -2.9051e-33, -2.6819e-33, -1.4165e-34, -3.6023e-33,\n",
      "         3.8374e-33, -1.0704e-33, -1.3867e-33,  4.0921e-35, -2.5195e-33,\n",
      "         2.2226e-33], dtype=torch.float64), tensor([ 0.1015,  0.0094,  0.0071,  0.0480,  0.0674, -0.0046,  0.0241,  0.0003,\n",
      "         0.0963,  0.0076,  0.0463, -0.0336,  0.0291,  0.0428,  0.0044,  0.0331],\n",
      "       dtype=torch.float64), tensor([ 0.0049, -0.0140, -0.0894,  0.0718, -0.0138, -0.0080, -0.0894,  0.0007,\n",
      "         0.0279, -0.0052,  0.0930,  0.0118, -0.0147,  0.0004, -0.0103, -0.0122],\n",
      "       dtype=torch.float64), tensor([-0.0109,  0.0199,  0.0197,  0.0642, -0.0067,  0.0750, -0.0038,  0.0214,\n",
      "         0.0519, -0.0434, -0.0320,  0.0555,  0.0665,  0.0298,  0.0572, -0.0233],\n",
      "       dtype=torch.float64), tensor([ 0.0058,  0.0388, -0.0203,  0.0190,  0.0032, -0.0250,  0.0219,  0.1168,\n",
      "         0.0344,  0.0426,  0.0729,  0.1247, -0.0309, -0.0142,  0.0603,  0.0098],\n",
      "       dtype=torch.float64), tensor([ 0.0126,  0.0173,  0.0212, -0.0037,  0.0238, -0.0283,  0.0658,  0.0773,\n",
      "         0.0662,  0.0317,  0.0226,  0.0329, -0.0741,  0.1118,  0.0141,  0.0336],\n",
      "       dtype=torch.float64), tensor([-0.0079,  0.0112, -0.0261,  0.0354, -0.0662, -0.0205,  0.0259,  0.0236,\n",
      "        -0.0130,  0.0485,  0.0596,  0.0540,  0.0497,  0.0002,  0.0811, -0.0389],\n",
      "       dtype=torch.float64), tensor([-0.1003, -0.1178, -0.0310, -0.0174,  0.0104, -0.0590, -0.0216, -0.0949,\n",
      "        -0.0212, -0.0465,  0.0107, -0.0203, -0.0130,  0.0025, -0.0255,  0.0871],\n",
      "       dtype=torch.float64), tensor([-0.0128, -0.0224,  0.0732,  0.0394,  0.0172,  0.0720, -0.0227,  0.0228,\n",
      "        -0.0482, -0.0223, -0.0028, -0.0121, -0.0043,  0.0218, -0.0105,  0.0575],\n",
      "       dtype=torch.float64), tensor([ 0.0441,  0.0316,  0.0224,  0.0693, -0.0239,  0.0278, -0.0060,  0.0651,\n",
      "         0.0213, -0.0238,  0.0559,  0.0064,  0.0005,  0.0587,  0.0445,  0.0167],\n",
      "       dtype=torch.float64), tensor([ 0.0427,  0.0984, -0.0616,  0.0495, -0.0711, -0.0015,  0.0146, -0.0129,\n",
      "        -0.0048,  0.0615, -0.0204,  0.0202, -0.0023,  0.0262, -0.0034, -0.0718],\n",
      "       dtype=torch.float64), tensor([-0.0398, -0.0577,  0.0100, -0.0170, -0.0202,  0.0236,  0.0355,  0.0227,\n",
      "         0.0906,  0.0566,  0.0827,  0.0351, -0.0119, -0.0011,  0.0516, -0.0258],\n",
      "       dtype=torch.float64), tensor([ 0.0043, -0.0104, -0.1160,  0.0102,  0.0052, -0.0076, -0.0090,  0.0530,\n",
      "        -0.0116, -0.0300,  0.0052,  0.0690,  0.0219, -0.0402, -0.0335,  0.0528],\n",
      "       dtype=torch.float64), tensor([ 0.0677,  0.0012, -0.0316,  0.0114,  0.0213,  0.0451,  0.0278, -0.0282,\n",
      "         0.0239, -0.0029, -0.0863, -0.0383, -0.0055, -0.0901, -0.0101, -0.0172],\n",
      "       dtype=torch.float64), tensor([ 0.0148, -0.0066, -0.0266, -0.0709, -0.0175,  0.0846, -0.0018,  0.0458,\n",
      "         0.0679,  0.0195,  0.0557,  0.0039,  0.1090,  0.1273, -0.0216,  0.0560],\n",
      "       dtype=torch.float64), tensor([-0.0121,  0.1226, -0.0046, -0.0263, -0.0131,  0.0157, -0.0305,  0.1369,\n",
      "         0.0116,  0.0690, -0.0270,  0.0778,  0.0173,  0.0574,  0.0755, -0.0511],\n",
      "       dtype=torch.float64), tensor([-0.0336, -0.0691,  0.0017,  0.0808,  0.0446,  0.0468,  0.0659, -0.0196,\n",
      "         0.0523,  0.0584,  0.0172,  0.0438, -0.0051,  0.0475,  0.0273,  0.0172],\n",
      "       dtype=torch.float64), tensor([-0.0469,  0.0070,  0.0279, -0.0353, -0.0082,  0.0181, -0.0882,  0.0184,\n",
      "        -0.0315,  0.0068, -0.1310,  0.0483,  0.0191,  0.0457,  0.0551,  0.0258],\n",
      "       dtype=torch.float64), tensor([ 0.1015, -0.0255,  0.0666,  0.0096, -0.0716,  0.0291,  0.0489,  0.0107,\n",
      "         0.0213,  0.0602,  0.0450,  0.0810,  0.0694, -0.0232,  0.0751, -0.0173],\n",
      "       dtype=torch.float64), tensor([-0.0128, -0.0947,  0.0112, -0.0194,  0.0783, -0.0270, -0.0411,  0.0187,\n",
      "         0.0353, -0.0507, -0.0564, -0.0179, -0.0492, -0.0739, -0.0923, -0.0272],\n",
      "       dtype=torch.float64), tensor([-0.0671, -0.0506, -0.0714,  0.0927, -0.0690, -0.0048, -0.0413,  0.0188,\n",
      "        -0.0313, -0.0316, -0.0280, -0.0198, -0.0047, -0.0667, -0.0059, -0.0452],\n",
      "       dtype=torch.float64), tensor([ 0.0526, -0.0592, -0.0479,  0.0829,  0.0317,  0.0322,  0.0394,  0.0438,\n",
      "         0.0524,  0.0299,  0.0307,  0.0435, -0.0141, -0.0183,  0.0792, -0.0084],\n",
      "       dtype=torch.float64), tensor([ 0.0624,  0.0197, -0.0689, -0.0308, -0.0114,  0.0341,  0.1204,  0.0025,\n",
      "         0.0513,  0.0402,  0.0278, -0.0525,  0.1062,  0.0715,  0.0287, -0.0247],\n",
      "       dtype=torch.float64), tensor([-0.0113,  0.0711, -0.0420,  0.0030, -0.0300, -0.0136, -0.0274, -0.0066,\n",
      "        -0.0143, -0.0744, -0.0468, -0.0840, -0.0674,  0.0360, -0.0034,  0.0303],\n",
      "       dtype=torch.float64), tensor([ 0.0206,  0.0101,  0.0249,  0.0038,  0.0107,  0.0412,  0.0590,  0.0367,\n",
      "        -0.0460,  0.0863, -0.0048,  0.0481, -0.0510,  0.0462, -0.0079, -0.0216],\n",
      "       dtype=torch.float64), tensor([-0.0621, -0.0099, -0.0061, -0.0657, -0.0296,  0.0258, -0.0174,  0.0197,\n",
      "        -0.0059, -0.0437, -0.0648, -0.0201,  0.0521, -0.0470, -0.0095, -0.0006],\n",
      "       dtype=torch.float64), tensor([-0.0399,  0.0283,  0.0301, -0.0232,  0.0355, -0.0864, -0.0619, -0.0413,\n",
      "        -0.0220,  0.0149,  0.0461,  0.0618, -0.0007, -0.0151, -0.0027,  0.0603],\n",
      "       dtype=torch.float64), tensor([ 0.0390,  0.1073, -0.0531,  0.0080,  0.0489,  0.0237,  0.0745,  0.0523,\n",
      "         0.0261, -0.0345,  0.0271, -0.0193,  0.0222,  0.0523,  0.0767,  0.0608],\n",
      "       dtype=torch.float64), tensor([ 0.0274, -0.0588,  0.0553, -0.0696,  0.0629,  0.0837,  0.0109, -0.0564,\n",
      "        -0.0623, -0.0576,  0.0099, -0.0048, -0.0433, -0.0297,  0.0116,  0.0101],\n",
      "       dtype=torch.float64), tensor([-0.0098, -0.0435, -0.0566,  0.0886, -0.0523, -0.0012,  0.0188, -0.0311,\n",
      "        -0.0485,  0.0371, -0.0138, -0.0060, -0.0043, -0.0342,  0.0124, -0.0256],\n",
      "       dtype=torch.float64), tensor([-0.0066,  0.0193, -0.0108,  0.0231, -0.0100, -0.0136,  0.0013,  0.0187,\n",
      "         0.0068, -0.0230,  0.0299,  0.0106,  0.0202,  0.0454, -0.0410,  0.0349],\n",
      "       dtype=torch.float64), tensor([-0.0889, -0.0339, -0.0308, -0.0160, -0.0616, -0.0424, -0.0203, -0.0527,\n",
      "        -0.1184,  0.0636,  0.0375,  0.0143, -0.0460,  0.0440, -0.0101, -0.0517],\n",
      "       dtype=torch.float64), tensor([-0.0424, -0.0967,  0.0051, -0.0044, -0.0724, -0.0434,  0.0507,  0.0614,\n",
      "         0.1105,  0.0039,  0.0728,  0.0176, -0.0379,  0.0109,  0.0602, -0.0448],\n",
      "       dtype=torch.float64), tensor([ 0.0029,  0.0183,  0.0032, -0.0158, -0.0036,  0.0240, -0.0180,  0.0259,\n",
      "        -0.0220,  0.0601, -0.0147,  0.0477, -0.1459,  0.0405,  0.0065,  0.0499],\n",
      "       dtype=torch.float64), tensor([-0.0067,  0.0857,  0.0187, -0.0339,  0.0316,  0.0202,  0.0236,  0.0517,\n",
      "        -0.0358,  0.0146,  0.0185, -0.0338,  0.0352,  0.0563, -0.0163, -0.0657],\n",
      "       dtype=torch.float64), tensor([ 0.1080, -0.0713, -0.0003,  0.0875,  0.0824,  0.0958,  0.0583, -0.0366,\n",
      "        -0.0320,  0.0247, -0.0779, -0.0070, -0.0206, -0.0680,  0.0149, -0.0402],\n",
      "       dtype=torch.float64), tensor([-0.0083,  0.0030,  0.0546, -0.0154,  0.0457, -0.0085,  0.0256, -0.0519,\n",
      "         0.0140, -0.0843, -0.0610, -0.0094, -0.0488, -0.0280, -0.0127,  0.0016],\n",
      "       dtype=torch.float64), tensor([ 0.0029,  0.0982,  0.0929,  0.0481,  0.0689,  0.0547, -0.0018, -0.0275,\n",
      "        -0.0065,  0.1512,  0.0030,  0.0727, -0.0124,  0.0831,  0.0311,  0.0029],\n",
      "       dtype=torch.float64), tensor([-5.2485e-05, -6.3924e-02,  8.1211e-02, -6.7981e-02, -9.7205e-03,\n",
      "         7.3508e-02, -2.0927e-02, -2.1756e-02, -4.1326e-02, -6.6434e-03,\n",
      "        -5.2000e-02,  1.0149e-01, -3.3523e-02, -1.7658e-02, -1.2773e-01,\n",
      "        -7.4831e-02], dtype=torch.float64), tensor([-0.0304, -0.0333, -0.0008, -0.0849, -0.0493, -0.0547, -0.0289, -0.0596,\n",
      "         0.0445, -0.0255, -0.0399, -0.0351, -0.0881, -0.0801,  0.0051, -0.0322],\n",
      "       dtype=torch.float64), tensor([-0.0007,  0.0562, -0.0990,  0.0096, -0.0095, -0.1589,  0.0264, -0.1195,\n",
      "        -0.0164, -0.0077, -0.0317, -0.0646, -0.0681,  0.0157,  0.0056, -0.0873],\n",
      "       dtype=torch.float64), tensor([-0.0122, -0.0815, -0.0431, -0.0047,  0.0992, -0.0013, -0.0023, -0.0278,\n",
      "        -0.0684, -0.0391, -0.0540,  0.0142,  0.0428,  0.0288, -0.0292,  0.0004],\n",
      "       dtype=torch.float64), tensor([ 0.1149,  0.0188, -0.0377,  0.0692, -0.0362,  0.0329,  0.0547,  0.0820,\n",
      "         0.0489,  0.0377,  0.0503,  0.0582,  0.0857, -0.0154, -0.0482,  0.0101],\n",
      "       dtype=torch.float64), tensor([-0.0422,  0.0562, -0.0209,  0.0378, -0.0476, -0.0576,  0.0652, -0.0298,\n",
      "        -0.0029,  0.0616, -0.0340,  0.0457, -0.0778, -0.0018,  0.0832,  0.0380],\n",
      "       dtype=torch.float64), tensor([-0.0254,  0.0559, -0.0009,  0.0923,  0.0023,  0.0592, -0.0307, -0.0321,\n",
      "         0.0063, -0.0062, -0.0067,  0.0518,  0.0776,  0.0579,  0.0461, -0.0009],\n",
      "       dtype=torch.float64), tensor([ 0.0676,  0.0621,  0.0741, -0.0738,  0.0700,  0.0842, -0.0362,  0.0826,\n",
      "        -0.0049,  0.0233,  0.0544,  0.1004,  0.0715,  0.0159, -0.0052, -0.0330],\n",
      "       dtype=torch.float64), tensor([ 0.0353,  0.0033, -0.0515,  0.0384,  0.0597,  0.0694,  0.0105, -0.0007,\n",
      "        -0.0228,  0.0813, -0.0165,  0.0059, -0.0080,  0.0173,  0.1162,  0.0639],\n",
      "       dtype=torch.float64), tensor([-0.0034,  0.0312,  0.1259,  0.0216, -0.0126,  0.0404, -0.0434,  0.1080,\n",
      "         0.0197,  0.0342, -0.0491, -0.0462,  0.0341,  0.0781,  0.0305,  0.0154],\n",
      "       dtype=torch.float64), tensor([-0.0379,  0.0384,  0.0160,  0.0410, -0.0004, -0.0501,  0.0125,  0.0464,\n",
      "         0.0745, -0.0009, -0.0067, -0.0409,  0.0394,  0.0354,  0.0458, -0.0064],\n",
      "       dtype=torch.float64), tensor([-0.0209, -0.0733, -0.0172,  0.0637,  0.0309, -0.0176,  0.0377, -0.0459,\n",
      "        -0.1034,  0.0259,  0.0218, -0.0354,  0.0195,  0.0058, -0.0429, -0.0609],\n",
      "       dtype=torch.float64), tensor([ 0.0267,  0.0364, -0.0546,  0.0311,  0.0868,  0.1008,  0.0019,  0.0292,\n",
      "         0.0530,  0.0207,  0.0244, -0.0276, -0.0451,  0.0680,  0.0688,  0.0978],\n",
      "       dtype=torch.float64), tensor([ 0.0250, -0.0036, -0.0004, -0.0207, -0.0397, -0.0249, -0.0150, -0.0273,\n",
      "        -0.0151,  0.0162, -0.0316, -0.0100, -0.0426,  0.0316,  0.0040,  0.0217],\n",
      "       dtype=torch.float64), tensor([0.1232, 0.0840, 0.0295, 0.0808, 0.1240, 0.0861, 0.1014, 0.0332, 0.1069,\n",
      "        0.0494, 0.1075, 0.0934, 0.0695, 0.0127, 0.0119, 0.0598],\n",
      "       dtype=torch.float64), tensor([ 0.0192,  0.0958,  0.0516, -0.0511, -0.0095,  0.0220,  0.0251,  0.0572,\n",
      "         0.0102,  0.0578,  0.0910,  0.0977, -0.0027,  0.0429,  0.0924,  0.0212],\n",
      "       dtype=torch.float64), tensor([ 0.0037,  0.0244,  0.0725,  0.0084, -0.0622,  0.0547, -0.0263, -0.0948,\n",
      "        -0.0434,  0.0256,  0.0477,  0.0294,  0.0736,  0.0269,  0.0349, -0.0452],\n",
      "       dtype=torch.float64), tensor([ 0.0108, -0.0120,  0.0602, -0.0222,  0.0131, -0.0030, -0.0327, -0.0031,\n",
      "         0.0250,  0.0050, -0.0126,  0.0127, -0.0439,  0.0421,  0.0699,  0.0037],\n",
      "       dtype=torch.float64), tensor([ 0.0250, -0.0102,  0.0213,  0.0524,  0.0537,  0.0029,  0.0039, -0.0796,\n",
      "        -0.0081,  0.0171, -0.0594, -0.0904, -0.0641, -0.0337,  0.0021, -0.0048],\n",
      "       dtype=torch.float64), tensor([-0.1117,  0.0091,  0.0703,  0.0261, -0.0300, -0.0943, -0.0692, -0.0579,\n",
      "        -0.0130, -0.0207, -0.0401,  0.0058, -0.0627, -0.0322, -0.0293, -0.0715],\n",
      "       dtype=torch.float64), tensor([-0.0016,  0.0030,  0.0355,  0.0717,  0.0438,  0.1063,  0.0459, -0.0133,\n",
      "        -0.0042,  0.0169,  0.0016,  0.0523, -0.0324, -0.0707, -0.0656, -0.0572],\n",
      "       dtype=torch.float64), tensor([ 0.0811, -0.0192,  0.0229,  0.0797,  0.0233, -0.0850,  0.0292,  0.0218,\n",
      "         0.0552,  0.0915, -0.0672, -0.0219, -0.0631, -0.0072,  0.0250,  0.0228],\n",
      "       dtype=torch.float64), tensor([-3.9830e-02, -3.1537e-02,  7.9192e-02,  3.0689e-02,  1.3624e-01,\n",
      "         3.5749e-02, -4.1667e-02, -3.2437e-02, -1.7409e-02, -1.5412e-02,\n",
      "         3.6931e-02,  4.5354e-02, -1.7424e-02, -7.4797e-03, -1.0348e-04,\n",
      "         5.5334e-02], dtype=torch.float64), tensor([ 0.0876, -0.0002,  0.1110,  0.0154,  0.0751, -0.0446, -0.0503,  0.0220,\n",
      "         0.0229,  0.0139,  0.0112,  0.0858, -0.0383,  0.0461, -0.0227,  0.0208],\n",
      "       dtype=torch.float64), tensor([-0.0354,  0.0429, -0.0449, -0.0120,  0.0289,  0.0313,  0.0267, -0.0080,\n",
      "         0.0056,  0.0380,  0.0958, -0.0651,  0.0532,  0.0580, -0.0191,  0.0918],\n",
      "       dtype=torch.float64), tensor([ 0.0055, -0.0243, -0.0317, -0.0234,  0.0105, -0.0210, -0.0231,  0.0358,\n",
      "        -0.0242,  0.0192, -0.0217, -0.0016,  0.0278,  0.0085, -0.0424,  0.0597],\n",
      "       dtype=torch.float64), tensor([-0.0829, -0.0452,  0.0121, -0.0146,  0.0662,  0.0135, -0.0407,  0.0058,\n",
      "         0.0298, -0.0854, -0.0522, -0.0368, -0.0880, -0.0965, -0.0920, -0.0102],\n",
      "       dtype=torch.float64), tensor([-0.0094, -0.0598, -0.0561, -0.0200, -0.0623,  0.0679, -0.0315, -0.0313,\n",
      "         0.0282,  0.0187,  0.0092, -0.0369, -0.0215, -0.0193, -0.0989,  0.0435],\n",
      "       dtype=torch.float64), tensor([ 0.0383,  0.0386,  0.0851, -0.1600,  0.1170,  0.0866, -0.0173, -0.0042,\n",
      "         0.0139, -0.0680, -0.0725,  0.0071,  0.0543, -0.0610, -0.0026,  0.0355],\n",
      "       dtype=torch.float64), tensor([-0.0762,  0.0255,  0.0409,  0.0169,  0.0026,  0.0226, -0.0759, -0.0012,\n",
      "        -0.0435,  0.0119, -0.0250,  0.0007, -0.1231, -0.0108, -0.0206, -0.0132],\n",
      "       dtype=torch.float64), tensor([-0.0213,  0.0635, -0.0373, -0.0196,  0.0105,  0.0216, -0.0793, -0.0899,\n",
      "        -0.0803,  0.0270,  0.0701, -0.0307, -0.0355,  0.0040, -0.0025,  0.0572],\n",
      "       dtype=torch.float64), tensor([-0.0160, -0.0163,  0.0777,  0.0427, -0.0016, -0.1009,  0.0109, -0.0089,\n",
      "         0.0758,  0.0228, -0.0629, -0.0231,  0.0251, -0.0550, -0.0109, -0.0294],\n",
      "       dtype=torch.float64), tensor([ 0.0208, -0.0679, -0.0218,  0.0033, -0.0506,  0.0165,  0.0704,  0.0156,\n",
      "        -0.0094,  0.0134,  0.0909,  0.0474,  0.0517,  0.0125,  0.1226,  0.0681],\n",
      "       dtype=torch.float64), tensor([ 0.0520, -0.0875,  0.0084,  0.0347,  0.0127,  0.0427,  0.0632, -0.1066,\n",
      "        -0.0326, -0.0274,  0.0022,  0.0018,  0.0219,  0.0791,  0.0296, -0.0372],\n",
      "       dtype=torch.float64), tensor([ 0.0608, -0.0129,  0.0153,  0.0019,  0.0483,  0.0027,  0.0769,  0.0294,\n",
      "        -0.0092,  0.0165,  0.0113,  0.0891, -0.0460,  0.0107,  0.0570,  0.0105],\n",
      "       dtype=torch.float64), tensor([-0.0175, -0.0564, -0.0572, -0.0048, -0.0144, -0.0178, -0.0036,  0.0012,\n",
      "        -0.0573,  0.0011,  0.0624,  0.0383, -0.0062, -0.0846, -0.0499,  0.0142],\n",
      "       dtype=torch.float64), tensor([-0.0644,  0.0188, -0.0415,  0.0275,  0.0398,  0.0344, -0.0448, -0.0325,\n",
      "        -0.0211, -0.0449, -0.0200, -0.0706, -0.0098, -0.0396, -0.0142,  0.0628],\n",
      "       dtype=torch.float64), tensor([-0.0188, -0.0321,  0.0152, -0.0901,  0.0144, -0.0112, -0.0465, -0.0290,\n",
      "        -0.0200, -0.0530, -0.0147,  0.0362, -0.0261, -0.0080, -0.0501,  0.0427],\n",
      "       dtype=torch.float64), tensor([-0.0220,  0.0035,  0.0260, -0.0056,  0.0305,  0.0209,  0.0049, -0.0135,\n",
      "        -0.0231, -0.0678, -0.0854, -0.0536,  0.0617, -0.0036,  0.0224,  0.0120],\n",
      "       dtype=torch.float64), tensor([-0.0767, -0.0548,  0.1145,  0.0594, -0.0151, -0.0212, -0.0047, -0.0368,\n",
      "        -0.0493,  0.0262,  0.0174,  0.0240,  0.0144,  0.0538, -0.0257, -0.0823],\n",
      "       dtype=torch.float64), tensor([-0.0287, -0.0154, -0.0102, -0.0297, -0.0702, -0.0219, -0.0001, -0.0179,\n",
      "        -0.0756, -0.0253,  0.0163, -0.0396,  0.0315, -0.0820, -0.0118, -0.0145],\n",
      "       dtype=torch.float64), tensor([ 0.0880,  0.0219, -0.0287, -0.0387, -0.0664,  0.0718, -0.0581, -0.0145,\n",
      "         0.0303, -0.0196, -0.0355,  0.0312, -0.0567,  0.0096, -0.0072,  0.0467],\n",
      "       dtype=torch.float64), tensor([ 0.0001, -0.0575,  0.0694,  0.0256,  0.0045, -0.0598,  0.0209,  0.0190,\n",
      "        -0.0186,  0.0185,  0.0039, -0.0125, -0.0228, -0.0238, -0.0575, -0.0380],\n",
      "       dtype=torch.float64), tensor([ 0.0761,  0.0120,  0.0919, -0.0322,  0.0660, -0.0081,  0.0093,  0.0571,\n",
      "         0.0659,  0.0244, -0.0526,  0.0367, -0.0007,  0.0738,  0.0258,  0.0630],\n",
      "       dtype=torch.float64), tensor([-0.0333,  0.0233,  0.0514, -0.0377, -0.0388, -0.0268, -0.0626,  0.0725,\n",
      "         0.0151,  0.0521, -0.0773, -0.0180, -0.0376,  0.0568, -0.0097,  0.0034],\n",
      "       dtype=torch.float64), tensor([ 0.0612, -0.0187, -0.0745,  0.0551, -0.0682, -0.0201,  0.0871, -0.0451,\n",
      "        -0.0513, -0.0244,  0.0527, -0.0671,  0.0901, -0.0230, -0.0784,  0.0376],\n",
      "       dtype=torch.float64), tensor([-0.0324,  0.0402,  0.0209, -0.0055,  0.0849, -0.0201, -0.0513,  0.0554,\n",
      "         0.0079, -0.0526, -0.0906,  0.0075, -0.0334,  0.0036,  0.0424,  0.0040],\n",
      "       dtype=torch.float64), tensor([-0.0241,  0.0656,  0.1282, -0.0660, -0.0981, -0.0711, -0.1095,  0.1079,\n",
      "         0.0373, -0.0568, -0.0637, -0.0151,  0.0498, -0.1174, -0.0349, -0.0358],\n",
      "       dtype=torch.float64), tensor([-0.0257,  0.0071,  0.0337, -0.0111, -0.0252,  0.0378, -0.0177, -0.0235,\n",
      "        -0.0509,  0.0409, -0.0242, -0.0259, -0.0234,  0.0020, -0.0387,  0.0357],\n",
      "       dtype=torch.float64), tensor([-0.0490,  0.0570, -0.0052, -0.0238,  0.0141,  0.0657, -0.0325,  0.0405,\n",
      "         0.0780, -0.0100,  0.0786,  0.0087,  0.0600, -0.0413,  0.0551,  0.0910],\n",
      "       dtype=torch.float64), tensor([-0.0266, -0.1409, -0.0777, -0.1245, -0.0686, -0.0636, -0.1059, -0.0986,\n",
      "        -0.0205, -0.0946, -0.0624, -0.0524, -0.0767, -0.0677, -0.0811, -0.0546],\n",
      "       dtype=torch.float64), tensor([ 0.0170,  0.0279, -0.0515, -0.0127,  0.0040,  0.0732,  0.0534, -0.0008,\n",
      "        -0.0561,  0.0827,  0.0238,  0.0613,  0.0297,  0.0922,  0.0560, -0.0012],\n",
      "       dtype=torch.float64), tensor([-0.0069,  0.0763, -0.0134,  0.0663, -0.0388, -0.0091,  0.0202,  0.0942,\n",
      "         0.0246, -0.0471, -0.0450,  0.0293,  0.0357, -0.0491, -0.1307, -0.0744],\n",
      "       dtype=torch.float64), tensor([-0.0599, -0.0236,  0.0014, -0.0051,  0.0414, -0.0111, -0.0432, -0.1143,\n",
      "         0.0272, -0.0129, -0.0665, -0.0817, -0.0298,  0.0049,  0.0062, -0.0058],\n",
      "       dtype=torch.float64), tensor([-0.0187,  0.0140, -0.0003,  0.0204,  0.0009,  0.0533, -0.0170,  0.0309,\n",
      "         0.0033,  0.0210,  0.0046, -0.0509,  0.0034, -0.0160,  0.0319, -0.0125],\n",
      "       dtype=torch.float64), tensor([ 0.0609,  0.0169,  0.0804, -0.0735,  0.0050,  0.0454, -0.0069,  0.0598,\n",
      "         0.0164,  0.0146, -0.0343, -0.0044,  0.0430, -0.0068,  0.0236, -0.0183],\n",
      "       dtype=torch.float64), tensor([ 0.0276, -0.0423, -0.0766, -0.0053, -0.0594,  0.0061,  0.0151, -0.0493,\n",
      "        -0.0581, -0.0736,  0.0425,  0.0292,  0.0430, -0.0383, -0.0215, -0.0964],\n",
      "       dtype=torch.float64), tensor([-0.0444,  0.0618, -0.0529, -0.0644, -0.0603, -0.1490, -0.0900, -0.0518,\n",
      "        -0.1022, -0.1349, -0.0658, -0.0698, -0.0998, -0.0602, -0.0168, -0.0539],\n",
      "       dtype=torch.float64), tensor([ 4.2381e-35,  1.8011e-33,  5.2516e-33, -1.6007e-33,  8.3888e-34,\n",
      "         3.1367e-34,  1.1135e-33,  1.6190e-33, -6.0589e-34,  2.8881e-33,\n",
      "        -3.8804e-33,  1.4688e-33, -6.8317e-35, -1.3433e-33,  2.2124e-33,\n",
      "        -4.1167e-33], dtype=torch.float64), tensor([-0.1186, -0.0308, -0.0292, -0.0226,  0.0088, -0.0527, -0.0966, -0.0168,\n",
      "         0.0290, -0.0009, -0.0527,  0.0346, -0.0277,  0.0156, -0.0223, -0.0260],\n",
      "       dtype=torch.float64), tensor([-0.0181, -0.0202,  0.0058, -0.0465,  0.0116, -0.0184, -0.0429,  0.0276,\n",
      "         0.0641,  0.0343,  0.0486,  0.0291, -0.0001,  0.0286,  0.0467,  0.0059],\n",
      "       dtype=torch.float64), tensor([-0.0742,  0.0087, -0.0855, -0.0059, -0.0218,  0.0078,  0.0274, -0.0338,\n",
      "        -0.0806, -0.0456, -0.0314,  0.0719,  0.0141,  0.0833, -0.0270, -0.0703],\n",
      "       dtype=torch.float64), tensor([ 0.0836,  0.0586,  0.0984, -0.0592, -0.0091,  0.0155,  0.1107,  0.0273,\n",
      "         0.0858,  0.0483,  0.1189,  0.0384,  0.0680,  0.0801,  0.0564, -0.0070],\n",
      "       dtype=torch.float64), tensor([ 0.0229,  0.0479, -0.0065,  0.0318, -0.0942, -0.1359,  0.0929, -0.0202,\n",
      "         0.0159,  0.0299,  0.0151, -0.0169, -0.0013,  0.0500,  0.0552, -0.1256],\n",
      "       dtype=torch.float64), tensor([ 0.0749, -0.0528, -0.0040, -0.0828,  0.0031,  0.0784,  0.0295, -0.0400,\n",
      "        -0.0035,  0.0165, -0.0815,  0.0069,  0.0025, -0.0353, -0.0467, -0.0034],\n",
      "       dtype=torch.float64), tensor([ 4.5034e-03, -1.0848e-01,  2.6489e-02, -5.2484e-02,  5.0077e-02,\n",
      "         2.3019e-02, -3.1414e-02,  2.0302e-02,  1.3314e-05, -1.1141e-02,\n",
      "         1.6213e-02,  2.1716e-02,  1.1116e-02, -5.3878e-02, -1.1196e-02,\n",
      "        -5.4772e-02], dtype=torch.float64), tensor([-0.0455, -0.0485,  0.0503,  0.0686,  0.0376,  0.0536, -0.0837, -0.0596,\n",
      "         0.0152, -0.0095, -0.0098, -0.0075, -0.0649, -0.0221, -0.0019,  0.0484],\n",
      "       dtype=torch.float64), tensor([ 0.0400, -0.0013, -0.0229,  0.0590, -0.0199,  0.0161,  0.0613,  0.0184,\n",
      "         0.0100,  0.0183,  0.0752,  0.0394,  0.0111,  0.1384,  0.0920, -0.0114],\n",
      "       dtype=torch.float64), tensor([-0.0061, -0.0800,  0.0018, -0.0022, -0.0240,  0.0558, -0.0057, -0.0128,\n",
      "         0.0243,  0.0180,  0.0806,  0.0337, -0.0051,  0.0847,  0.0141,  0.0205],\n",
      "       dtype=torch.float64), tensor([ 0.0239,  0.0230,  0.0485,  0.0109,  0.0116,  0.0465,  0.0320, -0.0307,\n",
      "        -0.0014,  0.0332,  0.0294,  0.0254, -0.0799,  0.0420, -0.0664, -0.0302],\n",
      "       dtype=torch.float64), tensor([-0.0927, -0.0416,  0.0180, -0.0661,  0.0228, -0.0683, -0.0185, -0.0337,\n",
      "         0.0161, -0.0639,  0.0610, -0.0046, -0.0055, -0.0667, -0.0493, -0.0574],\n",
      "       dtype=torch.float64), tensor([-0.0670, -0.0521,  0.1509,  0.0302,  0.0738, -0.0109, -0.0065,  0.0291,\n",
      "         0.0279,  0.1693, -0.0250,  0.0139,  0.0564,  0.0841, -0.0285, -0.0314],\n",
      "       dtype=torch.float64), tensor([-0.0147,  0.0876,  0.0222, -0.0157, -0.0602, -0.0067,  0.0199,  0.0248,\n",
      "         0.0099,  0.0381,  0.0182, -0.0045, -0.0248, -0.0279, -0.0854, -0.0757],\n",
      "       dtype=torch.float64), tensor([-0.0180,  0.1165,  0.0618,  0.0381, -0.0291, -0.0604,  0.0296, -0.0720,\n",
      "        -0.0588,  0.0686,  0.0541,  0.0044,  0.0161,  0.1132,  0.0088, -0.0455],\n",
      "       dtype=torch.float64), tensor([-0.1116, -0.0728,  0.0003,  0.0147,  0.0251, -0.0542, -0.0652, -0.1195,\n",
      "        -0.0763, -0.0192, -0.0360, -0.0516, -0.0546,  0.0083, -0.0220, -0.0610],\n",
      "       dtype=torch.float64), tensor([ 0.0591, -0.0532,  0.0205,  0.0734,  0.0152, -0.0080,  0.0129,  0.0947,\n",
      "         0.0671, -0.0290,  0.0800,  0.0581,  0.0272,  0.0007,  0.0465,  0.0181],\n",
      "       dtype=torch.float64), tensor([-0.0615,  0.0236, -0.0004,  0.0129,  0.0055, -0.0130, -0.0649, -0.0207,\n",
      "        -0.0255, -0.0371, -0.0313,  0.0280, -0.0489,  0.0278, -0.1050, -0.0267],\n",
      "       dtype=torch.float64), tensor([-0.0683,  0.0076, -0.0394, -0.0671,  0.0214, -0.0100, -0.0533, -0.1374,\n",
      "        -0.0504, -0.0340, -0.0278, -0.0827, -0.0274, -0.0506, -0.0245, -0.0250],\n",
      "       dtype=torch.float64), tensor([ 0.0388, -0.0309,  0.0120, -0.0352,  0.0192,  0.0055,  0.0260,  0.0795,\n",
      "         0.0417,  0.0055,  0.1043, -0.0035, -0.0331,  0.0181,  0.0309,  0.0709],\n",
      "       dtype=torch.float64), tensor([ 0.0157,  0.0828,  0.0193, -0.1023, -0.0018,  0.0085, -0.0229,  0.0430,\n",
      "         0.0251, -0.0041, -0.0038,  0.0315, -0.0274,  0.0199,  0.0026,  0.0410],\n",
      "       dtype=torch.float64), tensor([-0.0178,  0.0861,  0.0554,  0.0631, -0.0066,  0.0873,  0.0758,  0.0184,\n",
      "         0.0999, -0.0744, -0.0699,  0.0381,  0.0894,  0.0230,  0.0494, -0.0124],\n",
      "       dtype=torch.float64), tensor([-0.0261, -0.0617, -0.0537, -0.0920, -0.0873, -0.0729, -0.0047, -0.0674,\n",
      "        -0.0403, -0.1036, -0.0952, -0.0027, -0.0990,  0.0161, -0.0895, -0.0484],\n",
      "       dtype=torch.float64), tensor([0.0323, 0.0450, 0.0393, 0.0005, 0.0033, 0.0335, 0.0457, 0.0527, 0.0470,\n",
      "        0.0282, 0.0623, 0.0641, 0.0549, 0.0613, 0.0800, 0.0353],\n",
      "       dtype=torch.float64), tensor([ 0.0489,  0.0053, -0.0045,  0.0392,  0.0554, -0.0423, -0.0494, -0.0308,\n",
      "        -0.0154, -0.0279, -0.0460, -0.0075, -0.0371, -0.0321, -0.0594,  0.0039],\n",
      "       dtype=torch.float64), tensor([-0.0554,  0.0440,  0.0691, -0.0058,  0.0763,  0.0596, -0.0181, -0.0010,\n",
      "         0.0253,  0.0627,  0.0314,  0.0829,  0.0442, -0.0081,  0.0468,  0.0178],\n",
      "       dtype=torch.float64), tensor([ 0.0537, -0.0282,  0.0355,  0.0334,  0.0817,  0.0999,  0.1000,  0.0157,\n",
      "        -0.0591,  0.0599,  0.1502,  0.0964,  0.1569, -0.0032,  0.0693,  0.0467],\n",
      "       dtype=torch.float64), tensor([ 0.0324, -0.0088, -0.0001,  0.0737, -0.0030, -0.0310,  0.0349,  0.0277,\n",
      "         0.0478,  0.0073, -0.0056, -0.0777,  0.0075,  0.0499, -0.0498, -0.0559],\n",
      "       dtype=torch.float64), tensor([ 0.0338,  0.0870, -0.0619, -0.0445, -0.0508,  0.0341, -0.0045, -0.0781,\n",
      "        -0.0316,  0.0131,  0.0222,  0.0579,  0.0205, -0.0293, -0.0169,  0.0016],\n",
      "       dtype=torch.float64), tensor([-0.0041, -0.0081,  0.0063,  0.0254,  0.0254, -0.0638, -0.0327,  0.0395,\n",
      "        -0.0801, -0.0172, -0.0492, -0.0513, -0.0037,  0.0286,  0.0414,  0.0298],\n",
      "       dtype=torch.float64), tensor([ 0.0448, -0.0352,  0.0844,  0.0584,  0.0253,  0.0036, -0.0176,  0.0216,\n",
      "         0.0727,  0.0131,  0.0308,  0.0155, -0.0363,  0.0494,  0.0195,  0.0044],\n",
      "       dtype=torch.float64), tensor([ 0.0008, -0.0308, -0.0840,  0.0666, -0.1127,  0.0066,  0.0438,  0.0150,\n",
      "         0.0221, -0.0164,  0.0177, -0.0704, -0.0473,  0.0528, -0.0061, -0.0446],\n",
      "       dtype=torch.float64), tensor([-0.0001,  0.0290, -0.0132,  0.0022, -0.0094,  0.0053, -0.0767, -0.0791,\n",
      "         0.0140,  0.0181, -0.0498,  0.0238, -0.0281,  0.0296,  0.0041, -0.0408],\n",
      "       dtype=torch.float64), tensor([ 0.0953,  0.0164,  0.0109,  0.0097, -0.0153, -0.0678,  0.0271, -0.0374,\n",
      "        -0.0152, -0.0497, -0.0843,  0.0458, -0.0504,  0.0176,  0.0001, -0.0494],\n",
      "       dtype=torch.float64), tensor([ 0.0398,  0.0150, -0.0074, -0.1462, -0.0708,  0.0280, -0.0099, -0.0167,\n",
      "         0.0236,  0.0516,  0.0126, -0.0109,  0.0135,  0.0871,  0.0166, -0.0232],\n",
      "       dtype=torch.float64), tensor([ 0.0009,  0.1073, -0.0234,  0.0395, -0.0235, -0.0290,  0.0300,  0.0712,\n",
      "         0.0379,  0.0002, -0.0308, -0.0471,  0.0835, -0.0573,  0.0239, -0.0444],\n",
      "       dtype=torch.float64), tensor([-0.0246,  0.0690,  0.0687,  0.0694,  0.0682, -0.0232,  0.0142,  0.0012,\n",
      "        -0.0173,  0.0651,  0.0444, -0.0473,  0.0089, -0.0662,  0.0201, -0.0537],\n",
      "       dtype=torch.float64), tensor([ 0.0316,  0.0227, -0.0708, -0.0564, -0.0163,  0.0025, -0.0562,  0.0081,\n",
      "        -0.0808,  0.0049, -0.0018,  0.0972, -0.0850,  0.1158, -0.0572,  0.0825],\n",
      "       dtype=torch.float64), tensor([-0.0443, -0.0068, -0.0469,  0.0158,  0.0073,  0.0177, -0.0171, -0.0649,\n",
      "         0.0277, -0.0198, -0.0610, -0.0753,  0.0292,  0.0044,  0.0282,  0.0084],\n",
      "       dtype=torch.float64), tensor([ 0.0358, -0.0354, -0.0323, -0.0577, -0.0722, -0.0489, -0.0033, -0.0411,\n",
      "        -0.0593, -0.0174, -0.0083, -0.0385,  0.0205, -0.0124,  0.0036, -0.0425],\n",
      "       dtype=torch.float64), tensor([ 0.0708,  0.0201,  0.0255,  0.0414, -0.0028, -0.0788,  0.1286, -0.0052,\n",
      "        -0.0332,  0.1020,  0.0631,  0.0138,  0.0218,  0.0297,  0.0232, -0.0051],\n",
      "       dtype=torch.float64), tensor([-0.0179, -0.0110, -0.0161,  0.0213,  0.0449,  0.0180,  0.0335, -0.0325,\n",
      "        -0.0080,  0.0543, -0.0500,  0.0004,  0.0361, -0.0329, -0.0571,  0.0646],\n",
      "       dtype=torch.float64), tensor([-0.0496,  0.0075, -0.0605, -0.0078, -0.0601, -0.1107,  0.0135, -0.0896,\n",
      "        -0.0591,  0.0374,  0.0062, -0.0822, -0.0279, -0.0647, -0.0295, -0.0490],\n",
      "       dtype=torch.float64), tensor([-0.0553,  0.0390,  0.0147,  0.0067, -0.0430,  0.0379, -0.0735,  0.0488,\n",
      "         0.0486,  0.0635, -0.0064, -0.0308, -0.0446,  0.0613,  0.0075, -0.0335],\n",
      "       dtype=torch.float64), tensor([-0.1032, -0.0931, -0.0584, -0.0236, -0.0547,  0.0208, -0.0396, -0.0034,\n",
      "        -0.0133, -0.0165, -0.0253, -0.0840, -0.0823,  0.0461, -0.0257, -0.0650],\n",
      "       dtype=torch.float64), tensor([ 0.0470, -0.0405, -0.0510, -0.0258,  0.0325, -0.0099,  0.0631,  0.0139,\n",
      "        -0.1037,  0.0527, -0.0029, -0.0180,  0.0331, -0.0828, -0.0156,  0.0012],\n",
      "       dtype=torch.float64), tensor([ 0.0001, -0.0496,  0.0471, -0.0412,  0.0453, -0.0582, -0.0077, -0.0420,\n",
      "        -0.0439, -0.0402, -0.0187,  0.0269, -0.0359, -0.0446,  0.0823,  0.0161],\n",
      "       dtype=torch.float64), tensor([-0.0099,  0.0330,  0.0346,  0.1065,  0.1235, -0.0301,  0.0019, -0.0077,\n",
      "         0.0085,  0.0698, -0.0607, -0.0245,  0.1155, -0.0231,  0.0020,  0.0663],\n",
      "       dtype=torch.float64), tensor([-0.0064, -0.0974, -0.0343, -0.0592, -0.0060, -0.0480,  0.0174, -0.1021,\n",
      "        -0.0200, -0.1032, -0.0103, -0.0235, -0.0093, -0.0011, -0.0176,  0.0638],\n",
      "       dtype=torch.float64), tensor([ 0.0205,  0.0212,  0.0432,  0.0004, -0.0014,  0.0412, -0.0558,  0.0549,\n",
      "         0.0891,  0.0124, -0.0212,  0.0147,  0.0139, -0.0738,  0.0127, -0.0336],\n",
      "       dtype=torch.float64), tensor([ 0.0124, -0.0570,  0.0019, -0.0711,  0.0125, -0.1308, -0.0940, -0.0530,\n",
      "        -0.1161, -0.0610,  0.0677,  0.0001, -0.0676, -0.1097, -0.1217,  0.0140],\n",
      "       dtype=torch.float64), tensor([-0.0433,  0.0184, -0.0414, -0.0144,  0.0163, -0.0590, -0.0144, -0.0343,\n",
      "        -0.0185, -0.0084,  0.0094, -0.0432,  0.0319,  0.0286, -0.0598,  0.0055],\n",
      "       dtype=torch.float64), tensor([-0.0074, -0.0350, -0.0477,  0.0236,  0.0341, -0.0814, -0.0666, -0.0466,\n",
      "        -0.0178, -0.0889,  0.0296, -0.0471,  0.0079, -0.0466, -0.0695,  0.0027],\n",
      "       dtype=torch.float64), tensor([ 0.0333, -0.0033,  0.0342,  0.0061, -0.1012, -0.0882,  0.0628, -0.0279,\n",
      "        -0.0695,  0.0327,  0.0870, -0.0043,  0.0515,  0.0056, -0.0191, -0.0895],\n",
      "       dtype=torch.float64), tensor([ 0.0021, -0.0009, -0.0431,  0.0313,  0.0490, -0.0127,  0.0008, -0.0354,\n",
      "        -0.0243, -0.0024, -0.0229, -0.0464,  0.0279, -0.0411, -0.0171,  0.0382],\n",
      "       dtype=torch.float64), tensor([-0.0340, -0.0283, -0.0391, -0.0179,  0.0113,  0.0373, -0.0395,  0.0195,\n",
      "         0.0031, -0.0320, -0.0453,  0.0267,  0.0136,  0.0165, -0.0160,  0.0404],\n",
      "       dtype=torch.float64), tensor([-0.0170, -0.0318, -0.0336, -0.0295, -0.0771,  0.0016, -0.0069, -0.0376,\n",
      "         0.0929,  0.0308, -0.0519,  0.1274, -0.0622, -0.0403, -0.0077, -0.0644],\n",
      "       dtype=torch.float64), tensor([-0.0002, -0.0572,  0.0788,  0.0083,  0.0668,  0.0099,  0.0621,  0.0005,\n",
      "        -0.0188, -0.0175,  0.0635, -0.0363,  0.0469,  0.0274, -0.0111, -0.0177],\n",
      "       dtype=torch.float64), tensor([-0.0453, -0.0888,  0.0495,  0.0757,  0.0539,  0.0325,  0.0498,  0.0435,\n",
      "         0.0724,  0.0190, -0.1074,  0.0260,  0.0814, -0.0216,  0.1054, -0.0057],\n",
      "       dtype=torch.float64), tensor([-0.0224,  0.0367, -0.0633,  0.0072,  0.0767,  0.0172,  0.0247,  0.0522,\n",
      "         0.0481,  0.0453,  0.0928,  0.0420,  0.0893,  0.0662, -0.0114,  0.0035],\n",
      "       dtype=torch.float64), tensor([-0.0396, -0.0719, -0.0486,  0.0180,  0.0167, -0.0057, -0.0999,  0.0673,\n",
      "         0.0115, -0.0405,  0.0115, -0.0416,  0.0197, -0.0429, -0.0359,  0.0027],\n",
      "       dtype=torch.float64), tensor([ 0.0076,  0.0086,  0.0294, -0.0136,  0.0191, -0.0291,  0.0280,  0.0185,\n",
      "         0.0078,  0.0113,  0.0547, -0.0084,  0.0862, -0.0570,  0.0292,  0.0342],\n",
      "       dtype=torch.float64), tensor([-0.0430, -0.0466, -0.0004, -0.0691,  0.0069,  0.0032, -0.0311, -0.0329,\n",
      "         0.0059, -0.0070, -0.0313,  0.0198,  0.0083, -0.0354, -0.0501, -0.0281],\n",
      "       dtype=torch.float64), tensor([-0.0013,  0.0319,  0.0929,  0.0676,  0.0293, -0.0253,  0.0362, -0.0012,\n",
      "        -0.0872,  0.0165,  0.0108,  0.0567, -0.0805,  0.0231, -0.0006, -0.0021],\n",
      "       dtype=torch.float64), tensor([ 0.0501,  0.0922, -0.0083,  0.0141,  0.0328,  0.0453, -0.0275,  0.0268,\n",
      "         0.0380,  0.0096,  0.0028,  0.0594,  0.0173,  0.0492, -0.0267,  0.0391],\n",
      "       dtype=torch.float64), tensor([-0.0399, -0.0140,  0.0409, -0.0369, -0.0749, -0.1156, -0.0733, -0.0306,\n",
      "        -0.0323, -0.0098, -0.1245, -0.0415, -0.0663, -0.0829, -0.0540, -0.0094],\n",
      "       dtype=torch.float64), tensor([-0.0402, -0.0251, -0.0203, -0.0403, -0.0401,  0.0163, -0.0927, -0.0154,\n",
      "        -0.0397, -0.0466, -0.0430, -0.0438, -0.0229, -0.0479, -0.0604, -0.0033],\n",
      "       dtype=torch.float64), tensor([ 0.0101, -0.0335,  0.0840, -0.0691,  0.0532,  0.0350,  0.0690,  0.1293,\n",
      "         0.1145,  0.0287,  0.0370,  0.1667,  0.0341, -0.0002,  0.0502,  0.0508],\n",
      "       dtype=torch.float64), tensor([ 0.0913, -0.1407, -0.0391, -0.0543, -0.0285,  0.0540,  0.0306, -0.0469,\n",
      "         0.0232, -0.0200,  0.0771, -0.0323,  0.0605, -0.1260, -0.0436, -0.0491],\n",
      "       dtype=torch.float64), tensor([-0.0396,  0.0188, -0.0045,  0.0168, -0.0078,  0.0207, -0.0250,  0.0250,\n",
      "        -0.0323, -0.0346,  0.0175, -0.0535, -0.0105,  0.0075, -0.0275,  0.0229],\n",
      "       dtype=torch.float64), tensor([-0.0124, -0.0118,  0.0333,  0.0820, -0.0372,  0.0259,  0.0249, -0.0198,\n",
      "         0.0416, -0.0507,  0.0371, -0.0600,  0.0364,  0.0417, -0.0568, -0.1052],\n",
      "       dtype=torch.float64), tensor([-0.0419, -0.0137, -0.0134, -0.0638, -0.0040,  0.0015, -0.0893, -0.0160,\n",
      "         0.0254,  0.0358, -0.0315,  0.0783,  0.0150, -0.0627,  0.0033, -0.0851],\n",
      "       dtype=torch.float64), tensor([ 0.0189, -0.0516, -0.0507, -0.0057, -0.0550, -0.0539,  0.0090, -0.0310,\n",
      "        -0.0325,  0.0551,  0.0048,  0.0291, -0.0184, -0.0222,  0.0079,  0.0286],\n",
      "       dtype=torch.float64), tensor([-0.0879, -0.0894, -0.1049, -0.0389, -0.0345, -0.0792, -0.1062, -0.0695,\n",
      "        -0.1114, -0.1226, -0.0181, -0.1057,  0.0056, -0.0496, -0.1020, -0.0993],\n",
      "       dtype=torch.float64), tensor([-0.0519, -0.0173, -0.0784, -0.0700,  0.0069, -0.0641, -0.0314, -0.0439,\n",
      "        -0.0222, -0.1389, -0.0395, -0.0633,  0.0148, -0.0905, -0.0492,  0.0586],\n",
      "       dtype=torch.float64), tensor([ 0.0536,  0.0393, -0.0530,  0.0122,  0.1044,  0.0245,  0.0948,  0.0287,\n",
      "         0.0048, -0.0058, -0.0080,  0.0188,  0.0560, -0.0742,  0.0740,  0.1269],\n",
      "       dtype=torch.float64), tensor([-0.0044, -0.0186, -0.0184, -0.0679, -0.0506, -0.0164, -0.0278, -0.0840,\n",
      "        -0.0045,  0.0566, -0.0101, -0.0633, -0.0280, -0.0141, -0.0173,  0.0063],\n",
      "       dtype=torch.float64), tensor([-0.0459,  0.0313,  0.0736, -0.0159, -0.0066, -0.0012, -0.0025, -0.0451,\n",
      "        -0.0037,  0.0081, -0.0169, -0.0484,  0.0571, -0.0065, -0.0345,  0.0219],\n",
      "       dtype=torch.float64), tensor([-6.9456e-02,  3.9916e-02, -9.3968e-05, -1.2286e-02, -2.5199e-02,\n",
      "        -2.7004e-02, -5.1281e-02, -2.5790e-02, -6.8372e-02,  2.4055e-02,\n",
      "        -2.1477e-02,  5.4240e-02, -1.8597e-02, -3.7962e-02,  5.1876e-02,\n",
      "        -2.1825e-02], dtype=torch.float64), tensor([ 0.0046, -0.0027, -0.0190,  0.0221, -0.0209,  0.0515, -0.0264,  0.0459,\n",
      "        -0.0806, -0.0752, -0.0422,  0.0833,  0.0499,  0.0173, -0.0087, -0.0650],\n",
      "       dtype=torch.float64), tensor([ 0.0952,  0.0028, -0.0806,  0.0060, -0.0560,  0.0083,  0.0910, -0.0269,\n",
      "        -0.0333, -0.0850,  0.0531, -0.0829,  0.0925,  0.0221,  0.0311,  0.0679],\n",
      "       dtype=torch.float64), tensor([-0.0785, -0.0163, -0.0679, -0.0387, -0.1012,  0.0302, -0.0993,  0.0104,\n",
      "         0.0150, -0.0191, -0.0333, -0.0218,  0.0188, -0.0443, -0.0620, -0.0449],\n",
      "       dtype=torch.float64), tensor([ 0.0324,  0.0614, -0.0541,  0.0039, -0.0115, -0.0259, -0.0011,  0.0183,\n",
      "         0.0402,  0.0247, -0.0209, -0.0102, -0.0569,  0.0652,  0.0287, -0.0733],\n",
      "       dtype=torch.float64), tensor([ 0.0248,  0.0062, -0.0127,  0.0318,  0.0668,  0.1464,  0.0779,  0.0055,\n",
      "         0.0170, -0.0062,  0.0269, -0.0408,  0.0273, -0.0042, -0.0200,  0.0417],\n",
      "       dtype=torch.float64), tensor([-0.0190,  0.0019, -0.0084,  0.0720,  0.1054,  0.0426,  0.0608, -0.0225,\n",
      "         0.0234,  0.0483,  0.0661, -0.0607, -0.0047, -0.0397, -0.0182,  0.1301],\n",
      "       dtype=torch.float64), tensor([ 0.0131, -0.0407, -0.0722,  0.0273,  0.0334, -0.0548,  0.0536, -0.0549,\n",
      "         0.0717, -0.0399, -0.0386, -0.0560,  0.0261, -0.0315,  0.0035, -0.0911],\n",
      "       dtype=torch.float64), tensor([ 0.0264,  0.0651,  0.0672,  0.0546,  0.0843,  0.0672,  0.0363, -0.0049,\n",
      "        -0.0540,  0.1371,  0.0127,  0.1773,  0.0891, -0.0059,  0.1067, -0.0093],\n",
      "       dtype=torch.float64), tensor([ 0.0217,  0.0151,  0.0083,  0.0569,  0.0041,  0.0620, -0.0150,  0.0384,\n",
      "        -0.0121,  0.0280,  0.0967,  0.0789,  0.0115,  0.0482,  0.0515, -0.0038],\n",
      "       dtype=torch.float64), tensor([-0.0285,  0.0099,  0.0916,  0.0501,  0.0805,  0.0975, -0.0040,  0.0634,\n",
      "         0.0366,  0.0826,  0.0034,  0.0850,  0.0948, -0.0149,  0.0036,  0.0663],\n",
      "       dtype=torch.float64), tensor([ 7.9751e-02, -9.3173e-03,  3.7351e-02, -3.4643e-03,  8.6754e-02,\n",
      "         4.9433e-02,  1.0031e-01, -1.9611e-02, -2.6238e-03, -6.2451e-05,\n",
      "         2.4477e-04,  5.6731e-02,  3.1683e-02, -2.1179e-03,  1.4760e-02,\n",
      "         2.9929e-02], dtype=torch.float64), tensor([ 0.0233,  0.1165,  0.0165, -0.0398,  0.0072,  0.0936,  0.0373, -0.0060,\n",
      "         0.0246,  0.0118,  0.0438,  0.0686, -0.0322,  0.0309,  0.0807, -0.0023],\n",
      "       dtype=torch.float64), tensor([-0.0365,  0.0553,  0.0192, -0.0208, -0.0803, -0.0706, -0.1072, -0.0302,\n",
      "         0.0173, -0.0213, -0.0032, -0.0181, -0.0591,  0.0280, -0.0085,  0.0096],\n",
      "       dtype=torch.float64), tensor([-0.0683,  0.0123,  0.0973, -0.0059, -0.0158, -0.0101, -0.0250, -0.0577,\n",
      "        -0.0813, -0.0072, -0.0277, -0.0404, -0.0774, -0.0508,  0.0051, -0.0092],\n",
      "       dtype=torch.float64), tensor([ 0.0488,  0.0921, -0.0157,  0.0014,  0.0422,  0.0173,  0.0266,  0.0323,\n",
      "         0.0490,  0.0237, -0.0136, -0.0156,  0.0079, -0.0106,  0.0051,  0.0335],\n",
      "       dtype=torch.float64), tensor([-0.0124,  0.0141, -0.0682, -0.0443,  0.0248, -0.0251,  0.0372, -0.0289,\n",
      "        -0.0014, -0.0765,  0.0329,  0.0318,  0.0038, -0.0048, -0.0276,  0.0055],\n",
      "       dtype=torch.float64), tensor([-3.3031e-08, -2.1257e-08, -1.5292e-08, -3.2814e-08, -2.2357e-08,\n",
      "        -4.2741e-08, -2.9135e-08, -1.9522e-08, -2.8070e-08, -1.4715e-08,\n",
      "        -3.8405e-08, -2.6756e-08, -4.0828e-08, -2.7474e-08, -2.4008e-08,\n",
      "        -3.9794e-08], dtype=torch.float64), tensor([-0.0146, -0.0206,  0.0074,  0.0363,  0.0369,  0.0596,  0.0413,  0.0150,\n",
      "        -0.0644,  0.0520, -0.0243, -0.0296,  0.0259,  0.0306, -0.0664, -0.0311],\n",
      "       dtype=torch.float64), tensor([-0.0944,  0.0788, -0.0091,  0.0301,  0.0184, -0.0610,  0.0104, -0.0146,\n",
      "        -0.0253, -0.0074, -0.0232, -0.0066,  0.0383,  0.0735,  0.0714,  0.0963],\n",
      "       dtype=torch.float64), tensor([-0.0965, -0.0741,  0.0620, -0.0213,  0.0116, -0.0298, -0.1049,  0.0356,\n",
      "         0.0502,  0.0070, -0.0321,  0.0055, -0.0101,  0.0104, -0.0455,  0.0498],\n",
      "       dtype=torch.float64), tensor([ 0.0214, -0.0125, -0.0513, -0.0160, -0.0818, -0.0267, -0.0293, -0.0150,\n",
      "         0.1029, -0.0088,  0.0079, -0.0523,  0.0587, -0.0195, -0.0908,  0.0278],\n",
      "       dtype=torch.float64), tensor([ 0.0727,  0.0298,  0.0029, -0.0161,  0.0470, -0.0021,  0.0315,  0.0527,\n",
      "         0.0940,  0.0340,  0.0378,  0.0030,  0.0249,  0.0467,  0.0426,  0.0887],\n",
      "       dtype=torch.float64), tensor([ 0.0932,  0.1212,  0.0230,  0.0319,  0.0493,  0.0700,  0.0388,  0.0735,\n",
      "         0.0454,  0.1762,  0.0137,  0.0441, -0.0032,  0.1417,  0.1179,  0.1065],\n",
      "       dtype=torch.float64), tensor([ 0.0817, -0.0942,  0.0047, -0.0518,  0.1011, -0.0080,  0.0037, -0.0359,\n",
      "         0.0330, -0.0779, -0.0252, -0.1016, -0.0176, -0.1115, -0.0027,  0.0388],\n",
      "       dtype=torch.float64), tensor([-0.0357,  0.0467,  0.0177,  0.0505,  0.0327, -0.0075,  0.0268,  0.0557,\n",
      "         0.0581,  0.0141,  0.0007,  0.0217, -0.0247, -0.0278,  0.0190,  0.0297],\n",
      "       dtype=torch.float64), tensor([ 0.0027, -0.0825,  0.0202, -0.0262, -0.0028, -0.0150, -0.0331, -0.0058,\n",
      "        -0.0412,  0.0466, -0.0112,  0.0643,  0.0322, -0.0080,  0.0455, -0.0567],\n",
      "       dtype=torch.float64), tensor([ 0.0188, -0.0782,  0.0713,  0.0072, -0.0338,  0.0576,  0.0755,  0.0760,\n",
      "        -0.0201,  0.0753,  0.0491, -0.0012,  0.0326,  0.0519,  0.0124, -0.0199],\n",
      "       dtype=torch.float64), tensor([ 0.0766,  0.0857,  0.0655, -0.0970,  0.0014,  0.0403, -0.0323,  0.0118,\n",
      "        -0.0058,  0.0406, -0.0025,  0.0996,  0.0937, -0.0316,  0.0438, -0.0040],\n",
      "       dtype=torch.float64), tensor([-0.0341,  0.0703, -0.0311,  0.0319,  0.0013,  0.0417, -0.0253,  0.0155,\n",
      "        -0.0669, -0.0164, -0.0574, -0.0320,  0.0023,  0.0756, -0.0080,  0.0033],\n",
      "       dtype=torch.float64), tensor([-0.0602,  0.0060, -0.0145, -0.0214, -0.0030, -0.1382, -0.0016, -0.0433,\n",
      "        -0.0829, -0.0597,  0.0711,  0.0333, -0.0020, -0.0620, -0.0289, -0.1320],\n",
      "       dtype=torch.float64), tensor([-0.0020,  0.0575,  0.0405, -0.0599,  0.0803,  0.0698, -0.0291,  0.1264,\n",
      "         0.0929,  0.0326, -0.0279, -0.0088,  0.0352, -0.0444, -0.0196, -0.0147],\n",
      "       dtype=torch.float64), tensor([-0.0843, -0.1027, -0.0161,  0.0653, -0.0023,  0.0617, -0.0486,  0.0751,\n",
      "         0.0260, -0.0265,  0.0121, -0.0285, -0.0075, -0.0931,  0.0925,  0.0061],\n",
      "       dtype=torch.float64), tensor([ 0.0191, -0.0231, -0.0679, -0.0637, -0.0414, -0.0549,  0.0596,  0.0602,\n",
      "         0.0280, -0.0053,  0.0430,  0.0628,  0.0574, -0.0489, -0.0478,  0.0314],\n",
      "       dtype=torch.float64), tensor([-0.0772, -0.0659, -0.0020,  0.0501,  0.0570, -0.0139, -0.0697, -0.0645,\n",
      "         0.0162,  0.0753, -0.0419,  0.0294, -0.0887, -0.0270,  0.0047, -0.0140],\n",
      "       dtype=torch.float64), tensor([-0.1021,  0.0267, -0.0611,  0.0659, -0.0935, -0.0356, -0.0586, -0.0236,\n",
      "        -0.0076, -0.0426,  0.0244,  0.0186, -0.0871, -0.0434,  0.0408, -0.0391],\n",
      "       dtype=torch.float64), tensor([ 0.0444, -0.0321, -0.0372, -0.0236, -0.0072,  0.0547, -0.0134, -0.0296,\n",
      "        -0.0391, -0.0062,  0.0323, -0.0088, -0.0248,  0.0096, -0.0164,  0.0234],\n",
      "       dtype=torch.float64), tensor([-0.0036, -0.0128,  0.0835, -0.0026, -0.0193, -0.0185, -0.0887, -0.0419,\n",
      "        -0.0374,  0.0027,  0.0039, -0.0640, -0.1132,  0.0188, -0.0139, -0.0262],\n",
      "       dtype=torch.float64), tensor([-0.0180, -0.0121, -0.0908,  0.0130,  0.0354,  0.0022,  0.0166,  0.0390,\n",
      "         0.0722, -0.0121,  0.0096, -0.0075,  0.0262,  0.0285,  0.0708,  0.0489],\n",
      "       dtype=torch.float64), tensor([-0.0478, -0.0235,  0.0570, -0.0154, -0.0300,  0.0239,  0.0197, -0.0094,\n",
      "         0.0462,  0.0229, -0.0176,  0.0339, -0.0257,  0.0040,  0.0218,  0.0586],\n",
      "       dtype=torch.float64), tensor([-0.0738,  0.0611,  0.0374,  0.0355, -0.0162,  0.0237,  0.0532, -0.0471,\n",
      "        -0.1155,  0.0153, -0.0585, -0.0537, -0.0391,  0.0221, -0.0301,  0.0104],\n",
      "       dtype=torch.float64), tensor([ 0.0141, -0.0436, -0.0565,  0.0206, -0.0803, -0.0301,  0.0198, -0.0247,\n",
      "        -0.0652,  0.0284, -0.0264,  0.0040,  0.0156, -0.0060, -0.0840, -0.0828],\n",
      "       dtype=torch.float64), tensor([ 0.0172, -0.0900, -0.0120, -0.0600, -0.0331,  0.0080, -0.0766, -0.0243,\n",
      "        -0.0994, -0.0061, -0.0249,  0.0100,  0.0272, -0.0161, -0.0641,  0.0164],\n",
      "       dtype=torch.float64), tensor([ 0.1111, -0.0452,  0.0244,  0.0042, -0.0176, -0.0449,  0.0868,  0.0051,\n",
      "         0.0225,  0.0435, -0.0063, -0.0023,  0.0356,  0.0239,  0.0742,  0.0510],\n",
      "       dtype=torch.float64), tensor([-0.1320, -0.0309,  0.0910,  0.0727, -0.0427, -0.0059, -0.0131,  0.0070,\n",
      "         0.0190, -0.0255,  0.0127, -0.0315, -0.0620, -0.0459,  0.0389, -0.1304],\n",
      "       dtype=torch.float64), tensor([ 0.0065,  0.0022,  0.0683,  0.0528, -0.0006, -0.0636,  0.0023,  0.0167,\n",
      "        -0.0524, -0.0031,  0.0254,  0.1088,  0.0445,  0.0478,  0.0986, -0.0314],\n",
      "       dtype=torch.float64), tensor([-0.0051,  0.0172, -0.0335, -0.0240, -0.0232, -0.0520, -0.0242, -0.0580,\n",
      "        -0.1028, -0.0543, -0.0612, -0.0875, -0.0846, -0.0249, -0.0238,  0.0012],\n",
      "       dtype=torch.float64), tensor([ 0.1217,  0.0823,  0.0042,  0.0793, -0.0672,  0.0034,  0.0793,  0.0881,\n",
      "         0.0941,  0.0604,  0.0156,  0.0288,  0.0822,  0.1150,  0.0586,  0.0474],\n",
      "       dtype=torch.float64), tensor([ 0.0035,  0.0507,  0.0156, -0.0862, -0.0466, -0.0257, -0.0588,  0.0740,\n",
      "         0.0160, -0.0142,  0.0587,  0.0399, -0.0235,  0.1084,  0.0440,  0.0609],\n",
      "       dtype=torch.float64), tensor([-0.0574, -0.0620,  0.0114,  0.0826,  0.0072,  0.0249,  0.0874, -0.0292,\n",
      "         0.0006,  0.1148,  0.0447, -0.0490, -0.0243, -0.0189, -0.0279, -0.0056],\n",
      "       dtype=torch.float64), tensor([-0.0368, -0.0221, -0.1098, -0.0074, -0.0500, -0.0673, -0.0545, -0.0682,\n",
      "        -0.0491, -0.0517, -0.0504, -0.0888, -0.0667, -0.0326, -0.0201, -0.0418],\n",
      "       dtype=torch.float64), tensor([ 0.0186, -0.0407,  0.0462, -0.0470, -0.1310,  0.0413, -0.0111,  0.0607,\n",
      "        -0.0039, -0.0557,  0.0201,  0.0203, -0.0268, -0.0223,  0.0905, -0.0091],\n",
      "       dtype=torch.float64), tensor([-0.0046, -0.0722, -0.0495,  0.0185,  0.0376, -0.0433,  0.0753, -0.0817,\n",
      "        -0.0548, -0.0190, -0.0892, -0.0299,  0.0237, -0.0098,  0.0700,  0.0444],\n",
      "       dtype=torch.float64), tensor([ 0.0454,  0.0171,  0.0266,  0.0324,  0.0371,  0.0116,  0.0098, -0.0979,\n",
      "        -0.0369, -0.0362,  0.0593,  0.0543, -0.0279, -0.0407, -0.0018, -0.0494],\n",
      "       dtype=torch.float64), tensor([ 4.8935e-03, -8.4240e-02,  6.9873e-02,  2.3069e-02,  1.1984e-02,\n",
      "        -9.0203e-02, -4.1365e-03,  5.1279e-05,  5.6153e-02,  8.6946e-03,\n",
      "        -1.9181e-02, -1.2327e-02,  2.4483e-02, -3.3414e-02, -4.7790e-02,\n",
      "        -7.2732e-02], dtype=torch.float64), tensor([ 0.0570, -0.0044,  0.0598, -0.0270, -0.0120,  0.0003,  0.0707,  0.0831,\n",
      "         0.0361,  0.0151,  0.0354, -0.0406, -0.0214, -0.0245,  0.0246,  0.0342],\n",
      "       dtype=torch.float64), tensor([-0.0254,  0.0198,  0.0101, -0.0314, -0.0141, -0.0093,  0.0651, -0.0142,\n",
      "        -0.0637,  0.0191, -0.0020, -0.0544,  0.0421,  0.0328, -0.0221, -0.0598],\n",
      "       dtype=torch.float64), tensor([ 0.0139,  0.0164,  0.0361,  0.0470, -0.0713,  0.0544,  0.0850, -0.0249,\n",
      "        -0.0552, -0.0245,  0.0073,  0.0624,  0.0236,  0.0575,  0.1127, -0.0293],\n",
      "       dtype=torch.float64), tensor([ 0.0043,  0.0667, -0.0330, -0.0488,  0.0125, -0.0499, -0.0676,  0.0484,\n",
      "         0.0181, -0.0147, -0.0281, -0.0442, -0.0185,  0.0216, -0.0706,  0.0196],\n",
      "       dtype=torch.float64), tensor([ 0.0389, -0.0599,  0.0253, -0.0194,  0.0372,  0.0057, -0.0127,  0.0732,\n",
      "         0.0214, -0.0165,  0.0322, -0.0300,  0.0574, -0.0819,  0.0238, -0.0682],\n",
      "       dtype=torch.float64), tensor([ 0.0259, -0.0135, -0.0384, -0.0480, -0.0409, -0.0107, -0.0064,  0.0133,\n",
      "         0.0272, -0.0206,  0.0218,  0.0047,  0.0307,  0.0446,  0.0233,  0.0176],\n",
      "       dtype=torch.float64), tensor([-0.0572,  0.0518, -0.0374,  0.0280,  0.0656, -0.0074, -0.0025,  0.0858,\n",
      "         0.0327, -0.0746, -0.0089,  0.0244, -0.0073,  0.0053, -0.0390, -0.0049],\n",
      "       dtype=torch.float64), tensor([ 0.0631, -0.0135, -0.0415,  0.0330,  0.0418,  0.0405,  0.0314, -0.0115,\n",
      "        -0.0156, -0.1130, -0.0275, -0.0423, -0.0821, -0.0610,  0.0057,  0.0281],\n",
      "       dtype=torch.float64), tensor([ 0.0457, -0.0172, -0.0283,  0.0037, -0.0129, -0.0241,  0.0336, -0.0454,\n",
      "        -0.0143, -0.0443,  0.0012, -0.0221, -0.0358, -0.0321, -0.0138,  0.0102],\n",
      "       dtype=torch.float64), tensor([-0.0614, -0.0254, -0.0407, -0.0168, -0.0350, -0.0503, -0.0623, -0.0164,\n",
      "        -0.0190, -0.0817, -0.1147, -0.0585,  0.0237, -0.0287, -0.0309, -0.0588],\n",
      "       dtype=torch.float64), tensor([ 0.0078,  0.0084, -0.0353, -0.1295, -0.0838, -0.0076, -0.0314,  0.0023,\n",
      "        -0.0713, -0.0813, -0.0655, -0.0641, -0.0471, -0.0009, -0.1063,  0.0348],\n",
      "       dtype=torch.float64), tensor([-0.0109, -0.0264, -0.0224,  0.0856, -0.0117, -0.0569,  0.0022, -0.0519,\n",
      "        -0.0212, -0.0343,  0.0165,  0.0102, -0.0454, -0.0756, -0.0726, -0.0413],\n",
      "       dtype=torch.float64), tensor([ 0.0249, -0.0789, -0.0590,  0.0420, -0.0305, -0.0649,  0.0061, -0.0233,\n",
      "         0.0097, -0.0325, -0.0620, -0.0843, -0.0390, -0.0261, -0.1212, -0.0558],\n",
      "       dtype=torch.float64), tensor([ 0.0092, -0.0146,  0.0439, -0.0544,  0.0638, -0.0504,  0.0404, -0.0369,\n",
      "        -0.0104, -0.0587,  0.0157, -0.0840, -0.0653,  0.0097, -0.0483,  0.0001],\n",
      "       dtype=torch.float64), tensor([ 0.0102, -0.0199,  0.0705, -0.0518,  0.0632,  0.0278, -0.0038, -0.0355,\n",
      "         0.0049, -0.0425,  0.0187,  0.0068,  0.0024, -0.0690, -0.0257, -0.0390],\n",
      "       dtype=torch.float64), tensor([-0.0124,  0.0715, -0.0487, -0.0142,  0.0137,  0.0066,  0.0261,  0.0131,\n",
      "         0.0607,  0.0478,  0.0494,  0.0228, -0.0002, -0.0609,  0.0252,  0.0614],\n",
      "       dtype=torch.float64), tensor([ 0.1258,  0.0133, -0.0108,  0.0141,  0.0876,  0.0675,  0.0020,  0.0595,\n",
      "         0.0619,  0.1137,  0.0358,  0.0348,  0.1050,  0.0988,  0.0656,  0.0510],\n",
      "       dtype=torch.float64), tensor([ 0.0676,  0.0556,  0.0515, -0.0297,  0.0435,  0.0417,  0.0172,  0.1116,\n",
      "         0.1132,  0.0814,  0.0350,  0.0760,  0.0065, -0.0062,  0.0468,  0.0692],\n",
      "       dtype=torch.float64), tensor([ 0.0722,  0.0820, -0.0434, -0.0392, -0.0590, -0.0196,  0.0119, -0.0243,\n",
      "         0.0485, -0.0735, -0.0029, -0.0441,  0.0391, -0.1365, -0.0459, -0.0828],\n",
      "       dtype=torch.float64), tensor([ 0.0361,  0.0408,  0.0114,  0.0087, -0.0653, -0.0325, -0.0257,  0.0530,\n",
      "        -0.0370, -0.0196,  0.0473,  0.0317, -0.0319,  0.0268, -0.0032, -0.0054],\n",
      "       dtype=torch.float64), tensor([-0.0256,  0.0780, -0.0139, -0.0342,  0.0183,  0.0847, -0.0192,  0.0985,\n",
      "         0.1158, -0.0072,  0.0497, -0.0479,  0.0321,  0.0402,  0.0407, -0.0410],\n",
      "       dtype=torch.float64), tensor([-0.0043, -0.0381, -0.0586, -0.0088, -0.0232, -0.0242, -0.0563, -0.1307,\n",
      "        -0.1010, -0.0501, -0.0114, -0.1260,  0.0234,  0.0534, -0.0512,  0.0219],\n",
      "       dtype=torch.float64), tensor([-0.0083,  0.0050,  0.0776, -0.0279,  0.0144,  0.0485, -0.0202,  0.0395,\n",
      "         0.0580, -0.0204, -0.0259, -0.0243,  0.0009,  0.0065,  0.0188, -0.0261],\n",
      "       dtype=torch.float64), tensor([ 0.0742, -0.0608,  0.0543,  0.0233,  0.1283,  0.1006,  0.0602,  0.1222,\n",
      "         0.1027,  0.1031,  0.1173,  0.0572,  0.0673,  0.0699, -0.0118,  0.1134],\n",
      "       dtype=torch.float64), tensor([-0.0116, -0.0835, -0.0191, -0.0386,  0.0092, -0.0025,  0.0246, -0.0140,\n",
      "         0.1364, -0.0603, -0.0002,  0.0102,  0.0020,  0.0027,  0.0229, -0.0333],\n",
      "       dtype=torch.float64), tensor([ 0.0538,  0.0119, -0.0059, -0.0152, -0.0220, -0.0133,  0.0292, -0.0916,\n",
      "        -0.1324, -0.0107,  0.0575,  0.0071,  0.0205, -0.0066,  0.0054,  0.0303],\n",
      "       dtype=torch.float64), tensor([-0.0162,  0.0138, -0.0458,  0.0167, -0.0051, -0.0408,  0.0670, -0.0643,\n",
      "        -0.0131,  0.0189,  0.0189, -0.0126,  0.0353, -0.0477, -0.0040,  0.0500],\n",
      "       dtype=torch.float64)]\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv1d() received an invalid combination of arguments - got (list, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!list of [Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%5d\u001b[39;00m\u001b[38;5;124m] loss: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m))\n\u001b[0;32m     44\u001b[0m                 running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 46\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m101\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# test model\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtest\u001b[39m(model, test_loader):\n",
      "Cell \u001b[1;32mIn[46], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels)\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     39\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[46], line 12\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool1d(x, x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv1d() received an invalid combination of arguments - got (list, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!list of [Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, "
     ]
    }
   ],
   "source": [
    "input_dim = len(train_dataset[0][0])\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, hidden_channels, kernel_size, input_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=hidden_channels, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv1d(in_channels=hidden_channels, out_channels=2, kernel_size=kernel_size)\n",
    "        self.fc1 = nn.Linear(64, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, x.size()[2])\n",
    "        x = x.view(-1, 64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = CNN(hidden_channels=16, kernel_size=3, input_dim=input_dim)\n",
    "print(model)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train model\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            print(inputs)\n",
    "            print(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "\n",
    "train(model, train_loader, criterion, optimizer, num_epochs=101)\n",
    "\n",
    "# test model\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = torch.tensor(inputs).float().transpose(1, 2)  # Transpose the input tensor\n",
    "            labels = torch.tensor(labels).long()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the test data: %d %%' % (100 * correct / total))\n",
    "\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv1d() received an invalid combination of arguments - got (GabDataset, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!GabDataset!, !Parameter!, !Parameter!, !tuple of (int,)!, !tuple of (int,)!, !tuple of (int,)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!GabDataset!, !Parameter!, !Parameter!, !tuple of (int,)!, !tuple of (int,)!, !tuple of (int,)!, !int!)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_num):\n\u001b[1;32m---> 40\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test()\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[29], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, train_dataset)\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[27], line 12\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool1d(x, x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv1d() received an invalid combination of arguments - got (GabDataset, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!GabDataset!, !Parameter!, !Parameter!, !tuple of (int,)!, !tuple of (int,)!, !tuple of (int,)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!GabDataset!, !Parameter!, !Parameter!, !tuple of (int,)!, !tuple of (int,)!, !tuple of (int,)!, !int!)\n"
     ]
    }
   ],
   "source": [
    "# Plot the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
