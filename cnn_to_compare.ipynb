{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we're creating a CNN to compare it with GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import necessary stuff for\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', 10)  # Limit number of rows displayed\n",
    "pd.set_option('display.width', 1000)  # Set max width for table\n",
    "pd.set_option('display.colheader_justify', 'center')  # Center-align column headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_chars(value):\n",
    "    if isinstance(value, str):  \n",
    "        return value.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').replace('  ', ' ').strip()\n",
    "    return value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading gab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id                                                text                        hate_speech_idx                      response                     \n",
      "0                                    1. 39869714\\r\\n  1. i joined gab to remind myself how retarded ...         [1]      [\"Using words that insult one group while defe...\n",
      "1  1. 39845588\\r\\n2. \\t39848775\\r\\n3. \\t\\t3991101...  1. This is what the left is really scared of. ...         [3]      ['You can disagree with someones opinion witho...\n",
      "2                   1. 37485560\\r\\n2. \\t37528625\\r\\n  1. It makes you an asshole.\\r\\n2. \\tGive it to...         [2]      ['Your argument is more rational if you leave ...\n",
      "3                   1. 39787626\\r\\n2. \\t39794481\\r\\n  1. So they manage to provide a whole lot of da...         [2]      [\"You shouldn't generalize a specific group or...\n",
      "4  1. 37957930\\r\\n2. \\t39953348\\r\\n3. \\t\\t3996521...  1. Hi there, i,m Keith, i hope you are doing w...         [3]      ['If someone is rude it is better to ignore th...\n",
      "5                                    1. 38462712\\r\\n                    1. you sound like a faggot \\r\\n         [1]      [\"Please be careful with the words you choose ...\n",
      "6  1. 38052531\\r\\n2. \\t38103723\\r\\n3. \\t\\t3851658...  1. Hi developers, give us a follow for updates...         [3]      [\"The words you've chosen are hateful and dero...\n",
      "7                   1. 38352488\\r\\n2. \\t38373190\\r\\n  1. Well, you are the fuckers that lit the matc...         [2]      ['Please refrain from using such horrible bigo...\n",
      "8  1. 37238116\\r\\n2. \\t38348543\\r\\n3. \\t\\t3837623...  1. SELF-HATING WHITE CUCKS ON PARADE\\r\\n2. \\tD...      [1, 3]      ['Your words are derogatory and offensive, and...\n",
      "9  1. 37358018\\r\\n2. \\t37359176\\r\\n3. \\t\\t3738104...  1. So after 6 years and nearly 11K followers, ...         [3]      [\"Woah! Please don't use such strong and offen...\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "Index(['id', 'text', 'hate_speech_idx', 'response'], dtype='object')\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "1. 39845588\n",
      "2. \t39848775\n",
      "3. \t\t39911017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_gab = pd.read_csv('gab_reddit_benchmark/gab.csv')\n",
    "\n",
    "content_gab[\"text\"] = content_gab[\"text\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "content_gab[\"response\"] = content_gab[\"response\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\")\n",
    "content_gab[\"hate_speech_idx\"] = content_gab[\"hate_speech_idx\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "\n",
    "# content_gab[\"text\"] = content_gab[\"text\"].apply(clean_special_chars)\n",
    "# content_gab[\"response\"] = content_gab[\"response\"].apply(clean_special_chars)\n",
    "\n",
    "for index, row in content_gab.iterrows():\n",
    "    row['text'] = row['text'].replace(\"'\", '\"')\n",
    "    row['response'] = row['response'].replace(\"'\", '\"')\n",
    "\n",
    "# content_gab = content_gab.applymap(clean_special_chars)\n",
    "print(content_gab.head(n=10))\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab.columns)\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab.iloc[1]['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wrobl\\AppData\\Local\\Temp\\ipykernel_13228\\2213304540.py:30: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace('] [', ', ') if isinstance(x, str) else x)\n",
      "C:\\Users\\Wrobl\\AppData\\Local\\Temp\\ipykernel_13228\\2213304540.py:31: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace(']  [', ', ') if isinstance(x, str) else x)\n",
      "C:\\Users\\Wrobl\\AppData\\Local\\Temp\\ipykernel_13228\\2213304540.py:32: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: 'n/a' if isinstance(x, str) and x.strip() == '' else x)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_first_number(input_string):\n",
    "    match = re.search(r'\\d{2,}', input_string)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None\n",
    "\n",
    "content_gab['extracted_id'] = content_gab['id'].apply(get_first_number)\n",
    "\n",
    "# Find duplicate rows based on 'extracted_id'\n",
    "duplicates = content_gab[content_gab.duplicated(subset=['extracted_id'], keep=False)]\n",
    "filtered_groups = []\n",
    "grouped = content_gab.groupby('extracted_id')\n",
    "for key, group in grouped:\n",
    "    if len(group) > 1:\n",
    "        filtered_groups.append(group)\n",
    "\n",
    "merged_df = pd.concat(filtered_groups, ignore_index=True)\n",
    "\n",
    "merged_df.to_csv('gab_reddit_benchmark/gab_groups.csv', index=False)\n",
    "\n",
    "merged_df = grouped.agg({\n",
    "    'id': ' '.join,\n",
    "    'text': ' '.join,\n",
    "    'hate_speech_idx': ' '.join,\n",
    "    'response': ' '.join\n",
    "}).reset_index()\n",
    "merged_df.to_csv('gab_reddit_benchmark/gab_merged.csv')\n",
    "\n",
    "df = pd.read_csv('gab_reddit_benchmark/gab_merged.csv')\n",
    "df = df.applymap(lambda x: x.replace('] [', ', ') if isinstance(x, str) else x)\n",
    "df = df.applymap(lambda x: x.replace(']  [', ', ') if isinstance(x, str) else x)\n",
    "df = df.applymap(lambda x: 'n/a' if isinstance(x, str) and x.strip() == '' else x)\n",
    "df.to_csv('gab_reddit_benchmark/gab_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id                                                text                        hate_speech_idx                      response                     \n",
      "0  1. 37011807\\r\\n2. \\t37012801\\r\\n3. \\t\\t3701333...  1. 64% of Hispanics have IQ scores too low to ...     [2, 2, 3]    ['Racist and hate speech will not be tolerated...\n",
      "1                    1. 37012081\\r\\n 1. 37012081\\r\\n  1. LYING CUNT BITCH !!\\r\\n 1. LYING CUNT BITCH...        [1, 1]    ['Please refrain from using derogatory terms f...\n",
      "2                                    1. 37012105\\r\\n  1. Nigger Kills 2 cops https://www.google.com/...           [1]    ['The N word is highly offensive.   All ethnic...\n",
      "3                   1. 37012146\\r\\n2. \\t37013020\\r\\n  1. STABBED IN NECK   arrested but no descr   #...           [2]    ['Use of the n-word is unacceptable in our dis...\n",
      "4  1. 37012285\\r\\n2. \\t37013809\\r\\n3. \\t37013868\\r\\n  1. Retard 1st ClassÂ  w/ Cluster & Ribbon\\r\\n2....           [1]    ['We should try and not use words making fun o...\n",
      "5                                    1. 37012571\\r\\n  1. We should be happy they do fuck their cousi...           [1]    ['I cannot listen to your argument if you keep...\n",
      "6  1. 37012882\\r\\n2. \\t37013415\\r\\n3. \\t\\t3701686...  1. Stolen Valor! Prosecute the scum.\\r\\n2. \\t\\...        [3, 4]    ['Stick to your own beliefs without dismissing...\n",
      "7                                    1. 37013030\\r\\n  1. Yea, here's the scoop.  Fake News! \"We told...                                                                   \n",
      "8                                    1. 37013358\\r\\n           1. Throat-punch that faggot DeNigger\\r\\n           [1]    ['I hope that you can see that you are advocat...\n",
      "9                                    1. 37013922\\r\\n  1. https://www.youtube.com/watch?v=DmNRkp_fuoo...           [1]    ['Gender is a category that shouldnât be the b...\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "Index(['id', 'text', 'hate_speech_idx', 'response'], dtype='object')\n",
      "\n",
      "- - - - - -\n",
      "\n",
      "1. 37011807\n",
      "2. \t37012801\n",
      "3. \t\t37013338\n",
      "4. \t\t37013511\n",
      "5. \t\t37333801\n",
      " 1. 37011807\n",
      "2. \t37012913\n",
      "3. \t\t37013738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_gab_m = pd.read_csv('gab_reddit_benchmark/gab_merged.csv')\n",
    "content_gab_m = content_gab_m.drop('Unnamed: 0', axis=1)\n",
    "content_gab_m = content_gab_m.drop('extracted_id', axis=1)\n",
    "\n",
    "\n",
    "content_gab_m[\"text\"] = content_gab_m[\"text\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "content_gab_m[\"response\"] = content_gab_m[\"response\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\")\n",
    "content_gab_m[\"hate_speech_idx\"] = content_gab_m[\"hate_speech_idx\"].replace(to_replace=[None, np.nan, \"\", \"nan\", \"n/a\"], value=\"\") \n",
    "\n",
    "for index, row in content_gab_m.iterrows():\n",
    "    row['text'] = row['text'].replace(\"'\", '\"')\n",
    "    row['response'] = row['response'].replace(\"'\", '\"')\n",
    "\n",
    "print(content_gab_m.head(n=10))\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab_m.columns)\n",
    "print('\\n- - - - - -\\n')\n",
    "print(content_gab_m.iloc[0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_text_labels(text_utterances_length, labels):\n",
    "    if not labels:\n",
    "        # return ['other'] * text_utterances_length\n",
    "        return [0] * text_utterances_length\n",
    "    new_labels = []\n",
    "    int_list = ast.literal_eval(labels)\n",
    "    for i in range(text_utterances_length):\n",
    "        if i+1 in int_list:\n",
    "            # new_labels.append('hate_speech')\n",
    "            new_labels.append(1)\n",
    "        else:\n",
    "            # new_labels.append('other')\n",
    "            new_labels.append(0)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting 'text' and 'response'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id                                                text                        text_labels                      response                       extracted_id response_labels\n",
      "0                                    1. 39869714\\r\\n  [1. i joined gab to remind myself how retarded...         [1]  [Using words that insult one group while defen...    39869714       [0, 0, 0]  \n",
      "1  1. 39845588\\r\\n2. \\t39848775\\r\\n3. \\t\\t3991101...  [1. This is what the left is really scared of....   [0, 0, 1]  [You can disagree with someones opinion withou...    39845588       [0, 0, 0]  \n",
      "2                   1. 37485560\\r\\n2. \\t37528625\\r\\n  [1. It makes you an asshole., 2. Give it to a ...      [0, 1]  [Your argument is more rational if you leave y...    37485560       [0, 0, 0]  \n",
      "3                   1. 39787626\\r\\n2. \\t39794481\\r\\n  [1. So they manage to provide a whole lot of d...      [0, 1]  [You shouldn't generalize a specific group or ...    39787626       [0, 0, 0]  \n",
      "4  1. 37957930\\r\\n2. \\t39953348\\r\\n3. \\t\\t3996521...  [1. Hi there, i,m Keith, i hope you are doing ...   [0, 0, 1]  [If someone is rude it is better to ignore the...    37957930       [0, 0, 0]  \n",
      "- - - - \n",
      "Index(['id', 'text', 'text_labels', 'response', 'extracted_id', 'response_labels'], dtype='object')\n",
      "1. 39869714\n",
      " [\"1. i joined gab to remind myself how retarded jew haters are. You wouldn't be typing on your abacus without them you retard.\"] [1] [\"Using words that insult one group while defending another group doesn't come across as helpful.\", 'You can make the same point more effectively without the use of hateful terminology.', 'Use of the r-word is unacceptable in our discourse as it demeans and insults people with mental disabilities.'] [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "text_column = []\n",
    "text_labels_column = []\n",
    "response_column = []\n",
    "response_labels_column = []\n",
    "\n",
    "for index, row in content_gab.iterrows():\n",
    "    text_utterances = row['text'].split('\\n')\n",
    "    text_utterances = list(filter(None, text_utterances))\n",
    "\n",
    "    for i, t in enumerate(text_utterances):\n",
    "        text_utterances[i] = clean_special_chars(t)\n",
    "    text_labels = mark_text_labels(len(text_utterances), row['hate_speech_idx'])\n",
    "\n",
    "    response_utterances = ast.literal_eval(row['response']) if row['response'] else []\n",
    "    for i, r in enumerate(response_utterances):\n",
    "        response_utterances[i] = clean_special_chars(r)\n",
    "    # response_labels = ['other'] * len(response_utterances)  \n",
    "    response_labels = [0] * len(response_utterances)  \n",
    "\n",
    "    \n",
    "    text_column.append(text_utterances)\n",
    "    text_labels_column.append(text_labels)\n",
    "    response_column.append(response_utterances)\n",
    "    response_labels_column.append(response_labels)\n",
    "\n",
    "content_gab['text'] = text_column\n",
    "content_gab['hate_speech_idx'] = text_labels_column\n",
    "content_gab['response'] = response_column\n",
    "content_gab['response_labels'] = response_labels_column\n",
    "\n",
    "content_gab = content_gab.rename(columns={'hate_speech_idx': 'text_labels'})\n",
    "print(content_gab.head())\n",
    "print('- - - - ')\n",
    "print(content_gab.columns)\n",
    "\n",
    "for index, row in content_gab.iterrows():\n",
    "    if index == 1:\n",
    "        continue\n",
    "    print(row['id'], row['text'], row['text_labels'], row['response'], row['response_labels'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"1. i joined gab to remind myself how retarded jew haters are. You wouldn't be typing on your abacus without them you retard.\", '1. This is what the left is really scared of.  https://redd.it/9rfkts', '2. That literally looks like a monkey. Why are we supposed to pretend itâs a person bc itâs wearing a red hat?', '3.  Dumb Cunt', '1. It makes you an asshole.', '2. Give it to a soldier who has defend it , then try to burn it . Show me what a badass you really are. The flag is helpless to stop you or fight back . Completely unnecessary act , its proves nothing and accomplishes nothing. It only shows your a ignorant weak faggot . Like kicking a puppy ,,, it shows how weak and worthless you are', '1. So they manage to provide a whole lot of data, except a critical little nugget.  Gay men are well over half the patients who present at STI clinics, and are probably the source of the emerging antibiotic-resistant pathogens.  https://www.dailymail.co.uk/health/article-6315871/One-young-person-England-told-STI-FOUR-MINUTES.html', \"2. yup. i cant smoke in a bar because it's harmful to some faggot that's getting drunk, doing drugs & having an STD potluck w/ the boys later...... seems legit.\", '1. Hi there, i,m Keith, i hope you are doing well, i would be very honored to get to know you if you would be interested? @PugLife', '2. Fuck off']\n",
      "[1, 0, 0, 1, 0, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# get text, first label, response, first response label for each row\n",
    "texts = []\n",
    "text_labels = []\n",
    "for index, row in content_gab.iterrows():\n",
    "    for i, t in enumerate(row['text']):\n",
    "        texts.append(t)\n",
    "        text_labels.append(row['text_labels'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def generate_embeddings(sentences):\n",
    "    if isinstance(sentences, list):\n",
    "        return bert.encode(sentences, show_progress_bar=True).tolist()\n",
    "    elif isinstance(sentences, str):\n",
    "        return bert.encode([sentences], show_progress_bar=True).tolist()\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|ââââââââââ| 1056/1056 [00:10<00:00, 102.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TIME FOR TEXT EMBEDDINGS:  10.999762773513794\n",
      "\n",
      "- - - - - -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# content_gab = content_gab[:200]\n",
    "# before = time.time()\n",
    "# content_gab['text_embeddings'] = content_gab['text'].apply(generate_embeddings)\n",
    "# after_text = time.time()\n",
    "# print(content_gab.iloc[1]['text_embeddings'])\n",
    "# print('\\nTIME FOR TEXT EMBEDDINGS: ', after_text - before)\n",
    "# print('\\n- - - - - -\\n')\n",
    "# content_gab['response_embeddings'] = content_gab['response'].apply(generate_embeddings)\n",
    "# after_response = time.time()\n",
    "# print(content_gab.iloc[2]['response_embeddings'])\n",
    "# print('\\nTIME FOR RESPONSE EMBEDDINGS: ', after_response - after_text)\n",
    "# print('\\n- - - - - -\\n')\n",
    "# embed texts\n",
    "before = time.time()\n",
    "text_embeddings = generate_embeddings(texts)\n",
    "after_text = time.time()\n",
    "print('\\nTIME FOR TEXT EMBEDDINGS: ', after_text - before)\n",
    "print('\\n- - - - - -\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 27020 27020\n",
      "Test data: 6756 6756\n",
      "(array([-3.44326384e-02, -2.18355618e-02,  7.33759627e-02,  3.13405097e-02,\n",
      "       -1.66528411e-02, -4.02435921e-02,  1.23953640e-01, -5.40787540e-02,\n",
      "        1.17365345e-01,  3.48896198e-02,  4.76283766e-02, -1.34696305e-01,\n",
      "        6.20039515e-02, -6.48817271e-02,  9.58839548e-04,  3.08414735e-02,\n",
      "       -3.19390707e-02, -5.50945699e-02, -9.64566618e-02,  4.76733781e-03,\n",
      "       -1.00273183e-02, -1.01099841e-01, -2.89843939e-02, -6.20556399e-02,\n",
      "       -1.00673907e-01, -6.28759339e-02,  2.01311186e-02,  4.61148005e-03,\n",
      "       -4.59683761e-02,  4.29530814e-03, -6.55897986e-03,  5.86478934e-02,\n",
      "        1.49863232e-02, -5.21263620e-03,  5.80718927e-03, -3.22730951e-02,\n",
      "       -6.03998918e-03,  4.06275280e-02, -1.64304469e-02,  2.99233347e-02,\n",
      "       -9.05553475e-02, -2.32283920e-02,  3.35135162e-02,  7.70595670e-02,\n",
      "        1.80413574e-02,  3.54835130e-02,  7.96923786e-02,  7.17390701e-02,\n",
      "       -1.62498638e-01, -6.56728223e-02, -8.81093815e-02,  3.67264263e-02,\n",
      "       -1.05273254e-01,  7.62019753e-02,  7.37342564e-03,  5.72585559e-04,\n",
      "       -6.36852831e-02, -1.22725619e-02,  5.05260266e-02,  6.11608438e-02,\n",
      "       -8.70664790e-03, -1.07482644e-02, -9.74018779e-03,  3.33403535e-02,\n",
      "        1.72754005e-02,  5.03425859e-02, -3.48507031e-03, -2.91120890e-03,\n",
      "       -1.92417186e-02,  5.58400266e-02,  3.59043069e-02,  8.75305757e-02,\n",
      "       -5.89504316e-02,  2.08170936e-02, -2.32147239e-02,  9.82308388e-03,\n",
      "        8.75084028e-02, -1.41760036e-02, -1.24257123e-02, -5.20023964e-02,\n",
      "       -5.02691902e-02, -3.20928320e-02,  5.94524182e-02,  7.86868185e-02,\n",
      "        5.11892401e-02,  4.00870740e-02, -4.11238782e-02, -1.04724698e-01,\n",
      "       -1.91123605e-01,  5.96913882e-02, -5.68959191e-02,  6.06866553e-02,\n",
      "        1.98007431e-02, -1.80294905e-02,  3.05408889e-06, -4.00181711e-02,\n",
      "        1.56152658e-02, -1.48142464e-02,  2.18880512e-02,  4.27554548e-02,\n",
      "       -5.22179389e-03,  1.03091784e-02,  1.06816161e-02,  6.99759126e-02,\n",
      "       -2.33659018e-02, -9.68523100e-02, -2.93898527e-02, -5.26097417e-03,\n",
      "        7.25108990e-03, -4.25920123e-03,  4.96249199e-02, -3.47294770e-02,\n",
      "       -1.72165465e-02, -1.43590728e-02,  2.17450727e-02, -2.57576406e-02,\n",
      "        7.73725063e-02,  2.86350474e-02,  5.39895929e-02, -4.83570714e-03,\n",
      "       -2.87024733e-02,  6.13336191e-02, -2.39518303e-02, -4.23923582e-02,\n",
      "       -5.48996851e-02, -9.82470512e-02, -2.47396454e-02, -2.78418023e-33,\n",
      "        4.52180244e-02,  2.19254401e-02,  3.06451917e-02,  3.63492966e-02,\n",
      "        1.43761355e-02,  2.05094405e-02, -7.04749525e-02,  8.44530668e-03,\n",
      "        2.80963406e-02,  1.70679908e-04,  4.35506403e-02,  7.82361031e-02,\n",
      "       -1.10380426e-01,  2.01497879e-02,  7.33519867e-02,  1.08461440e-01,\n",
      "        5.51578440e-02,  1.48302950e-02, -8.41973796e-02,  3.18812244e-02,\n",
      "        7.95175135e-02,  7.71261081e-02,  1.21233417e-02, -5.50371557e-02,\n",
      "       -6.56509697e-02, -6.43033311e-02,  8.54088366e-02,  2.10790783e-02,\n",
      "       -8.20060447e-03, -1.66712459e-02, -1.63520090e-02, -2.24231929e-02,\n",
      "        6.73388243e-02,  9.58821699e-02, -1.66145526e-02, -4.00720909e-02,\n",
      "       -5.67993941e-03, -5.93210869e-02, -2.59017479e-02,  5.28294742e-02,\n",
      "       -1.22955628e-02,  8.00892361e-04,  2.45805662e-02,  1.04646660e-01,\n",
      "       -8.27043783e-03,  2.50970051e-02,  6.20753467e-02, -7.69661134e-03,\n",
      "       -1.90371815e-02,  5.56145534e-02, -7.92581737e-02,  6.09002635e-02,\n",
      "        3.00647430e-02,  3.91116440e-02, -3.52811366e-02,  2.26850882e-02,\n",
      "        1.79345459e-02, -6.49848804e-02,  1.28964335e-02,  2.56162360e-02,\n",
      "       -7.25114485e-03, -1.84467025e-02, -3.21106170e-03, -3.83319519e-02,\n",
      "       -2.27385927e-02, -2.10053548e-02, -4.56223674e-02, -7.06297159e-02,\n",
      "        1.36845186e-02, -1.39038144e-02,  3.58394049e-02,  5.08173704e-02,\n",
      "       -2.69617829e-02, -1.54185286e-02,  8.34365562e-03, -3.77411395e-02,\n",
      "        8.59528929e-02,  3.04943006e-02,  9.50836018e-03, -3.76914181e-02,\n",
      "       -3.12871602e-03,  7.82454088e-02, -7.67531618e-02,  2.74523385e-02,\n",
      "       -1.09966308e-01, -2.95870136e-02,  7.64989257e-02, -6.73741400e-02,\n",
      "        1.55655770e-02, -3.37990932e-02, -5.71940802e-02, -1.05513465e-02,\n",
      "       -1.13662733e-02, -7.52232447e-02, -6.34541959e-02,  1.48636265e-33,\n",
      "       -2.38884240e-02, -2.07985770e-02, -5.21323131e-03,  8.14235285e-02,\n",
      "        8.76642913e-02, -4.85501550e-02,  2.46634725e-02, -5.87572809e-03,\n",
      "        6.60147890e-02,  3.81787904e-02, -1.72264501e-02, -5.69754057e-02,\n",
      "       -9.19315591e-03,  5.98751865e-02,  1.10418051e-02, -6.22866675e-02,\n",
      "        4.41359915e-02, -3.47389258e-04,  6.37802004e-04,  4.78761271e-02,\n",
      "       -1.47460932e-02,  7.16543868e-02, -1.39025263e-02,  6.70341402e-02,\n",
      "       -8.23602229e-02,  1.15359992e-01,  7.99051449e-02,  2.50750850e-03,\n",
      "        1.28272199e-03,  3.19502465e-02,  3.47734801e-02,  6.88823611e-02,\n",
      "       -9.16807801e-02,  1.91955478e-03,  8.26295242e-02, -4.52083349e-02,\n",
      "       -2.27166507e-02,  1.69685893e-02,  3.92575152e-02,  4.48798984e-02,\n",
      "       -4.84345295e-02, -3.03514227e-02, -5.17473966e-02,  3.94372456e-02,\n",
      "        6.97827339e-02, -3.12525481e-02,  5.98567054e-02, -1.94844007e-02,\n",
      "        1.20356660e-02, -5.38959503e-02, -5.02067246e-02,  3.63401091e-03,\n",
      "       -6.07308857e-02,  1.48229692e-02,  1.95640121e-02, -1.06285267e-01,\n",
      "        5.96529171e-02,  9.30345282e-02,  6.77254647e-02,  1.18059732e-01,\n",
      "        1.17200101e-02,  2.48170812e-02, -2.01744083e-02,  7.34404149e-03,\n",
      "       -5.35902344e-02, -3.51138972e-02, -3.00018135e-02,  2.79652290e-02,\n",
      "        3.81243825e-02, -5.35171106e-02, -2.13522986e-02, -4.65846732e-02,\n",
      "       -3.33625264e-02, -1.38921607e-02, -7.30092674e-02, -4.69162576e-02,\n",
      "       -5.32383695e-02, -9.19023007e-02, -1.83149129e-02,  6.44185171e-02,\n",
      "       -5.93541488e-02, -9.24584083e-03,  1.28312171e-01,  3.01309470e-02,\n",
      "       -1.28485460e-03, -1.11469859e-02,  2.23505385e-02,  6.78213686e-02,\n",
      "        6.95806518e-02, -3.59311998e-02,  9.99664739e-02,  2.46284753e-02,\n",
      "       -5.33304214e-02,  2.72065774e-03, -7.54385665e-02, -1.95109262e-08,\n",
      "        1.26574226e-02,  5.12535200e-02,  4.24666479e-02, -4.04631719e-02,\n",
      "       -4.31012586e-02,  4.64189611e-02,  1.19260782e-02, -6.24039099e-02,\n",
      "        1.46802412e-02,  7.83563359e-04, -2.56381929e-02,  1.65908728e-02,\n",
      "       -3.82518955e-02, -3.57316546e-02, -6.65740995e-03,  1.43828169e-02,\n",
      "       -4.41876426e-02, -7.79431984e-02,  3.40056047e-02,  7.11183697e-02,\n",
      "       -1.45893800e-03,  2.99899057e-02, -9.32462513e-02, -3.95143069e-02,\n",
      "       -6.46723956e-02,  8.21748078e-02, -1.24176819e-04, -3.89493071e-02,\n",
      "       -1.44783314e-02,  8.22601691e-02,  5.00571541e-02,  3.63903269e-02,\n",
      "        9.32216237e-04,  1.91568453e-02, -1.99675597e-02, -1.70457587e-02,\n",
      "       -2.53014155e-02,  2.87322607e-02, -2.96465643e-02, -1.51546216e-02,\n",
      "       -2.86927056e-02, -5.93634732e-02, -1.65926628e-02,  3.25288065e-02,\n",
      "        1.59577578e-02, -8.18610191e-02, -3.46243046e-02, -2.26563048e-02,\n",
      "       -4.08842042e-02,  4.01172936e-02, -4.62879986e-02,  8.78206361e-03,\n",
      "        1.02968952e-02,  2.91699432e-02, -8.67312588e-03, -8.84303823e-02,\n",
      "       -2.36038063e-02,  6.07488714e-02,  2.72297044e-03,  2.62287688e-02,\n",
      "        8.09820071e-02, -3.39738396e-03, -1.67966858e-02, -7.45952427e-02]), 0)\n",
      "(array([-6.81960285e-02,  7.41169080e-02, -5.40767275e-02, -4.48195152e-02,\n",
      "        4.06141058e-02,  5.04650362e-02,  1.33179259e-02, -1.09984970e-03,\n",
      "       -3.81117426e-02,  6.72109127e-02, -5.70887066e-02,  3.11955847e-02,\n",
      "       -1.66244637e-02, -6.82583377e-02, -7.49558285e-02, -6.86876476e-04,\n",
      "        7.12155830e-03, -1.05559401e-01, -1.95784122e-02,  8.67085531e-03,\n",
      "        3.46686803e-02,  4.45314907e-02,  8.37554485e-02, -1.41360443e-02,\n",
      "        2.03118958e-02, -4.23733406e-02,  2.09543891e-02, -2.81608589e-02,\n",
      "       -1.79612935e-02,  8.67289528e-02,  2.89390944e-02,  8.08125734e-03,\n",
      "        1.46274865e-02,  6.08112663e-02,  3.08790933e-02, -8.57205912e-02,\n",
      "        1.02805547e-01, -5.51849119e-02,  2.35143546e-02, -1.10624805e-02,\n",
      "       -8.20331275e-03, -9.24096033e-02, -3.90155390e-02, -1.27815912e-02,\n",
      "        4.26230133e-02, -2.92827524e-02, -1.29406191e-02, -1.51251471e-02,\n",
      "       -1.49196358e-02, -1.05930222e-02,  5.75962178e-02,  4.50129397e-02,\n",
      "       -4.05045040e-02,  8.10791552e-02, -1.84727982e-02, -3.36664394e-02,\n",
      "       -7.64035359e-02, -7.00007677e-02,  2.05542799e-02, -4.55635265e-02,\n",
      "       -4.65750732e-02, -4.88564931e-03, -3.23952921e-02,  5.47113195e-02,\n",
      "       -2.70703156e-02, -4.44911011e-02,  3.06047518e-02, -3.87289040e-02,\n",
      "       -5.52273542e-02,  5.42936996e-02, -2.03255191e-03,  2.47955993e-02,\n",
      "       -3.18957195e-02, -1.80889294e-02, -2.72381380e-02, -3.52725498e-02,\n",
      "        4.43030968e-02, -1.13442577e-02,  3.29622347e-03, -5.47566712e-02,\n",
      "       -5.34599461e-02,  2.42758393e-02,  2.05247966e-03, -4.46238648e-03,\n",
      "       -4.25357297e-02, -8.03940147e-02,  4.32700897e-03,  8.49854127e-02,\n",
      "        8.90398212e-03,  1.73831824e-02,  3.96740576e-03,  3.34531851e-02,\n",
      "       -2.53357831e-02,  3.88112850e-02,  1.09273661e-02, -2.39931475e-02,\n",
      "        9.33204778e-03,  6.24888390e-02, -1.01801723e-01, -1.68087613e-03,\n",
      "       -4.64520454e-02, -1.33176474e-02,  2.17642952e-02,  3.32873985e-02,\n",
      "       -3.30409817e-02,  2.03352682e-02, -2.42322795e-02,  1.14455875e-02,\n",
      "       -2.07320414e-03, -9.56341475e-02, -4.68980819e-02, -3.56178656e-02,\n",
      "       -3.15726385e-03,  6.69421852e-02,  1.29540951e-03, -4.18017358e-02,\n",
      "       -5.48020098e-03, -9.28129535e-03,  8.60795081e-02,  8.89439136e-02,\n",
      "        4.94009722e-03,  4.53831218e-02, -1.16340220e-01,  6.99404180e-02,\n",
      "       -1.20933950e-01, -8.83736014e-02, -9.36597213e-03, -4.22365994e-33,\n",
      "       -3.11951269e-03, -6.80997744e-02,  1.09352162e-02,  8.59653801e-02,\n",
      "       -5.60917184e-02,  9.46203768e-02, -5.19958958e-02,  1.72612164e-02,\n",
      "       -5.45299985e-02,  6.57430515e-02,  1.66110229e-02, -8.32677484e-02,\n",
      "       -1.89238638e-02,  3.90782282e-02, -1.99081581e-02, -5.88955998e-04,\n",
      "       -8.71990696e-02,  3.19668627e-03,  7.07173944e-02, -4.75693196e-02,\n",
      "       -1.81881571e-03,  1.68675333e-02, -2.07182039e-02, -8.87990184e-03,\n",
      "       -4.59884144e-02, -8.64913967e-03, -1.54914176e-02,  1.23471171e-02,\n",
      "        8.23721290e-02,  2.48891264e-02,  3.32822502e-02, -4.63558361e-02,\n",
      "        4.63469401e-02, -8.67115520e-03, -6.91062659e-02,  2.47204248e-02,\n",
      "        1.58212125e-01,  1.17462249e-02, -4.73794043e-02, -1.16319820e-01,\n",
      "        3.68872061e-02, -9.45534371e-03,  4.49896380e-02,  1.61981359e-02,\n",
      "        6.96472898e-02, -1.82377007e-02,  8.16038251e-02, -7.54221752e-02,\n",
      "        4.34679277e-02,  5.37706725e-02, -2.57820804e-02,  3.89094576e-02,\n",
      "        4.81001139e-02, -4.98991869e-02, -9.34903231e-03, -1.16897579e-02,\n",
      "       -1.37587711e-02,  3.26801240e-02, -2.29436439e-02,  2.09730826e-02,\n",
      "        8.67638364e-03, -5.45791797e-02,  1.08232103e-01,  1.54610192e-02,\n",
      "        4.35716892e-03,  3.08037568e-02,  1.02207303e-01,  3.29995248e-03,\n",
      "        9.70402174e-03,  1.38616562e-02,  3.65772955e-02, -6.06191680e-02,\n",
      "       -1.28457338e-01, -2.09496710e-02,  2.06929669e-02,  4.81457636e-02,\n",
      "        1.15823917e-01, -7.38470815e-03,  9.67245996e-02, -1.92487258e-02,\n",
      "        5.47539117e-03,  1.83558501e-02, -2.60944180e-02,  7.22710714e-02,\n",
      "        5.54320216e-02,  1.31484191e-03, -3.90171558e-02, -7.69400373e-02,\n",
      "        5.57321729e-03, -3.87906954e-02,  3.33545767e-02, -8.54934156e-02,\n",
      "        7.24223033e-02,  8.20747577e-04,  2.56299376e-02,  5.53104301e-34,\n",
      "       -9.77500435e-03,  1.49800936e-02, -6.37369677e-02,  2.79098633e-03,\n",
      "       -3.18253450e-02, -4.17691506e-02,  2.38703657e-02, -3.63659952e-03,\n",
      "       -6.01615570e-03,  4.29483950e-02,  5.41844033e-02, -2.82901023e-02,\n",
      "        1.08038388e-01,  5.66186458e-02,  6.65552914e-04,  3.11132800e-03,\n",
      "       -6.65149977e-03,  7.81658385e-03,  1.65518783e-02,  1.22913988e-02,\n",
      "        1.86093189e-02,  5.62183782e-02, -3.86864617e-02,  3.02603049e-03,\n",
      "       -3.26906256e-02,  4.39927280e-02, -2.44799219e-02, -5.20555303e-03,\n",
      "       -5.74637502e-02, -5.69390692e-02, -4.97543849e-02,  5.06667932e-03,\n",
      "       -7.25930408e-02,  4.77846749e-02, -1.41972126e-02, -4.47150506e-02,\n",
      "        7.15317354e-02,  5.46111688e-02, -1.73171796e-02, -1.56529676e-02,\n",
      "        3.03819329e-02, -2.19130851e-02, -9.65796337e-02,  4.48844358e-02,\n",
      "       -2.96774991e-02, -8.45069718e-03,  1.86050534e-02,  3.08384001e-02,\n",
      "        5.29985167e-02,  7.72769749e-02, -5.45641221e-02,  6.85114563e-02,\n",
      "        1.86225101e-02, -4.06855978e-02, -1.88128557e-02, -5.11195771e-02,\n",
      "        5.72829368e-03,  1.85122471e-02,  1.19696297e-01,  7.86688924e-02,\n",
      "        3.41848359e-02,  1.11461215e-01, -1.11395130e-02, -6.28047585e-02,\n",
      "       -3.04558733e-03,  4.83954139e-02,  1.33940368e-03,  3.47397961e-02,\n",
      "       -5.23576364e-02,  6.16435260e-02,  3.45343985e-02, -2.28071269e-02,\n",
      "       -9.45214853e-02, -6.61671162e-03, -7.94973876e-03, -6.94908574e-02,\n",
      "        8.04641247e-02,  2.82158740e-02, -5.00945076e-02, -1.28942942e-02,\n",
      "       -3.58138755e-02, -2.60976143e-02,  1.62811782e-02,  3.83669771e-02,\n",
      "        1.27748027e-02, -1.02082565e-01,  1.07712194e-01,  6.17679581e-02,\n",
      "        8.27870965e-02,  8.23412463e-02,  8.00582208e-03, -1.09058239e-01,\n",
      "       -8.09521750e-02, -6.18056692e-02, -6.33766726e-02, -2.61152699e-08,\n",
      "       -3.37328203e-03,  9.50812772e-02, -2.92708841e-03, -3.76345329e-02,\n",
      "        5.12166470e-02,  6.74103945e-02, -2.92178895e-02,  1.78203806e-02,\n",
      "        4.98370454e-02, -8.81213881e-03,  9.03643016e-03, -7.89987855e-03,\n",
      "        4.36377339e-02,  6.08886369e-02,  1.27170486e-02,  4.62292992e-02,\n",
      "        7.53874471e-03, -7.51216561e-02, -3.90824564e-02, -1.31750228e-02,\n",
      "        1.46696847e-02,  1.06567591e-01,  7.03002587e-02, -5.35576269e-02,\n",
      "       -2.83881985e-02, -2.27716696e-02, -3.06004044e-02, -1.43896937e-02,\n",
      "       -7.03701153e-02, -6.44496307e-02,  3.67944464e-02,  2.10632272e-02,\n",
      "       -1.25900030e-01, -5.70253357e-02, -2.77697947e-02,  6.20078482e-02,\n",
      "       -7.35907108e-02,  4.34663333e-02, -2.44953763e-02, -1.12712644e-01,\n",
      "       -7.18755275e-02,  4.87329699e-02, -1.98218096e-02, -3.54876406e-02,\n",
      "        1.24695636e-02, -3.57563496e-02, -1.28585383e-01,  1.86037254e-02,\n",
      "       -1.03928801e-02, -6.42491737e-03, -6.21931180e-02,  1.86154973e-02,\n",
      "        3.44247632e-02,  1.56412460e-02,  1.21586852e-01, -1.05702475e-01,\n",
      "       -2.57684523e-03,  1.88964307e-02, -5.46313822e-02,  5.44995740e-02,\n",
      "        1.01627454e-01, -1.72722265e-02, -9.48564261e-02, -3.12049761e-02]), 0)\n"
     ]
    }
   ],
   "source": [
    "# make dataset from embeddings and labels\n",
    "class GabDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "# split data into train and test\n",
    "text_embeddings = np.array(text_embeddings)\n",
    "text_labels = np.array(text_labels)\n",
    "\n",
    "text_embeddings = np.stack(text_embeddings, axis=0)\n",
    "text_labels = np.stack(text_labels, axis=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_embeddings, text_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Train data:', len(X_train), len(y_train))\n",
    "print('Test data:', len(X_test), len(y_test))\n",
    "\n",
    "# create dataset\n",
    "train_dataset = GabDataset(X_train, y_train)\n",
    "test_dataset = GabDataset(X_test, y_test)\n",
    "\n",
    "print(train_dataset[0])\n",
    "print(test_dataset[0])\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv1d(1, 16, kernel_size=(2,), stride=(1,))\n",
      "  (conv2): Conv1d(16, 16, kernel_size=(2,), stride=(1,))\n",
      ")\n",
      "torch.Size([16, 16, 382])\n",
      "torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wrobl\\AppData\\Local\\Temp\\ipykernel_13228\\972640007.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs).float()\n",
      "C:\\Users\\Wrobl\\AppData\\Local\\Temp\\ipykernel_13228\\972640007.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).long()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [16, 382], got [16]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%5d\u001b[39;00m\u001b[38;5;124m] loss: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m))\n\u001b[0;32m     43\u001b[0m                 running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 45\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m101\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# test model\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtest\u001b[39m(model, test_loader):\n",
      "Cell \u001b[1;32mIn[115], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 37\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [16, 382], got [16]"
     ]
    }
   ],
   "source": [
    "input_dim = len(train_dataset[0][0])\n",
    "sequence_length = len(train_dataset[0][0])\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=hidden_channels, kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=hidden_channels, out_channels=hidden_channels, kernel_size=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "# Adjust kernel size to 2\n",
    "model = CNN(hidden_channels=16)\n",
    "print(model)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train model\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            inputs = torch.tensor(inputs).float()\n",
    "            labels = torch.tensor(labels).long()\n",
    "            outputs = model(inputs)\n",
    "            print(outputs.shape)\n",
    "            print(labels.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "\n",
    "train(model, train_loader, criterion, optimizer, num_epochs=101)\n",
    "\n",
    "# test model\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = torch.tensor(inputs).float().permute(0, 2, 1)\n",
    "            labels = torch.tensor(labels).long()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the test data: %d %%' % (100 * correct / total))\n",
    "\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv1d() received an invalid combination of arguments - got (GabDataset, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!GabDataset!, !Parameter!, !Parameter!, !tuple of (int,)!, !tuple of (int,)!, !tuple of (int,)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!GabDataset!, !Parameter!, !Parameter!, !tuple of (int,)!, !tuple of (int,)!, !tuple of (int,)!, !int!)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_num):\n\u001b[1;32m---> 40\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test()\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[29], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, train_dataset)\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[27], line 12\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool1d(x, x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\dev\\NLP\\GNN_application_in_hate_speech_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv1d() received an invalid combination of arguments - got (GabDataset, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!GabDataset!, !Parameter!, !Parameter!, !tuple of (int,)!, !tuple of (int,)!, !tuple of (int,)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!GabDataset!, !Parameter!, !Parameter!, !tuple of (int,)!, !tuple of (int,)!, !tuple of (int,)!, !int!)\n"
     ]
    }
   ],
   "source": [
    "# Plot the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
